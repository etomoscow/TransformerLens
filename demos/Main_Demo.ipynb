{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Main_Demo.ipynb\">\n",
                "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
                "</a>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformer Lens Main Demo Notebook\n",
                "\n",
                "<b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>\n",
                "\n",
                "This is a reference notebook covering the main features of the [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens) library for mechanistic interpretability. See [Callum McDougall's tutorial](https://transformerlens-intro.streamlit.app/TransformerLens_&_induction_circuits) for a more structured and gentler introduction to the library"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Tips for reading this Colab:**\n",
                "* You can run all this code for yourself! \n",
                "* The graphs are interactive!\n",
                "* Use the table of contents pane in the sidebar to navigate\n",
                "* Collapse irrelevant sections with the dropdown arrows\n",
                "* Search the page using the search in the sidebar, not CTRL+F"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Setup\n",
                "(No need to read)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "DEVELOPMENT_MODE = False\n",
                "# Detect if we're running in Google Colab\n",
                "try:\n",
                "    import google.colab\n",
                "    IN_COLAB = True\n",
                "    print(\"Running as a Colab notebook\")\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "# Install if in Colab\n",
                "if IN_COLAB:\n",
                "    %pip install transformer_lens\n",
                "    %pip install circuitsvis\n",
                "    # Install a faster Node version\n",
                "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs  # noqa\n",
                "\n",
                "# Hot reload in development mode & not running on the CD\n",
                "if not IN_COLAB:\n",
                "    from IPython import get_ipython\n",
                "    ip = get_ipython()\n",
                "    if not ip.extension_manager.loaded:\n",
                "        ip.extension_manager.load('autoreload')\n",
                "        %autoreload 2\n",
                "        \n",
                "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using renderer: colab\n"
                    ]
                }
            ],
            "source": [
                "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
                "import plotly.io as pio\n",
                "if IN_COLAB or not DEVELOPMENT_MODE:\n",
                "    pio.renderers.default = \"colab\"\n",
                "else:\n",
                "    pio.renderers.default = \"notebook_connected\"\n",
                "print(f\"Using renderer: {pio.renderers.default}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div id=\"circuits-vis-bf880809-b023\" style=\"margin: 15px 0;\"/>\n",
                            "    <script crossorigin type=\"module\">\n",
                            "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
                            "    render(\n",
                            "      \"circuits-vis-bf880809-b023\",\n",
                            "      Hello,\n",
                            "      {\"name\": \"Neel\"}\n",
                            "    )\n",
                            "    </script>"
                        ],
                        "text/plain": [
                            "<circuitsvis.utils.render.RenderedHTML at 0x12b5273d0>"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import circuitsvis as cv\n",
                "# Testing that the library works\n",
                "cv.examples.hello(\"Neel\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import stuff\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import einops\n",
                "from fancy_einsum import einsum\n",
                "import tqdm.auto as tqdm\n",
                "import plotly.express as px\n",
                "\n",
                "from jaxtyping import Float\n",
                "from functools import partial"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n"
                    ]
                }
            ],
            "source": [
                "# import transformer_lens\n",
                "import transformer_lens.utils as utils\n",
                "from transformer_lens.hook_points import (\n",
                "    HookPoint,\n",
                ")  # Hooking utilities\n",
                "from transformer_lens.model_bridge import TransformerBridge"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<torch.autograd.grad_mode.set_grad_enabled at 0x13b0e9510>"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.set_grad_enabled(False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Plotting helper functions:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
                "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
                "\n",
                "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
                "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
                "\n",
                "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
                "    x = utils.to_numpy(x)\n",
                "    y = utils.to_numpy(y)\n",
                "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is a demo notebook for [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens), **a library I ([Neel Nanda](https://neelnanda.io)) wrote for doing [mechanistic interpretability](https://distill.pub/2020/circuits/zoom-in/) of GPT-2 Style language models.** The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights. It is a fact about the world today that we have computer programs that can essentially speak English at a human level (GPT-3, PaLM, etc), yet we have no idea how they work nor how to write one ourselves. This offends me greatly, and I would like to solve this! Mechanistic interpretability is a very young and small field, and there are a *lot* of open problems - if you would like to help, please try working on one! **If you want to skill up, check out [my guide to getting started](https://neelnanda.io/getting-started), and if you want to jump into an open problem check out my sequence [200 Concrete Open Problems in Mechanistic Interpretability](https://neelnanda.io/concrete-open-problems).**\n",
                "\n",
                "I wrote this library because after I left the Anthropic interpretability team and started doing independent research, I got extremely frustrated by the state of open source tooling. There's a lot of excellent infrastructure like HuggingFace and DeepSpeed to *use* or *train* models, but very little to dig into their internals and reverse engineer how they work. **This library tries to solve that**, and to make it easy to get into the field even if you don't work at an industry org with real infrastructure! The core features were heavily inspired by [Anthropic's excellent Garcon tool](https://transformer-circuits.pub/2021/garcon/index.html). Credit to Nelson Elhage and Chris Olah for building Garcon and showing me the value of good infrastructure for accelerating exploratory research!\n",
                "\n",
                "The core design principle I've followed is to enable exploratory analysis - one of the most fun parts of mechanistic interpretability compared to normal ML is the extremely short feedback loops! The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for **research to feel like play** and to enter a flow state. This notebook demonstrates how the library works and how to use it, but if you want to see how well it works for exploratory research, check out [my notebook analysing Indirect Objection Identification](https://neelnanda.io/exploratory-analysis-demo) or [my recording of myself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading and Running Models\n",
                "\n",
                "TransformerLens comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. For this demo notebook we'll look at GPT-2 Small, an 80M parameter model, see the Available Models section for info on the rest."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = utils.get_device()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
                        "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_resid_pre' is deprecated and will be removed in a future version. Use 'hook_in' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_resid_mid' is deprecated and will be removed in a future version. Use 'attn.hook_out' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_resid_post' is deprecated and will be removed in a future version. Use 'hook_out' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_attn_in' is deprecated and will be removed in a future version. Use 'attn.hook_in' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_attn_out' is deprecated and will be removed in a future version. Use 'attn.hook_out' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_q_input' is deprecated and will be removed in a future version. Use 'attn.q.hook_in' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_k_input' is deprecated and will be removed in a future version. Use 'attn.k.hook_in' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_v_input' is deprecated and will be removed in a future version. Use 'attn.v.hook_in' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_mlp_in' is deprecated and will be removed in a future version. Use 'mlp.hook_in' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_mlp_out' is deprecated and will be removed in a future version. Use 'mlp.hook_out' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_result' is deprecated and will be removed in a future version. Use 'hook_out' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_q' is deprecated and will be removed in a future version. Use 'q.hook_out' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_k' is deprecated and will be removed in a future version. Use 'k.hook_out' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_v' is deprecated and will be removed in a future version. Use 'v.hook_out' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_z' is deprecated and will be removed in a future version. Use 'hook_hidden_states' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_pre' is deprecated and will be removed in a future version. Use 'in.hook_out' instead.\n",
                        "\n",
                        "/Users/fabiandegen/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:160: FutureWarning:\n",
                        "\n",
                        "Hook 'hook_post' is deprecated and will be removed in a future version. Use 'out.hook_in' instead.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# NBVAL_IGNORE_OUTPUT\n",
                "model = TransformerBridge.boot_transformers(\"gpt2\", device=device)\n",
                "model.enable_compatibility_mode()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To try the model out, let's find the loss on this text! Models can be run on a single string or a tensor of tokens (shape: [batch, position], all integers), and the possible return types are: \n",
                "* \"logits\" (shape [batch, position, d_vocab], floats), \n",
                "* \"loss\" (the cross-entropy loss when predicting the next token), \n",
                "* \"both\" (a tuple of (logits, loss)) \n",
                "* None (run the model, but don't calculate the logits - this is faster when we only want to use intermediate activations)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loss: tensor(10.5568, device='mps:0')\n"
                    ]
                }
            ],
            "source": [
                "model_description_text = \"\"\"## Loading Models\n",
                "\n",
                "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. See my explainer for documentation of all supported models, and this table for hyper-parameters and the name used to load them. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly. \n",
                "\n",
                "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
                "loss = model(model_description_text, return_type=\"loss\")\n",
                "print(\"Model loss:\", loss)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Caching all Activations\n",
                "\n",
                "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`. Let's try this out on the first line of the abstract of the GPT-2 paper.\n",
                "\n",
                "<details><summary>On `remove_batch_dim`</summary>\n",
                "\n",
                "Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and kinda annoying, so passing in the `remove_batch_dim=True` keyword removes it. `gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()` would have achieved the same effect.\n",
                "</details?>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "mps:0\n"
                    ]
                }
            ],
            "source": [
                "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
                "gpt2_tokens = model.to_tokens(gpt2_text)\n",
                "print(gpt2_tokens.device)\n",
                "gpt2_logits, gpt2_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's visualize the attention pattern of all the heads in layer 0, using [Alan Cooney's CircuitsVis library](https://github.com/alan-cooney/CircuitsVis) (based on [Anthropic's PySvelte library](https://github.com/anthropics/PySvelte)). \n",
                "\n",
                "We look this the attention pattern in `gpt2_cache`, an `ActivationCache` object, by entering in the name of the activation, followed by the layer index (here, the activation is called \"attn\" and the layer index is 0). This has shape [head_index, destination_position, source_position], and we use the `model.to_str_tokens` method to convert the text to a list of tokens as strings, since there is an attention weight between each pair of tokens.\n",
                "\n",
                "This visualization is interactive! Try hovering over a token or head, and click to lock. The grid on the top left and for each head is the attention pattern as a destination position by source position grid. It's lower triangular because GPT-2 has **causal attention**, attention can only look backwards, so information can only move forwards in the network.\n",
                "\n",
                "See the ActivationCache section for more on what `gpt2_cache` can do."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<class 'transformer_lens.ActivationCache.ActivationCache'>\n",
                        "torch.Size([12, 33, 33])\n"
                    ]
                }
            ],
            "source": [
                "print(type(gpt2_cache))\n",
                "attention_pattern = gpt2_cache[\"pattern\", 0, \"attn\"]\n",
                "print(attention_pattern.shape)\n",
                "gpt2_str_tokens = model.to_str_tokens(gpt2_text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Layer 0 Head Attention Patterns:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div id=\"circuits-vis-76bf92d2-5739\" style=\"margin: 15px 0;\"/>\n",
                            "    <script crossorigin type=\"module\">\n",
                            "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
                            "    render(\n",
                            "      \"circuits-vis-76bf92d2-5739\",\n",
                            "      AttentionPatterns,\n",
                            "      {\"tokens\": [\"<|endoftext|>\", \"Natural\", \" language\", \" processing\", \" tasks\", \",\", \" such\", \" as\", \" question\", \" answering\", \",\", \" machine\", \" translation\", \",\", \" reading\", \" comprehension\", \",\", \" and\", \" summar\", \"ization\", \",\", \" are\", \" typically\", \" approached\", \" with\", \" supervised\", \" learning\", \" on\", \" tasks\", \"pe\", \"cific\", \" datasets\", \".\"], \"attention\": [[[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]], [[0.07829540222883224, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835, 0.02880326844751835], [0.04807126149535179, 0.04807126149535179, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027, 0.029156694188714027], [0.04082309082150459, 0.04082309082150459, 0.04082309082150459, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138, 0.02925102226436138], [0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.03761487826704979, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958, 0.02929449826478958], [0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.03581089526414871, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365, 0.029319483786821365], [0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.03465602919459343, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027, 0.02933569625020027], [0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.03385375067591667, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327, 0.029347067698836327], [0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.033264122903347015, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657, 0.0293554849922657], [0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.03281255066394806, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428, 0.029361959546804428], [0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.03245566785335541, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455, 0.029367102310061455], [0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.032166529446840286, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118, 0.02937128022313118], [0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.031927525997161865, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875, 0.02937474474310875], [0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.03172667324542999, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744, 0.029377665370702744], [0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.031555503606796265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265, 0.029380155727267265], [0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.03140789642930031, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627, 0.029382307082414627], [0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0312793031334877, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702, 0.0293841864913702], [0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.03116627223789692, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718, 0.029385840520262718], [0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.031066134572029114, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014, 0.029387304559350014], [0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.030976811423897743, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923, 0.029388613998889923], [0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.030896637588739395, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088, 0.029389789327979088], [0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.03082427568733692, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415, 0.02939085103571415], [0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.030758637934923172, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145, 0.02939181588590145], [0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.030698830261826515, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187, 0.029392698779702187], [0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.03064410388469696, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356, 0.029393499717116356], [0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.030593840405344963, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545, 0.02939423732459545], [0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.030547520145773888, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037, 0.02939492277801037], [0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.03050469234585762, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107, 0.029395556077361107], [0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.03046497330069542, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816, 0.029396139085292816], [0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.030428044497966766, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669, 0.02939668670296669], [0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.030393613502383232, 0.02939719147980213, 0.02939719147980213, 0.02939719147980213], [0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.030361438170075417, 0.029397664591670036, 0.029397664591670036], [0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.030331308022141457, 0.029398111626505852], [0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098, 0.03030303120613098]]]}\n",
                            "    )\n",
                            "    </script>"
                        ],
                        "text/plain": [
                            "<circuitsvis.utils.render.RenderedHTML at 0x10b3a1110>"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "print(\"Layer 0 Head Attention Patterns:\")\n",
                "cv.attention.attention_patterns(tokens=gpt2_str_tokens, attention=attention_pattern)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this case, we only wanted the layer 0 attention patterns, but we are storing the internal activations from all locations in the model. It's convenient to have access to all activations, but this can be prohibitively expensive for memory use with larger models, batch sizes, or sequence lengths. In addition, we don't need to do the full forward pass through the model to collect layer 0 attention patterns. The following cell will collect only the layer 0 attention patterns and stop the forward pass at layer 1, requiring far less memory and compute."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "attn_hook_name = \"blocks.0.attn.hook_pattern\"\n",
                "attn_layer = 0\n",
                "_, gpt2_attn_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True, stop_at_layer=attn_layer + 1, names_filter=[attn_hook_name])\n",
                "gpt2_attn = gpt2_attn_cache[attn_hook_name]\n",
                "assert torch.equal(gpt2_attn, attention_pattern)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hooks: Intervening on Activations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One of the great things about interpreting neural networks is that we have *full control* over our system. From a computational perspective, we know exactly what operations are going on inside (even if we don't know what they mean!). And we can make precise, surgical edits and see how the model's behaviour and other internals change. This is an extremely powerful tool, because it can let us eg set up careful counterfactuals and causal intervention to easily understand model behaviour. \n",
                "\n",
                "Accordingly, being able to do this is a pretty core operation, and this is one of the main things TransformerLens supports! The key feature here is **hook points**. Every activation inside the transformer is surrounded by a hook point, which allows us to edit or intervene on it. \n",
                "\n",
                "We do this by adding a **hook function** to that activation. The hook function maps `current_activation_value, hook_point` to `new_activation_value`. As the model is run, it computes that activation as normal, and then the hook function is applied to compute a replacement, and that is substituted in for the activation. The hook function can be an arbitrary Python function, so long as it returns a tensor of the correct shape.\n",
                "\n",
                "<details><summary>Relationship to PyTorch hooks</summary>\n",
                "\n",
                "[PyTorch hooks](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/) are a great and underrated, yet incredibly janky, feature. They can act on a layer, and edit the input or output of that layer, or the gradient when applying autodiff. The key difference is that **Hook points** act on *activations* not layers. This means that you can intervene within a layer on each activation, and don't need to care about the precise layer structure of the transformer. And it's immediately clear exactly how the hook's effect is applied. This adjustment was shamelessly inspired by [Garcon's use of ProbePoints](https://transformer-circuits.pub/2021/garcon/index.html).\n",
                "\n",
                "They also come with a range of other quality of life improvements, like the model having a `model.reset_hooks()` method to remove all hooks, or helper methods to temporarily add hooks for a single forward pass - it is *incredibly* easy to shoot yourself in the foot with standard PyTorch hooks!\n",
                "</details>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As a basic example, let's [ablate](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=fh-HJyz1CgUVrXuoiban6bYx) head 7 in layer 0 on the text above. \n",
                "\n",
                "We define a `head_ablation_hook` function. This takes the value tensor for attention layer 0, and sets the component with `head_index==7` to zero and returns it (Note - we return by convention, but since we're editing the activation in-place, we don't strictly *need* to).\n",
                "\n",
                "We then use the `run_with_hooks` helper function to run the model and *temporarily* add in the hook for just this run. We enter in the hook as a tuple of the activation name (also the hook point name - found with `utils.get_act_name`) and the hook function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Shape of the value tensor: torch.Size([1, 33, 12, 64])\n",
                        "Original Loss: 11.896\n",
                        "Ablated Loss: 11.712\n"
                    ]
                }
            ],
            "source": [
                "layer_to_ablate = 0\n",
                "head_index_to_ablate = 8\n",
                "\n",
                "# We define a head ablation hook\n",
                "# The type annotations are NOT necessary, they're just a useful guide to the reader\n",
                "# \n",
                "def head_ablation_hook(\n",
                "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
                "    hook: HookPoint\n",
                ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
                "    print(f\"Shape of the value tensor: {value.shape}\")\n",
                "    value[:, :, head_index_to_ablate, :] = 0.\n",
                "    return value\n",
                "\n",
                "original_loss = model(gpt2_tokens, return_type=\"loss\")\n",
                "ablated_loss = model.run_with_hooks(\n",
                "    gpt2_tokens, \n",
                "    return_type=\"loss\", \n",
                "    fwd_hooks=[(\n",
                "        utils.get_act_name(\"v\", layer_to_ablate), \n",
                "        head_ablation_hook\n",
                "        )]\n",
                "    )\n",
                "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
                "print(f\"Ablated Loss: {ablated_loss.item():.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Gotcha:** Hooks are global state - they're added in as part of the model, and stay there until removed. `run_with_hooks` tries to create an abstraction where these are local state, by removing all hooks at the end of the function. But you can easily shoot yourself in the foot if there's, eg, an error in one of your hooks so the function never finishes. If you start getting bugs, try `model.reset_hooks()` to clean things up. Further, if you *do* add hooks of your own that you want to keep, which you can do with `add_perma_hook` on the relevant HookPoint"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Activation Patching on the Indirect Object Identification Task"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For a somewhat more involved example, let's use hooks to apply **[activation patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx)** on the **[Indirect Object Identification](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iWsV3s5Kdd2ca3zNgXr5UPHa)** (IOI) task. \n",
                "\n",
                "The IOI task is the task of identifying that a sentence like \"After John and Mary went to the store, Mary gave a bottle of milk to\" continues with \" John\" rather than \" Mary\" (ie, finding the indirect object), and Redwood Research have [an excellent paper studying the underlying circuit in GPT-2 Small](https://arxiv.org/abs/2211.00593).\n",
                "\n",
                "**[Activation patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx)** is a technique from [Kevin Meng and David Bau's excellent ROME paper](https://rome.baulab.info/). The goal is to identify which model activations are important for completing a task. We do this by setting up a **clean prompt** and a **corrupted prompt** and a **metric** for performance on the task. We then pick a specific model activation, run the model on the corrupted prompt, but then *intervene* on that activation and patch in its value when run on the clean prompt. We then apply the metric, and see how much this patch has recovered the clean performance. \n",
                "(See [a more detailed demonstration of activation patching here](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Exploratory_Analysis_Demo.ipynb))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here, our clean prompt is \"After John and Mary went to the store, **Mary** gave a bottle of milk to\", our corrupted prompt is \"After John and Mary went to the store, **John** gave a bottle of milk to\", and our metric is the difference between the correct logit ( John) and the incorrect logit ( Mary) on the final token. \n",
                "\n",
                "We see that the logit difference is significantly positive on the clean prompt, and significantly negative on the corrupted prompt, showing that the model is capable of doing the task!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Clean logit difference: 1.263\n",
                        "Corrupted logit difference: 1.379\n"
                    ]
                }
            ],
            "source": [
                "clean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\n",
                "corrupted_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
                "\n",
                "clean_tokens = model.to_tokens(clean_prompt)\n",
                "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
                "\n",
                "def logits_to_logit_diff(logits, correct_answer=\" John\", incorrect_answer=\" Mary\"):\n",
                "    # model.to_single_token maps a string value of a single token to the token index for that token\n",
                "    # If the string is not a single token, it raises an error.\n",
                "    correct_index = model.to_single_token(correct_answer)\n",
                "    incorrect_index = model.to_single_token(incorrect_answer)\n",
                "    return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]\n",
                "\n",
                "# We run on the clean prompt with the cache so we store activations to patch in later.\n",
                "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
                "clean_logit_diff = logits_to_logit_diff(clean_logits)\n",
                "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
                "\n",
                "# We don't need to cache on the corrupted prompt.\n",
                "corrupted_logits = model(corrupted_tokens)\n",
                "corrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\n",
                "print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now setup the hook function to do activation patching. Here, we'll patch in the [residual stream](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=DHp9vZ0h9lA9OCrzG2Y3rrzH) at the start of a specific layer and at a specific position. This will let us see how much the model is using the residual stream at that layer and position to represent the key information for the task. \n",
                "\n",
                "We want to iterate over all layers and positions, so we write the hook to take in an position parameter. Hook functions must have the input signature (activation, hook), but we can use `functools.partial` to set the position parameter before passing it to `run_with_hooks`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
                        "To disable this warning, you can either:\n",
                        "\t- Avoid using `tokenizers` before the fork if possible\n",
                        "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7fc6b6459b9b4ce1977c15565715d23d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/12 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# We define a residual stream patching hook\n",
                "# We choose to act on the residual stream at the start of the layer, so we call it resid_pre\n",
                "# The type annotations are a guide to the reader and are not necessary\n",
                "def residual_stream_patching_hook(\n",
                "    resid_pre: Float[torch.Tensor, \"batch pos d_model\"],\n",
                "    hook: HookPoint,\n",
                "    position: int\n",
                ") -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
                "    # Each HookPoint has a name attribute giving the name of the hook.\n",
                "    clean_resid_pre = clean_cache[hook.name]\n",
                "    resid_pre[:, position, :] = clean_resid_pre[:, position, :]\n",
                "    return resid_pre\n",
                "\n",
                "# We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
                "num_positions = len(clean_tokens[0])\n",
                "ioi_patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
                "\n",
                "for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
                "    for position in range(num_positions):\n",
                "        # Use functools.partial to create a temporary hook function with the position fixed\n",
                "        temp_hook_fn = partial(residual_stream_patching_hook, position=position)\n",
                "        # Run the model with the patching hook\n",
                "        patched_logits = model.run_with_hooks(corrupted_tokens, fwd_hooks=[\n",
                "            (utils.get_act_name(\"resid_pre\", layer), temp_hook_fn)\n",
                "        ])\n",
                "        # Calculate the logit difference\n",
                "        patched_logit_diff = logits_to_logit_diff(patched_logits).detach()\n",
                "        # Store the result, normalizing by the clean and corrupted logit difference so it's between 0 and 1 (ish)\n",
                "        ioi_patching_result[layer, position] = (patched_logit_diff - corrupted_logit_diff)/(clean_logit_diff - corrupted_logit_diff)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can now visualize the results, and see that this computation is extremely localised within the model. Initially, the second subject (Mary) token is all that matters (naturally, as it's the only different token), and all relevant information remains here until heads in layer 7 and 8 move this to the final token where it's used to predict the indirect object.\n",
                "(Note - the heads are in layer 7 and 8, not 8 and 9, because we patched in the residual stream at the *start* of each layer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"fa7a74b6-88e8-43ef-875f-dfc636e889df\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"fa7a74b6-88e8-43ef-875f-dfc636e889df\")) {                    Plotly.newPlot(                        \"fa7a74b6-88e8-43ef-875f-dfc636e889df\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"\\u003c|endoftext|\\u003e_0\",\"After_1\",\" John_2\",\" and_3\",\" Mary_4\",\" went_5\",\" to_6\",\" the_7\",\" store_8\",\",_9\",\" Mary_10\",\" gave_11\",\" a_12\",\" bottle_13\",\" of_14\",\" milk_15\",\" to_16\"],\"z\":{\"dtype\":\"f4\",\"bdata\":\"AAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAgGkYPgAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAICocdk+AAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgBfMNj8AAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACARIxcPwAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAICsulk\\u002fAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgI6UST8AAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAYzUtPwAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIB\\u002f0js\\u002fAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgIOTZD8AAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAbshXPwAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIAAAACAAAAAgAAAAIBz2lo\\u002f\",\"shape\":\"12, 17\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Position: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Position\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Normalized Logit Difference After Patching Residual Stream on the IOI Task\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('fa7a74b6-88e8-43ef-875f-dfc636e889df');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Add the index to the end of the label, because plotly doesn't like duplicate labels\n",
                "token_labels = [f\"{token}_{index}\" for index, token in enumerate(model.to_str_tokens(clean_tokens))]\n",
                "imshow(ioi_patching_result, x=token_labels, xaxis=\"Position\", yaxis=\"Layer\", title=\"Normalized Logit Difference After Patching Residual Stream on the IOI Task\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hooks: Accessing Activations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hooks can also be used to just **access** an activation - to run some function using that activation value, *without* changing the activation value. This can be achieved by just having the hook return nothing, and not editing the activation in place. \n",
                "\n",
                "This is useful for eg extracting activations for a specific task, or for doing some long-running calculation across many inputs, eg finding the text that most activates a specific neuron. (Note - everything this can do *could* be done with `run_with_cache` and post-processing, but this workflow can be more intuitive and memory efficient.)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To demonstrate this, let's look for **[induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)** in GPT-2 Small. \n",
                "\n",
                "Induction circuits are a very important circuit in generative language models, which are used to detect and continue repeated subsequences. They consist of two heads in separate layers that compose together, a **previous token head** which always attends to the previous token, and an **induction head** which attends to the token *after* an earlier copy of the current token. \n",
                "\n",
                "To see why this is important, let's say that the model is trying to predict the next token in a news article about Michael Jordan. The token \" Michael\", in general, could be followed by many surnames. But an induction head will look from that occurrence of \" Michael\" to the token after previous occurrences of \" Michael\", ie \" Jordan\" and can confidently predict that that will come next."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "An interesting fact about induction heads is that they generalise to arbitrary sequences of repeated tokens. We can see this by generating sequences of 50 random tokens, repeated twice, and plotting the average loss at predicting the next token, by position. We see that the model goes from terrible to very good at the halfway point."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"f966f82d-cd67-4049-a541-ecf3c0425d0c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"f966f82d-cd67-4049-a541-ecf3c0425d0c\")) {                    Plotly.newPlot(                        \"f966f82d-cd67-4049-a541-ecf3c0425d0c\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":{\"dtype\":\"i1\",\"bdata\":\"AAECAwQFBgcICQoLDA0ODxAREhMUFRYXGBkaGxwdHh8gISIjJCUmJygpKissLS4vMDEyMzQ1Njc4OTo7PD0+P0BBQkNERUZHSElKS0xNTk9QUVJTVFVWV1hZWltcXV5fYGFi\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"f4\",\"bdata\":\"nT9rQXaLfUE6KG9BrmBwQbuhcEFrX4FBA5d0QbaMdUEmQnlBF7iZQZAabkHAvXNBOkR4QbN0dEEtZWdBKuZ9QV1Zh0FaMIFBaoptQUapZ0F653BBfSJhQdKqaEGAtoVBDmOCQamagUFw\\u002f2tBvhB0QSYtdkG2uohBPTl1Qdfgh0Hq8XRBW4trQaomd0E2IIVB4HqAQSflhkGwfHZBb8iBQcpBdkGepnBB3b2AQfgVZEEN+HFB4PR4QXcthkEaBG9B\\u002fgJtQTjgZEGNQ2pBNu98QeP2bkEb4m9B4xxwQW5GgUHdoXRBLot1QagEeUGykJlBsv1tQTOpc0FgJHhBPi10Qd1fZ0Ht3X1BizuHQeoYgUHDXG1BxqFnQUuUcEH2BmFBi51oQYqohUEbbIJBGpGBQaP7a0G94XNBPRV2QcWuiEFSA3VBju2HQVCydEELgGtBXQ53QT8IhUFLgIBBevuGQaBydkGVr4FB4kB2QeqscEHqtYBB3vZjQVD1cUFL8XhBgyGGQVPqbkGyIm1B\"},\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss by position on random repeated tokens\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('f966f82d-cd67-4049-a541-ecf3c0425d0c');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "batch_size = 10\n",
                "seq_len = 50\n",
                "size = (batch_size, seq_len)\n",
                "input_tensor = torch.randint(1000, 10000, size)\n",
                "\n",
                "random_tokens = input_tensor.to(model.cfg.device)\n",
                "repeated_tokens = einops.repeat(random_tokens, \"batch seq_len -> batch (2 seq_len)\")\n",
                "repeated_logits = model(repeated_tokens)\n",
                "correct_log_probs = model.loss_fn(repeated_logits, repeated_tokens, per_token=True)\n",
                "loss_by_position = einops.reduce(correct_log_probs, \"batch position -> position\", \"mean\")\n",
                "line(loss_by_position, xaxis=\"Position\", yaxis=\"Loss\", title=\"Loss by position on random repeated tokens\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The induction heads will be attending from the second occurrence of each token to the token *after* its first occurrence, ie the token `50-1==49` places back. So by looking at the average attention paid 49 tokens back, we can identify induction heads! Let's define a hook to do this!\n",
                "\n",
                "<details><summary>Technical details</summary>\n",
                "\n",
                "* We attach the hook to the attention pattern activation. There's one big pattern activation per layer, stacked across all heads, so we need to do some tensor manipulation to get a per-head score. \n",
                "* Hook functions can access global state, so we make a big tensor to store the induction head score for each head, and then we just add the score for each head to the appropriate position in the tensor. \n",
                "* To get a single hook function that works for each layer, we use the `hook.layer()` method to get the layer index (internally this is just inferred from the hook names).\n",
                "* As we want to add this to *every* activation pattern hook point, rather than giving the string for an activation name, this time we give a **name filter**. This is a Boolean function on hook point names, and it adds the hook function to every hook point where the function evaluates as true. \n",
                "    * `run_with_hooks` allows us to enter a list of (act_name, hook_function) pairs to all be added at once, so we could also have done this by inputting a list with a hook for each layer.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"1edaa2b2-34be-4de5-8930-3448f6b9d10b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"1edaa2b2-34be-4de5-8930-3448f6b9d10b\")) {                    Plotly.newPlot(                        \"1edaa2b2-34be-4de5-8930-3448f6b9d10b\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":{\"dtype\":\"f4\",\"bdata\":\"snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8snkkPLJ5JDyyeSQ8\",\"shape\":\"12, 12\"},\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Induction Score by Head\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('1edaa2b2-34be-4de5-8930-3448f6b9d10b');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# We make a tensor to store the induction score for each head. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
                "induction_score_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
                "def induction_score_hook(\n",
                "    pattern: Float[torch.Tensor, \"batch head_index dest_pos source_pos\"],\n",
                "    hook: HookPoint,\n",
                "):\n",
                "    # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
                "    # (This only has entries for tokens with index>=seq_len)\n",
                "    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len)\n",
                "    # Get an average score per head\n",
                "    induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
                "    # Store the result.\n",
                "    induction_score_store[hook.layer(), :] = induction_score\n",
                "\n",
                "# We make a boolean filter on activation names, that's true only on attention pattern names.\n",
                "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
                "\n",
                "model.run_with_hooks(\n",
                "    repeated_tokens, \n",
                "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
                "    fwd_hooks=[(\n",
                "        pattern_hook_names_filter,\n",
                "        induction_score_hook\n",
                "    )]\n",
                ")\n",
                "\n",
                "imshow(induction_score_store, xaxis=\"Head\", yaxis=\"Layer\", title=\"Induction Score by Head\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Head 5 in Layer 5 scores extremely highly on this score, and we can feed in a shorter repeated random sequence, visualize the attention pattern for it and see this directly - including the \"induction stripe\" at `seq_len-1` tokens back.\n",
                "\n",
                "This time we put in a hook on the attention pattern activation to visualize the pattern of the relevant head."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "ename": "IndexError",
                    "evalue": "too many indices for tensor of dimension 3",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[21], line 22\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvisualize_pattern_hook\u001b[39m(\n\u001b[1;32m     12\u001b[0m     pattern: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch head_index dest_pos source_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     13\u001b[0m     hook: HookPoint,\n\u001b[1;32m     14\u001b[0m ):\n\u001b[1;32m     15\u001b[0m     display(\n\u001b[1;32m     16\u001b[0m         cv\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mattention_patterns(\n\u001b[1;32m     17\u001b[0m             tokens\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto_str_tokens(repeated_random_sequence), \n\u001b[1;32m     18\u001b[0m             attention\u001b[38;5;241m=\u001b[39mpattern[\u001b[38;5;241m0\u001b[39m, induction_head_index, :, :][\u001b[38;5;28;01mNone\u001b[39;00m, :, :] \u001b[38;5;66;03m# Add a dummy axis, as CircuitsVis expects 3D patterns.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeated_random_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_act_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpattern\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minduction_head_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvisualize_pattern_hook\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:1895\u001b[0m, in \u001b[0;36mTransformerBridge.run_with_hooks\u001b[0;34m(self, input, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, return_type, names_filter, stop_at_layer, remove_batch_dim, **kwargs)\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1895\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m StopAtLayerException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1897\u001b[0m     \u001b[38;5;66;03m# Return the intermediate output from the specified layer\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m     output \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mlayer_output\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/bridge.py:1411\u001b[0m, in \u001b[0;36mTransformerBridge.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, past_kv_cache, attention_mask, start_at_layer, **kwargs)\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1410\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m input_ids\n\u001b[0;32m-> 1411\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:927\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    925\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 927\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/generalized_components/block.py:69\u001b[0m, in \u001b[0;36mBlockBridge.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_in(args[\u001b[38;5;241m0\u001b[39m]),) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 69\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_component\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Handle tuple outputs from transformer blocks\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Apply hook to first element (hidden states) and preserve the rest\u001b[39;00m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:411\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, past_key_value, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    410\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 411\u001b[0m attn_output, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    422\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_output \u001b[38;5;241m+\u001b[39m residual\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/generalized_components/joint_qkv_attention.py:149\u001b[0m, in \u001b[0;36mJointQKVAttentionBridge.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_output(output)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/generalized_components/attention.py:439\u001b[0m, in \u001b[0;36mAttentionBridge.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_component(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# Process output\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/transformer_lens/model_bridge/generalized_components/attention.py:141\u001b[0m, in \u001b[0;36mAttentionBridge._process_output\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be a Tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pattern)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m pattern \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39misnan(pattern), torch\u001b[38;5;241m.\u001b[39mzeros_like(pattern), pattern)\n\u001b[0;32m--> 141\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_pattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Store the pattern for potential use in result calculation\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pattern \u001b[38;5;241m=\u001b[39m pattern\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1857\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1818\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1816\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1818\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1821\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
                        "File \u001b[0;32m~/Documents/VSCODE/TransformerLens/transformer_lens/hook_points.py:124\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m    121\u001b[0m     module_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_conversion\u001b[38;5;241m.\u001b[39mconvert(module_output)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Apply the hook\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Apply output reversion if hook_conversion exists and hook returned a value\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_conversion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "Cell \u001b[0;32mIn[21], line 18\u001b[0m, in \u001b[0;36mvisualize_pattern_hook\u001b[0;34m(pattern, hook)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvisualize_pattern_hook\u001b[39m(\n\u001b[1;32m     12\u001b[0m     pattern: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch head_index dest_pos source_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     13\u001b[0m     hook: HookPoint,\n\u001b[1;32m     14\u001b[0m ):\n\u001b[1;32m     15\u001b[0m     display(\n\u001b[1;32m     16\u001b[0m         cv\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mattention_patterns(\n\u001b[1;32m     17\u001b[0m             tokens\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto_str_tokens(repeated_random_sequence), \n\u001b[0;32m---> 18\u001b[0m             attention\u001b[38;5;241m=\u001b[39m\u001b[43mpattern\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minduction_head_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m, :, :] \u001b[38;5;66;03m# Add a dummy axis, as CircuitsVis expects 3D patterns.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m     )\n",
                        "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
                    ]
                }
            ],
            "source": [
                "if IN_GITHUB:\n",
                "    torch.manual_seed(50)\n",
                "    \n",
                "induction_head_layer = 5\n",
                "induction_head_index = 5\n",
                "size = (1, 20)\n",
                "input_tensor = torch.randint(1000, 10000, size)\n",
                "\n",
                "single_random_sequence = input_tensor.to(model.cfg.device)\n",
                "repeated_random_sequence = einops.repeat(single_random_sequence, \"batch seq_len -> batch (2 seq_len)\")\n",
                "def visualize_pattern_hook(\n",
                "    pattern: Float[torch.Tensor, \"batch head_index dest_pos source_pos\"],\n",
                "    hook: HookPoint,\n",
                "):\n",
                "    display(\n",
                "        cv.attention.attention_patterns(\n",
                "            tokens=model.to_str_tokens(repeated_random_sequence), \n",
                "            attention=pattern[0, induction_head_index, :, :][None, :, :] # Add a dummy axis, as CircuitsVis expects 3D patterns.\n",
                "        )\n",
                "    )\n",
                "\n",
                "model.run_with_hooks(\n",
                "    repeated_random_sequence, \n",
                "    return_type=None, \n",
                "    fwd_hooks=[(\n",
                "        utils.get_act_name(\"pattern\", induction_head_layer), \n",
                "        visualize_pattern_hook\n",
                "    )]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Available Models"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TransformerLens comes with over 40 open source models available, all of which can be loaded into a consistent(-ish) architecture by just changing the name in `from_pretrained`. The open source models available are [documented here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=jHj79Pj58cgJKdq4t-ygK-4h), and a set of interpretability friendly models I've trained are [documented here](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=NCJ6zH_Okw_mUYAwGnMKsj2m), including a set of toy language models (tiny one to four layer models) and a set of [SoLU models](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=FZ5W6GGcy6OitPEaO733JLqf) up to GPT-2 Medium size (300M parameters). You can see [a table of the official alias and hyper-parameters of available models here](https://github.com/TransformerLensOrg/TransformerLens/blob/main/transformer_lens/model_properties_table.md).\n",
                "\n",
                "**Note:** TransformerLens does not currently support multi-GPU models (which you want for models above eg 7B parameters), but this feature is coming soon!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Notably, this means that analysis can be near immediately re-run on a different model by just changing the name - to see this, let's load in DistilGPT-2 (a distilled version of GPT-2, with half as many layers) and copy the code from above to see the induction heads in that model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Using pad_token, but it is not set yet.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded pretrained model distilgpt2 into HookedTransformer\n"
                    ]
                }
            ],
            "source": [
                "# NBVAL_IGNORE_OUTPUT\n",
                "distilgpt2 = HookedTransformer.from_pretrained(\"distilgpt2\", device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"378fa384-2cc7-4a1c-b871-c643c0e527e8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"378fa384-2cc7-4a1c-b871-c643c0e527e8\")) {                    Plotly.newPlot(                        \"378fa384-2cc7-4a1c-b871-c643c0e527e8\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.009922467172145844,0.00014764934894628823,0.011443736031651497,3.4372510526736733e-06,0.0006867930642329156,5.491139290825231e-06,0.008899171836674213,0.0014873683685436845,0.008349093608558178,0.00954477023333311,0.009120927192270756,0.01596890762448311],[0.003026863094419241,0.018020611256361008,0.003568781539797783,0.0006068727816455066,0.013361244462430477,0.002166191814467311,0.005081023555248976,0.01559095922857523,0.0044562919065356255,8.614760008640587e-05,0.003938235342502594,0.014764327555894852],[0.010219043120741844,0.00780068663880229,0.011151125654578209,0.0025892923586070538,0.019908905029296875,0.005215638317167759,0.00943383015692234,0.0015526963397860527,0.017547965049743652,0.011387993581593037,0.021231677383184433,1.451456820402222e-13],[0.007366393692791462,0.22952795028686523,0.8673726916313171,0.016448020935058594,0.01666181907057762,0.011658817529678345,0.01942884363234043,0.20720870792865753,0.014285862445831299,0.016440654173493385,0.9371501803398132,0.49821725487709045],[0.27513042092323303,0.23388457298278809,0.08511187136173248,0.010210886597633362,0.08357562869787216,0.02389742061495781,0.6404961943626404,0.025386787950992584,0.06823859363794327,0.651100218296051,0.015778401866555214,0.06186722218990326],[0.021418118849396706,0.07837661355733871,0.05457017943263054,0.01195995882153511,0.03577948361635208,0.17681372165679932,0.07745278626680374,0.10316655784845352,0.0073539442382752895,0.4502670466899872,0.19098441302776337,0.029486285522580147]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Induction Score by Head in Distil GPT-2\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('378fa384-2cc7-4a1c-b871-c643c0e527e8');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "\n",
                "# We make a tensor to store the induction score for each head. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
                "distilgpt2_induction_score_store = torch.zeros((distilgpt2.cfg.n_layers, distilgpt2.cfg.n_heads), device=distilgpt2.cfg.device)\n",
                "def induction_score_hook(\n",
                "    pattern: Float[torch.Tensor, \"batch head_index dest_pos source_pos\"],\n",
                "    hook: HookPoint,\n",
                "):\n",
                "    # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
                "    # (This only has entries for tokens with index>=seq_len)\n",
                "    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len)\n",
                "    # Get an average score per head\n",
                "    induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
                "    # Store the result.\n",
                "    distilgpt2_induction_score_store[hook.layer(), :] = induction_score\n",
                "\n",
                "# We make a boolean filter on activation names, that's true only on attention pattern names.\n",
                "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
                "\n",
                "distilgpt2.run_with_hooks(\n",
                "    repeated_tokens, \n",
                "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
                "    fwd_hooks=[(\n",
                "        pattern_hook_names_filter,\n",
                "        induction_score_hook\n",
                "    )]\n",
                ")\n",
                "\n",
                "imshow(distilgpt2_induction_score_store, xaxis=\"Head\", yaxis=\"Layer\", title=\"Induction Score by Head in Distil GPT-2\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### An overview of the important open source models in the library\n",
                "\n",
                "* **GPT-2** - the classic generative pre-trained models from OpenAI\n",
                "    * Sizes Small (85M), Medium (300M), Large (700M) and XL (1.5B).\n",
                "    * Trained on ~22B tokens of internet text. ([Open source replication](https://huggingface.co/datasets/openwebtext))\n",
                "* **GPT-Neo** - Eleuther's replication of GPT-2\n",
                "    * Sizes 125M, 1.3B, 2.7B\n",
                "    * Trained on 300B(ish?) tokens of [the Pile](https://pile.eleuther.ai/) a large and diverse dataset including a bunch of code (and weird stuff)\n",
                "* **[OPT](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/)** - Meta AI's series of open source models\n",
                "    * Trained on 180B tokens of diverse text.\n",
                "    * 125M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B\n",
                "* **GPT-J** - Eleuther's 6B parameter model, trained on the Pile\n",
                "* **GPT-NeoX** - Eleuther's 20B parameter model, trained on the Pile\n",
                "* **StableLM** - Stability AI's 3B and 7B models, with and without chat and instruction fine-tuning\n",
                "* **Stanford CRFM models** - a replication of GPT-2 Small and GPT-2 Medium, trained on 5 different random seeds.\n",
                "    * Notably, 600 checkpoints were taken during training per model, and these are available in the library with eg `HookedTransformer.from_pretrained(\"stanford-gpt2-small-a\", checkpoint_index=265)`.\n",
                "- **BERT** - Google's bidirectional encoder-only transformer.\n",
                "    - Size Base (108M), trained on English Wikipedia and BooksCorpus.\n",
                " \n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "### An overview of some interpretability-friendly models I've trained and included\n",
                "\n",
                "(Feel free to [reach out](mailto:neelnanda27@gmail.com) if you want more details on any of these models)\n",
                "\n",
                "Each of these models has about ~200 checkpoints taken during training that can also be loaded from TransformerLens, with the `checkpoint_index` argument to `from_pretrained`.\n",
                "\n",
                "Note that all models are trained with a Beginning of Sequence token, and will likely break if given inputs without that! \n",
                "\n",
                "* **Toy Models**: Inspired by [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html), I've trained 12 tiny language models, of 1-4L and each of width 512. I think that interpreting these is likely to be far more tractable than larger models, and both serve as good practice and will likely contain motifs and circuits that generalise to far larger models (like induction heads):\n",
                "    * Attention-Only models (ie without MLPs): attn-only-1l, attn-only-2l, attn-only-3l, attn-only-4l\n",
                "    * GELU models (ie with MLP, and the standard GELU activations): gelu-1l, gelu-2l, gelu-3l, gelu-4l\n",
                "    * SoLU models (ie with MLP, and [Anthropic's SoLU activation](https://transformer-circuits.pub/2022/solu/index.html), designed to make MLP neurons more interpretable): solu-1l, solu-2l, solu-3l, solu-4l\n",
                "    * All models are trained on 22B tokens of data, 80% from C4 (web text) and 20% from Python Code\n",
                "    * Models of the same layer size were trained with the same weight initialization and data shuffle, to more directly compare the effect of different activation functions.\n",
                "* **SoLU** models: A larger scan of models trained with [Anthropic's SoLU activation](https://transformer-circuits.pub/2022/solu/index.html), in the hopes that it makes the MLP neuron interpretability easier. \n",
                "    * A scan up to GPT-2 Medium size, trained on 30B tokens of the same data as toy models, 80% from C4 and 20% from Python code. \n",
                "        * solu-6l (40M), solu-8l (100M), solu-10l (200M), solu-12l (340M)\n",
                "    * An older scan up to GPT-2 Medium size, trained on 15B tokens of [the Pile](https://pile.eleuther.ai/)\n",
                "        * solu-1l-pile (13M), solu-2l-pile (13M), solu-4l-pile (13M), solu-6l-pile (40M), solu-8l-pile (100M), solu-10l-pile (200M), solu-12l-pile (340M)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Other Resources:\n",
                "\n",
                "* [Concrete Steps to Get Started in Mechanistic Interpretability](https://neelnanda.io/getting-started): A guide I wrote for how to get involved in mechanistic interpretability, and how to learn the basic skills\n",
                "* [A Comprehensive Mechanistic Interpretability Explainer](https://neelnanda.io/glossary): An overview of concepts in the field and surrounding ideas in ML and transformers, with long digressions to give context and build intuitions.\n",
                "* [Concrete Open Problems in Mechanistic Interpretability](https://neelnanda.io/concrete-open-problems), a doc I wrote giving a long list of open problems in mechanistic interpretability, and thoughts on how to get started on trying to work on them. \n",
                "    * There's a lot of low-hanging fruit in the field, and I expect that many people reading this could use TransformerLens to usefully make progress on some of these!\n",
                "* Other demos:\n",
                "    * **[Exploratory Analysis Demo](https://neelnanda.io/exploratory-analysis-demo)**, a demonstration of my standard toolkit for how to use TransformerLens to explore a mysterious behaviour in a language model.\n",
                "    * [Interpretability in the Wild](https://github.com/redwoodresearch/Easy-Transformer) a codebase from Arthur Conmy and Alex Variengien at Redwood research using this library to do a detailed and rigorous reverse engineering of the Indirect Object Identification circuit, to accompany their paper\n",
                "        * Note - this was based on an earlier version of this library, called EasyTransformer. It's pretty similar, but several breaking changes have been made since. \n",
                "    * A [recorded walkthrough](https://www.youtube.com/watch?v=yo4QvDn-vsU) of me doing research with TransformerLens on whether a tiny model can re-derive positional information, with [an accompanying Colab](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/No_Position_Experiment.ipynb)\n",
                "* [Neuroscope](https://neuroscope.io), a website showing the text in the dataset that most activates each neuron in some selected models. Good to explore to get a sense for what kind of features the model tends to represent, and as a \"wiki\" to get some info\n",
                "    * A tutorial on how to make an [Interactive Neuroscope](https://github.com/TransformerLensOrg/TransformerLens/blob/main/Hacky-Interactive-Lexoscope.ipynb), where you type in text and see the neuron activations over the text update live."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transformer architecture\n",
                "\n",
                "HookedTransformer is a somewhat adapted GPT-2 architecture, but is computationally identical. The most significant changes are to the internal structure of the attention heads: \n",
                "* The weights (W_K, W_Q, W_V) mapping the residual stream to queries, keys and values are 3 separate matrices, rather than big concatenated one.\n",
                "* The weight matrices (W_K, W_Q, W_V, W_O) and activations (keys, queries, values, z (values mixed by attention pattern)) have separate head_index and d_head axes, rather than flattening them into one big axis.\n",
                "    * The activations all have shape `[batch, position, head_index, d_head]`\n",
                "    * W_K, W_Q, W_V have shape `[head_index, d_model, d_head]` and W_O has shape `[head_index, d_head, d_model]`\n",
                "\n",
                "The actual code is a bit of a mess, as there's a variety of Boolean flags to make it consistent with the various different model families in TransformerLens - to understand it and the internal structure, I instead recommend reading the code in [CleanTransformerDemo](https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Parameter Names\n",
                "\n",
                "Here is a list of the parameters and shapes in the model. By convention, all weight matrices multiply on the right (ie `new_activation = old_activation @ weights + bias`). \n",
                "\n",
                "Reminder of the key hyper-params:\n",
                "* `n_layers`: 12. The number of transformer blocks in the model (a block contains an attention layer and an MLP layer)\n",
                "* `n_heads`: 12. The number of attention heads per attention layer\n",
                "* `d_model`: 768. The residual stream width.\n",
                "* `d_head`: 64. The internal dimension of an attention head activation.\n",
                "* `d_mlp`: 3072. The internal dimension of the MLP layers (ie the number of neurons).\n",
                "* `d_vocab`: 50267. The number of tokens in the vocabulary.\n",
                "* `n_ctx`: 1024. The maximum number of tokens in an input prompt.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Transformer Block parameters:** \n",
                "Replace 0 with the relevant layer index."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "blocks.0.attn.W_Q torch.Size([12, 768, 64])\n",
                        "blocks.0.attn.W_O torch.Size([12, 64, 768])\n",
                        "blocks.0.attn.b_Q torch.Size([12, 64])\n",
                        "blocks.0.attn.b_O torch.Size([768])\n",
                        "blocks.0.attn.W_K torch.Size([12, 768, 64])\n",
                        "blocks.0.attn.W_V torch.Size([12, 768, 64])\n",
                        "blocks.0.attn.b_K torch.Size([12, 64])\n",
                        "blocks.0.attn.b_V torch.Size([12, 64])\n",
                        "blocks.0.mlp.W_in torch.Size([768, 3072])\n",
                        "blocks.0.mlp.b_in torch.Size([3072])\n",
                        "blocks.0.mlp.W_out torch.Size([3072, 768])\n",
                        "blocks.0.mlp.b_out torch.Size([768])\n"
                    ]
                }
            ],
            "source": [
                "for name, param in model.named_parameters():\n",
                "    if name.startswith(\"blocks.0.\"):\n",
                "        print(name, param.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Embedding & Unembedding parameters:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "embed.W_E torch.Size([50257, 768])\n",
                        "pos_embed.W_pos torch.Size([1024, 768])\n",
                        "unembed.W_U torch.Size([768, 50257])\n",
                        "unembed.b_U torch.Size([50257])\n"
                    ]
                }
            ],
            "source": [
                "for name, param in model.named_parameters():\n",
                "    if not name.startswith(\"blocks\"):\n",
                "        print(name, param.shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Activation + Hook Names\n",
                "\n",
                "Lets get out a list of the activation/hook names in the model and their shapes. In practice, I recommend using the `utils.get_act_name` function to get the names, but this is a useful fallback, and necessary to eg write a name filter function.\n",
                "\n",
                "Let's do this by entering in a short, 10 token prompt, and add a hook function to each activations to print its name and shape. To avoid spam, let's just add this to activations in the first block or not in a block.\n",
                "\n",
                "Note 1: Each LayerNorm has a hook for the scale factor (ie the standard deviation of the input activations for each token position & batch element) and for the normalized output (ie the input activation with mean 0 and standard deviation 1, but *before* applying scaling or translating with learned weights). LayerNorm is applied every time a layer reads from the residual stream: `ln1` is the LayerNorm before the attention layer in a block, `ln2` the one before the MLP layer, and `ln_final` is the LayerNorm before the unembed. \n",
                "\n",
                "Note 2: *Every* activation apart from the attention pattern and attention scores has shape beginning with `[batch, position]`. The attention pattern and scores have shape `[batch, head_index, dest_position, source_position]` (the numbers are the same, unless we're using caching)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Num tokens: 10\n",
                        "hook_embed torch.Size([1, 10, 768])\n",
                        "hook_pos_embed torch.Size([1, 10, 768])\n",
                        "blocks.0.hook_resid_pre torch.Size([1, 10, 768])\n",
                        "blocks.0.ln1.hook_scale torch.Size([1, 10, 1])\n",
                        "blocks.0.ln1.hook_normalized torch.Size([1, 10, 768])\n",
                        "blocks.0.ln1.hook_scale torch.Size([1, 10, 1])\n",
                        "blocks.0.ln1.hook_normalized torch.Size([1, 10, 768])\n",
                        "blocks.0.ln1.hook_scale torch.Size([1, 10, 1])\n",
                        "blocks.0.ln1.hook_normalized torch.Size([1, 10, 768])\n",
                        "blocks.0.attn.hook_q torch.Size([1, 10, 12, 64])\n",
                        "blocks.0.attn.hook_k torch.Size([1, 10, 12, 64])\n",
                        "blocks.0.attn.hook_v torch.Size([1, 10, 12, 64])\n",
                        "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 10, 10])\n",
                        "blocks.0.attn.hook_pattern torch.Size([1, 12, 10, 10])\n",
                        "blocks.0.attn.hook_z torch.Size([1, 10, 12, 64])\n",
                        "blocks.0.hook_attn_out torch.Size([1, 10, 768])\n",
                        "blocks.0.hook_resid_mid torch.Size([1, 10, 768])\n",
                        "blocks.0.ln2.hook_scale torch.Size([1, 10, 1])\n",
                        "blocks.0.ln2.hook_normalized torch.Size([1, 10, 768])\n",
                        "blocks.0.mlp.hook_pre torch.Size([1, 10, 3072])\n",
                        "blocks.0.mlp.hook_post torch.Size([1, 10, 3072])\n",
                        "blocks.0.hook_mlp_out torch.Size([1, 10, 768])\n",
                        "blocks.0.hook_resid_post torch.Size([1, 10, 768])\n",
                        "ln_final.hook_scale torch.Size([1, 10, 1])\n",
                        "ln_final.hook_normalized torch.Size([1, 10, 768])\n"
                    ]
                }
            ],
            "source": [
                "test_prompt = \"The quick brown fox jumped over the lazy dog\"\n",
                "print(\"Num tokens:\", len(model.to_tokens(test_prompt)[0]))\n",
                "\n",
                "def print_name_shape_hook_function(activation, hook):\n",
                "    print(hook.name, activation.shape)\n",
                "\n",
                "not_in_late_block_filter = lambda name: name.startswith(\"blocks.0.\") or not name.startswith(\"blocks\")\n",
                "\n",
                "model.run_with_hooks(\n",
                "    test_prompt,\n",
                "    return_type=None,\n",
                "    fwd_hooks=[(not_in_late_block_filter, print_name_shape_hook_function)],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Folding LayerNorm (For the Curious)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "(For the curious - this is an important technical detail that's worth understanding, especially if you have preconceptions about how transformers work, but not necessary to use TransformerLens)\n",
                "\n",
                "LayerNorm is a normalization technique used by transformers, analogous to BatchNorm but more friendly to massive parallelisation. No one *really* knows why it works, but it seems to improve model numerical stability. Unlike BatchNorm, LayerNorm actually changes the functional form of the model, which makes it a massive pain for interpretability! \n",
                "\n",
                "Folding LayerNorm is a technique to make it lower overhead to deal with, and the flags `center_writing_weights` and `fold_ln` in `HookedTransformer.from_pretrained` apply this automatically (they default to True). These simplify the internal structure without changing the weights.\n",
                "\n",
                "Intuitively, LayerNorm acts on each residual stream vector (ie for each batch element and token position) independently, sets their mean to 0 (centering) and standard deviation to 1 (normalizing) (*across* the residual stream dimension - very weird!), and then applies a learned elementwise scaling and translation to each vector.\n",
                "\n",
                "Mathematically, centering is a linear map, normalizing is *not* a linear map, and scaling and translation are linear maps. \n",
                "* **Centering:** LayerNorm is applied every time a layer reads from the residual stream, so the mean of any residual stream vector can never matter - `center_writing_weights` set every weight matrix writing to the residual to have zero mean. \n",
                "* **Normalizing:** Normalizing is not a linear map, and cannot be factored out. The `hook_scale` hook point lets you access and control for this.\n",
                "* **Scaling and Translation:** Scaling and translation are linear maps, and are always followed by another linear map. The composition of two linear maps is another linear map, so we can *fold* the scaling and translation weights into the weights of the subsequent layer, and simplify things without changing the underlying computation. \n",
                "\n",
                "[See the docs for more details](https://github.com/TransformerLensOrg/TransformerLens/blob/main/further_comments.md#what-is-layernorm-folding-fold_ln)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A fun consequence of LayerNorm folding is that it creates a bias across the unembed, a `d_vocab` length vector that is added to the output logits - GPT-2 is not trained with this, but it *is* trained with a final LayerNorm that contains a bias. \n",
                "\n",
                "Turns out, this LayerNorm bias learns structure of the data that we can only see after folding! In particular, it essentially learns **unigram statistics** - rare tokens get suppressed, common tokens get boosted, by pretty dramatic degrees! Let's list the top and bottom 20 - at the top we see common punctuation and words like \" the\" and \" and\", at the bottom we see weird-ass tokens like \" RandomRedditor\":"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "unembed_bias = model.unembed.b_U\n",
                "bias_values, bias_indices = unembed_bias.sort(descending=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top 20 values\n",
                        "7.03 ','\n",
                        "6.98 ' the'\n",
                        "6.68 ' and'\n",
                        "6.49 '.'\n",
                        "6.48 '\\n'\n",
                        "6.47 ' a'\n",
                        "6.41 ' in'\n",
                        "6.25 ' to'\n",
                        "6.16 ' of'\n",
                        "6.04 '-'\n",
                        "6.03 ' ('\n",
                        "5.88 ' \"'\n",
                        "5.80 ' for'\n",
                        "5.72 ' that'\n",
                        "5.64 ' on'\n",
                        "5.59 ' is'\n",
                        "5.52 ' as'\n",
                        "5.49 ' at'\n",
                        "5.45 ' with'\n",
                        "5.44 ' or'\n",
                        "...\n",
                        "Bottom 20 values\n",
                        "-3.82 ' '\n",
                        "-3.83 '\\x18'\n",
                        "-3.83 '\\x14'\n",
                        "-3.83 ' RandomRedditor'\n",
                        "-3.83 ''\n",
                        "-3.83 ''\n",
                        "-3.83 '\\x1b'\n",
                        "-3.83 ''\n",
                        "-3.83 '\\x05'\n",
                        "-3.83 '\\x00'\n",
                        "-3.83 '\\x06'\n",
                        "-3.83 '\\x07'\n",
                        "-3.83 '\\x0c'\n",
                        "-3.83 '\\x02'\n",
                        "-3.83 'oreAndOnline'\n",
                        "-3.84 '\\x11'\n",
                        "-3.84 ''\n",
                        "-3.84 '\\x10'\n",
                        "-3.84 ''\n",
                        "-3.84 ''\n"
                    ]
                }
            ],
            "source": [
                "top_k = 20\n",
                "print(f\"Top {top_k} values\")\n",
                "for i in range(top_k):\n",
                "    print(f\"{bias_values[i].item():.2f} {repr(model.to_string(bias_indices[i]))}\")\n",
                "\n",
                "print(\"...\")\n",
                "print(f\"Bottom {top_k} values\")\n",
                "for i in range(top_k, 0, -1):\n",
                "    print(f\"{bias_values[-i].item():.2f} {repr(model.to_string(bias_indices[-i]))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This can have real consequences for interpretability - for example, this bias favours \" John\" over \" Mary\" by about 1.2, about 1/3 of the effect size of the Indirect Object Identification Circuit! All other things being the same, this makes the John token 3.6x times more likely than the Mary token."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "John bias: 2.8995\n",
                        "Mary bias: 1.6034\n",
                        "Prob ratio bias: 3.6550x\n"
                    ]
                }
            ],
            "source": [
                "john_bias = model.unembed.b_U[model.to_single_token(' John')]\n",
                "mary_bias = model.unembed.b_U[model.to_single_token(' Mary')]\n",
                "\n",
                "print(f\"John bias: {john_bias.item():.4f}\")\n",
                "print(f\"Mary bias: {mary_bias.item():.4f}\")\n",
                "print(f\"Prob ratio bias: {torch.exp(john_bias - mary_bias).item():.4f}x\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Features\n",
                "\n",
                "An overview of some other important features of the library. I recommend checking out the [Exploratory Analysis Demo](https://colab.research.google.com/github/TransformerLensOrg/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb) for some other important features not mentioned here, and for a demo of what using the library in practice looks like."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dealing with tokens\n",
                "\n",
                "**Tokenization** is one of the most annoying features of studying language models. We want language models to be able to take in arbitrary text as input, but the transformer architecture needs the inputs to be elements of a fixed, finite vocabulary. The solution to this is **tokens**, a fixed vocabulary of \"sub-words\", that any natural language can be broken down into with a **tokenizer**. This is invertible, and we can recover the original text, called **de-tokenization**. \n",
                "\n",
                "TransformerLens comes with a range of utility functions to deal with tokenization. Different models can have different tokenizers, so these are all methods on the model.\n",
                "\n",
                "get_token_position, to_tokens, to_string, to_str_tokens, prepend_bos, to_single_token"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\n",
                "\n",
                "Some observations - there are a lot of arbitrary-ish details in here!\n",
                "* The tokenizer splits on spaces, so no token contains two words.\n",
                "* Tokens include the preceding space, and whether the first token is a capital letter. `how` and ` how` are different tokens!\n",
                "* Common words are single tokens, even if fairly long (` paragraph`) while uncommon words are split into multiple tokens (` token|ized`).\n",
                "* Tokens *mostly* split on punctuation characters (eg `*` and `.`), but eg `'s` is a single token."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['<|endoftext|>', 'The', ' first', ' thing', ' you', ' need', ' to', ' figure', ' out', ' is', ' *', 'how', '*', ' things', ' are', ' token', 'ized', '.', ' `', 'model', '.', 'to', '_', 'str', '_', 't', 'ok', 'ens', '`', ' splits', ' a', ' string', ' into', ' the', ' tokens', ' *', 'as', ' a', ' list', ' of', ' sub', 'strings', '*,', ' and', ' so', ' lets', ' you', ' explore', ' what', ' the', ' text', ' looks', ' like', '.', ' To', ' demonstrate', ' this', ',', ' let', \"'s\", ' use', ' it', ' on', ' this', ' paragraph', '.']\n"
                    ]
                }
            ],
            "source": [
                "example_text = \"The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\"\n",
                "example_text_str_tokens = model.to_str_tokens(example_text)\n",
                "print(example_text_str_tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The transformer needs to take in a sequence of integers, not strings, so we need to convert these tokens into integers. `model.to_tokens` does this, and returns a tensor of integers on the model's device (shape `[batch, position]`). It maps a string to a batch of size 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[50256,   464,   717,  1517,   345,   761,   284,  3785,   503,   318,\n",
                        "          1635,  4919,     9,  1243,   389, 11241,  1143,    13,  4600, 19849,\n",
                        "            13,  1462,    62,  2536,    62,    83,   482,   641,    63, 30778,\n",
                        "           257,  4731,   656,   262, 16326,  1635,   292,   257,  1351,   286,\n",
                        "           850, 37336, 25666,   290,   523,  8781,   345,  7301,   644,   262,\n",
                        "          2420,  3073,   588,    13,  1675, 10176,   428,    11,  1309,   338,\n",
                        "           779,   340,   319,   428,  7322,    13]])\n"
                    ]
                }
            ],
            "source": [
                "example_text_tokens = model.to_tokens(example_text)\n",
                "print(example_text_tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`to_tokens` can also take in a list of strings, and return a batch of size `len(strings)`. If the strings are different numbers of tokens, it adds a PAD token to the end of the shorter strings to make them the same length.\n",
                "\n",
                "(Note: In GPT-2, 50256 signifies both the beginning of sequence, end of sequence and padding token - see the `prepend_bos` section for details)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([[50256,   464,  3797,  3332,   319,   262,  2603,    13, 50256, 50256],\n",
                        "        [50256,   464,  3797,  3332,   319,   262,  2603,  1107,  1327,    13]])\n"
                    ]
                }
            ],
            "source": [
                "example_multi_text = [\"The cat sat on the mat.\", \"The cat sat on the mat really hard.\"]\n",
                "example_multi_text_tokens = model.to_tokens(example_multi_text)\n",
                "print(example_multi_text_tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`model.to_single_token` is a convenience function that takes in a string corresponding to a *single* token and returns the corresponding integer. This is useful for eg looking up the logit corresponding to a single token. \n",
                "\n",
                "For example, let's input `The cat sat on the mat.` to GPT-2, and look at the log prob predicting that the next token is ` The`. \n",
                "\n",
                "<details><summary>Technical notes</summary>\n",
                "\n",
                "Note that if we input a string to the model, it's implicitly converted to a string with `to_tokens`. \n",
                "\n",
                "Note further that the log probs have shape `[batch, position, d_vocab]==[1, 8, 50257]`, with a vector of log probs predicting the next token for *every* token position. GPT-2 uses causal attention which means heads can only look backwards (equivalently, information can only move forwards in the model.), so the log probs at position k are only a function of the first k tokens, and it can't just cheat and look at the k+1 th token. This structure lets it generate text more efficiently, and lets it treat every *token* as a training example, rather than every *sequence*.\n",
                "</details>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Probability tensor shape [batch, position, d_vocab] == torch.Size([1, 8, 50257])\n",
                        "| The| probability: 11.98%\n"
                    ]
                }
            ],
            "source": [
                "cat_text = \"The cat sat on the mat.\"\n",
                "cat_logits = model(cat_text)\n",
                "cat_probs = cat_logits.softmax(dim=-1)\n",
                "print(f\"Probability tensor shape [batch, position, d_vocab] == {cat_probs.shape}\")\n",
                "\n",
                "capital_the_token_index = model.to_single_token(\" The\")\n",
                "print(f\"| The| probability: {cat_probs[0, -1, capital_the_token_index].item():.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`model.to_string` is the inverse of `to_tokens` and maps a tensor of integers to a string or list of strings. It also works on integers and lists of integers.\n",
                "\n",
                "For example, let's look up token 256 (due to technical details of tokenization, this will be the most common pair of ASCII characters!), and also verify that our tokens above map back to a string."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Token 256 - the most common pair of ASCII characters: | t|\n",
                        "De-Tokenizing the example tokens: <|endoftext|>The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Token 256 - the most common pair of ASCII characters: |{model.to_string(256)}|\")\n",
                "# Squeeze means to remove dimensions of length 1. \n",
                "# Here, that removes the dummy batch dimension so it's a rank 1 tensor and returns a string\n",
                "# Rank 2 tensors map to a list of strings\n",
                "print(f\"De-Tokenizing the example tokens: {model.to_string(example_text_tokens.squeeze())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A related annoyance of tokenization is that it's hard to figure out how many tokens a string will break into. `model.get_token_position(single_token, tokens)` returns the position of `single_token` in `tokens`. `tokens` can be either a string or a tensor of tokens. \n",
                "\n",
                "Note that position is zero-indexed, it's two (ie third) because there's a beginning of sequence token automatically prepended (see the next section for details)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "With BOS: 2\n",
                        "Without BOS: 1\n"
                    ]
                }
            ],
            "source": [
                "print(\"With BOS:\", model.get_token_position(\" cat\", \"The cat sat on the mat\"))\n",
                "print(\"Without BOS:\", model.get_token_position(\" cat\", \"The cat sat on the mat\", prepend_bos=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If there are multiple copies of the token, we can set `mode=\"first\"` to find the first occurrence's position and `mode=\"last\"` to find the last"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "First occurrence 2\n",
                        "Final occurrence 13\n"
                    ]
                }
            ],
            "source": [
                "print(\"First occurrence\", model.get_token_position(\n",
                "    \" cat\", \n",
                "    \"The cat sat on the mat. The mat sat on the cat.\", \n",
                "    mode=\"first\"))\n",
                "print(\"Final occurrence\", model.get_token_position(\n",
                "    \" cat\", \n",
                "    \"The cat sat on the mat. The mat sat on the cat.\", \n",
                "    mode=\"last\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In general, tokenization is a pain, and full of gotchas. I highly recommend just playing around with different inputs and their tokenization and getting a feel for it. As another \"fun\" example, let's look at the tokenization of arithmetic expressions - tokens do *not* contain consistent numbers of digits. (This makes it even more impressive that GPT-3 can do arithmetic!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['<|endoftext|>', '23', '42', '+', '2017', '=', '214', '45']\n",
                        "['<|endoftext|>', '1000', '+', '1', '000000', '=', '9999', '99']\n"
                    ]
                }
            ],
            "source": [
                "print(model.to_str_tokens(\"2342+2017=21445\"))\n",
                "print(model.to_str_tokens(\"1000+1000000=999999\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "I also *highly* recommend investigating prompts with easy tokenization when starting out - ideally key words should form a single token, be in the same position in different prompts, have the same total length, etc. Eg study Indirect Object Identification with common English names like ` Tim` rather than ` Ne|el`. Transformers need to spend some parameters in early layers converting multi-token words to a single feature, and then de-converting this in the late layers, and unless this is what you're explicitly investigating, this will make the behaviour you're investigating be messier."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Gotcha: `prepend_bos`\n",
                "\n",
                "Key Takeaway: **If you get weird off-by-one errors, check whether there's an unexpected `prepend_bos`!**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A weirdness you may have noticed in the above is that `to_tokens` and `to_str_tokens` added a weird `<|endoftext|>` to the start of each prompt. TransformerLens does this by default, and it can easily trip up new users. Notably, **this includes `model.forward`** (which is what's implicitly used when you do eg `model(\"Hello World\")`). This is called a **Beginning of Sequence (BOS)** token, and it's a special token used to mark the beginning of the sequence. Confusingly, in GPT-2, the End of Sequence (EOS), Beginning of Sequence (BOS) and Padding (PAD) tokens are all the same, `<|endoftext|>` with index `50256`.\n",
                "\n",
                "**Gotcha:** You only want to prepend a BOS token at the *start* of a prompt. If you, eg, want to input a question followed by an answer, and want to tokenize these separately, you do *not* want to prepend_bos on the answer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Logits shape by default (with BOS) torch.Size([1, 3, 50257])\n",
                        "Logits shape with BOS torch.Size([1, 3, 50257])\n",
                        "Logits shape without BOS - only 2 positions! torch.Size([1, 2, 50257])\n"
                    ]
                }
            ],
            "source": [
                "print(\"Logits shape by default (with BOS)\", model(\"Hello World\").shape)\n",
                "print(\"Logits shape with BOS\", model(\"Hello World\", prepend_bos=True).shape)\n",
                "print(\"Logits shape without BOS - only 2 positions!\", model(\"Hello World\", prepend_bos=False).shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`prepend_bos` is a bit of a hack, and I've gone back and forth on what the correct default here is. The reason I do this is that transformers tend to treat the first token weirdly - this doesn't really matter in training (where all inputs are >1000 tokens), but this can be a big issue when investigating short prompts! The reason for this is that attention patterns are a probability distribution and so need to add up to one, so to simulate being \"off\" they normally look at the first token. Giving them a BOS token lets the heads rest by looking at that, preserving the information in the first \"real\" token.\n",
                "\n",
                "Further, *some* models are trained to need a BOS token (OPT and my interpretability-friendly models are, GPT-2 and GPT-Neo are not). But despite GPT-2 not being trained with this, empirically it seems to make interpretability easier.\n",
                "\n",
                "(However, if you want to change the default behaviour to *not* prepending a BOS token, pass `default_prepend_bos=False` when you instantiate the model, e.g., `model = HookedTransformer.from_pretrained('gpt2', default_prepend_bos=False)`.)\n",
                "\n",
                "For example, the model can get much worse at Indirect Object Identification without a BOS (and with a name as the first token):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Logit difference with BOS: 6.754\n",
                        "Logit difference without BOS: 2.782\n"
                    ]
                }
            ],
            "source": [
                "ioi_logits_with_bos = model(\"Claire and Mary went to the shops, then Mary gave a bottle of milk to\", prepend_bos=True)\n",
                "mary_logit_with_bos = ioi_logits_with_bos[0, -1, model.to_single_token(\" Mary\")].item()\n",
                "claire_logit_with_bos = ioi_logits_with_bos[0, -1, model.to_single_token(\" Claire\")].item()\n",
                "print(f\"Logit difference with BOS: {(claire_logit_with_bos - mary_logit_with_bos):.3f}\")\n",
                "\n",
                "ioi_logits_without_bos = model(\"Claire and Mary went to the shops, then Mary gave a bottle of milk to\", prepend_bos=False)\n",
                "mary_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(\" Mary\")].item()\n",
                "claire_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(\" Claire\")].item()\n",
                "print(f\"Logit difference without BOS: {(claire_logit_without_bos - mary_logit_without_bos):.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Though, note that this also illustrates another gotcha - when `Claire` is at the start of a sentence (no preceding space), it's actually *two* tokens, not one, which probably confuses the relevant circuit. (Note - in this test we put `prepend_bos=False`, because we want to analyse the tokenization of a specific string, not to give an input to the model!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "| Claire| -> [' Claire']\n",
                        "|Claire| -> ['Cl', 'aire']\n"
                    ]
                }
            ],
            "source": [
                "print(f\"| Claire| -> {model.to_str_tokens(' Claire', prepend_bos=False)}\")\n",
                "print(f\"|Claire| -> {model.to_str_tokens('Claire', prepend_bos=False)}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Factored Matrix Class\n",
                "\n",
                "In transformer interpretability, we often need to analyse low rank factorized matrices - a matrix $M = AB$, where M is `[large, large]`, but A is `[large, small]` and B is `[small, large]`. This is a common structure in transformers, and the `FactoredMatrix` class is a convenient way to work with these. It implements efficient algorithms for various operations on these, such as computing the trace, eigenvalues, Frobenius norm, singular value decomposition, and products with other matrices. It can (approximately) act as a drop-in replacement for the original matrix, and supports leading batch dimensions to the factored matrix. \n",
                "\n",
                "<details><summary>Why are low-rank factorized matrices useful for transformer interpretability?</summary>\n",
                "\n",
                "As argued in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html), an unexpected fact about transformer attention heads is that rather than being best understood as keys, queries and values (and the requisite weight matrices), they're actually best understood as two low rank factorized matrices. \n",
                "* **Where to move information from:** $W_QK = W_Q W_K^T$, used for determining the attention pattern - what source positions to move information from and what destination positions to move them to.\n",
                "    * Intuitively, residual stream -> query and residual stream -> key are linear maps, *and* `attention_score = query @ key.T` is a linear map, so the whole thing can be factored into one big bilinear form `residual @ W_QK @ residual.T`\n",
                "* **What information to move:** $W_OV = W_V W_O$, used to determine what information to copy from the source position to the destination position (weighted by the attention pattern weight from that destination to that source). \n",
                "    * Intuitively, the residual stream is a `[position, d_model]` tensor (ignoring batch). The attention pattern acts on the *position* dimension (where to move information from and to) and the value and output weights act on the *d_model* dimension - ie *what* information is contained at that source position. So we can factor it all into `attention_pattern @ residual @ W_V @ W_O`, and so only need to care about `W_OV = W_V @ W_O`\n",
                "* Note - the internal head dimension is smaller than the residual stream dimension, so the factorization is low rank. (here, `d_model=768` and `d_head=64`)\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Basic Examples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can use the basic class directly - let's make a factored matrix directly and look at the basic operations:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Norms:\n",
                        "tensor(9.9105)\n",
                        "tensor(9.9105)\n",
                        "Right dimension: 5, Left dimension: 5, Hidden dimension: 2\n"
                    ]
                }
            ],
            "source": [
                "if IN_GITHUB:\n",
                "    torch.manual_seed(50)\n",
                "A = torch.randn(5, 2)\n",
                "B = torch.randn(2, 5)\n",
                "\n",
                "AB = A @ B\n",
                "AB_factor = FactoredMatrix(A, B)\n",
                "print(\"Norms:\")\n",
                "print(AB.norm())\n",
                "print(AB_factor.norm())\n",
                "\n",
                "print(f\"Right dimension: {AB_factor.rdim}, Left dimension: {AB_factor.ldim}, Hidden dimension: {AB_factor.mdim}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also look at the eigenvalues and singular values of the matrix. Note that, because the matrix is rank 2 but 5 by 5, the final 3 eigenvalues and singular values are zero - the factored class omits the zeros."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Eigenvalues:\n",
                        "tensor([-6.2877e+00+0.j,  1.9337e-07+0.j,  2.3121e+00+0.j, -5.9987e-07+0.j,\n",
                        "        -1.1409e-07+0.j])\n",
                        "tensor([-6.2877+0.j,  2.3121+0.j])\n",
                        "\n",
                        "Singular Values:\n",
                        "tensor([8.3126e+00, 5.3963e+00, 1.4519e-07, 7.4293e-08, 2.1726e-09])\n",
                        "tensor([8.3126, 5.3963])\n"
                    ]
                }
            ],
            "source": [
                "# NBVAL_IGNORE_OUTPUT\n",
                "print(\"Eigenvalues:\")\n",
                "print(torch.linalg.eig(AB).eigenvalues)\n",
                "print(AB_factor.eigenvalues)\n",
                "print()\n",
                "print(\"Singular Values:\")\n",
                "print(torch.linalg.svd(AB).S)\n",
                "print(AB_factor.S)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can multiply with other matrices - it automatically chooses the smallest possible dimension to factor along (here it's 2, rather than 5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unfactored: torch.Size([5, 300]) tensor(160.0830)\n",
                        "Factored: torch.Size([5, 300]) tensor(160.0830)\n",
                        "Right dimension: 300, Left dimension: 5, Hidden dimension: 2\n"
                    ]
                }
            ],
            "source": [
                "if IN_GITHUB:\n",
                "    torch.manual_seed(50)\n",
                "    \n",
                "C = torch.randn(5, 300)\n",
                "\n",
                "ABC = AB @ C\n",
                "ABC_factor = AB_factor @ C\n",
                "print(\"Unfactored:\", ABC.shape, ABC.norm().round(decimals=3))\n",
                "print(\"Factored:\", ABC_factor.shape, ABC_factor.norm().round(decimals=3))\n",
                "print(f\"Right dimension: {ABC_factor.rdim}, Left dimension: {ABC_factor.ldim}, Hidden dimension: {ABC_factor.mdim}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If we want to collapse this back to an unfactored matrix, we can use the AB property to get the product:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor(True)\n"
                    ]
                }
            ],
            "source": [
                "AB_unfactored = AB_factor.AB\n",
                "print(torch.isclose(AB_unfactored, AB).all())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Medium Example: Eigenvalue Copying Scores\n",
                "\n",
                "(This is a more involved example of how to use the factored matrix class, skip it if you aren't following)\n",
                "\n",
                "For a more involved example, let's look at the eigenvalue copying score from [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) of the OV circuit for various heads. The OV Circuit for a head (the factorised matrix $W_OV = W_V W_O$) is a linear map that determines what information is moved from the source position to the destination position. Because this is low rank, it can be thought of as *reading in* some low rank subspace of the source residual stream and *writing to* some low rank subspace of the destination residual stream (with maybe some processing happening in the middle).\n",
                "\n",
                "A common operation for this will just be to *copy*, ie to have the same reading and writing subspace, and to do minimal processing in the middle. Empirically, this tends to coincide with the OV Circuit having (approximately) positive real eigenvalues. I mostly assert this as an empirical fact, but intuitively, operations that involve mapping eigenvectors to different directions (eg rotations) tend to have complex eigenvalues. And operations that preserve eigenvector direction but negate it tend to have negative real eigenvalues. And \"what happens to the eigenvectors\" is a decent proxy for what happens to an arbitrary vector.\n",
                "\n",
                "We can get a score for \"how positive real the OV circuit eigenvalues are\" with $\\frac{\\sum \\lambda_i}{\\sum |\\lambda_i|}$, where $\\lambda_i$ are the eigenvalues of the OV circuit. This is a bit of a hack, but it seems to work well in practice."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's use FactoredMatrix to compute this for every head in the model! We use the helper `model.OV` to get the concatenated OV circuits for all heads across all layers in the model. This has the shape `[n_layers, n_heads, d_model, d_model]`, where `n_layers` and `n_heads` are batch dimensions and the final two dimensions are factorised as `[n_layers, n_heads, d_model, d_head]` and `[n_layers, n_heads, d_head, d_model]` matrices.\n",
                "\n",
                "We can then get the eigenvalues for this, where there are separate eigenvalues for each element of the batch (a `[n_layers, n_heads, d_head]` tensor of complex numbers), and calculate the copying score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "FactoredMatrix: Shape(torch.Size([12, 12, 768, 768])), Hidden Dim(64)\n"
                    ]
                }
            ],
            "source": [
                "OV_circuit_all_heads = model.OV\n",
                "print(OV_circuit_all_heads)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([12, 12, 64])\n",
                        "torch.complex64\n"
                    ]
                }
            ],
            "source": [
                "OV_circuit_all_heads_eigenvalues = OV_circuit_all_heads.eigenvalues \n",
                "print(OV_circuit_all_heads_eigenvalues.shape)\n",
                "print(OV_circuit_all_heads_eigenvalues.dtype)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"a09834af-1357-479f-b958-ca36d205cbf8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a09834af-1357-479f-b958-ca36d205cbf8\")) {                    Plotly.newPlot(                        \"a09834af-1357-479f-b958-ca36d205cbf8\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.7775010466575623,0.3527269959449768,0.25961846113204956,0.6670257449150085,0.8384260535240173,0.5584430694580078,0.8444744944572449,0.4137910008430481,0.24488940834999084,0.028157662600278854,0.3584098219871521,0.16288265585899353],[-0.45419126749038696,-0.6529328227043152,-0.5484569072723389,-0.7990369200706482,-0.7736425995826721,-0.8522581458091736,0.9774324893951416,0.6626249551773071,-0.7303224205970764,-0.7007019519805908,-0.6946625709533691,-0.9996722340583801],[-0.7837163805961609,0.8967759013175964,0.4750954806804657,-0.667197585105896,0.7881461977958679,-0.8547751307487488,-0.9054184556007385,-0.5749384760856628,-0.32175111770629883,-0.028594352304935455,-0.9247617721557617,-0.9699268341064453],[0.5864037275314331,-0.7614347338676453,0.5971695780754089,0.7854393720626831,-0.8788883686065674,0.3908745050430298,0.044738516211509705,0.11028008162975311,-0.8169988989830017,0.22129566967487335,-0.9939578771591187,0.5774399042129517],[0.525479257106781,0.3049013912677765,-0.10729152709245682,0.9433151483535767,-0.9314428567886353,0.5273632407188416,-0.4264712631702423,-0.9984429478645325,0.5296756029129028,0.8604294061660767,-0.8895052075386047,0.9556970596313477],[0.6629186868667603,0.42956963181495667,0.9736858010292053,0.6555483937263489,0.12201889604330063,0.7442770004272461,0.5037952661514282,0.9525359272956848,-0.6507164239883423,-0.9316279292106628,0.9791510105133057,-0.9972584843635559],[0.9613031148910522,0.7501779794692993,-0.3806658685207367,0.6429786682128906,0.9557769298553467,-0.9428839683532715,-0.9948079586029053,0.785298764705658,0.9657301306724548,0.707301676273346,0.3687230050563812,0.8128011226654053],[0.9659481644630432,0.9730121493339539,0.31900617480278015,-0.30290520191192627,0.9790953397750854,0.9357923269271851,-0.5550313591957092,-0.005466493312269449,0.9867776036262512,0.8249565958976746,0.566429615020752,0.1000526174902916],[-0.9464486837387085,-0.25471997261047363,0.6522327661514282,0.1415255218744278,0.9884140491485596,0.9860583543777466,0.6949270367622375,0.9901810884475708,0.9791202545166016,-0.2359553426504135,-0.9820711612701416,0.6506689190864563],[0.9895943999290466,-0.29178157448768616,0.9714024662971497,0.9951602220535278,0.18783769011497498,-0.9460937976837158,0.47801902890205383,-0.2489192932844162,0.9437097907066345,0.11866245418787003,0.9941242933273315,-0.38088178634643555],[0.9564487934112549,0.5542725920677185,0.42118048667907715,0.6628789901733398,0.8659590482711792,0.9937117695808411,0.9069075584411621,0.39811065793037415,-0.4134220480918884,0.9971913695335388,0.34596705436706543,0.9938657283782959],[0.5891268849372864,0.9313740134239197,0.9268401861190796,0.9993564486503601,0.6227539777755737,0.8463947772979736,0.6584346294403076,0.8423126339912415,0.2978496253490448,0.8728679418563843,0.9963144659996033,0.986752450466156]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0,\"cmin\":-1.0,\"cmax\":1.0},\"title\":{\"text\":\"OV Copying Score for each head in GPT-2 Small\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('a09834af-1357-479f-b958-ca36d205cbf8');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "OV_copying_score = OV_circuit_all_heads_eigenvalues.sum(dim=-1).real / OV_circuit_all_heads_eigenvalues.abs().sum(dim=-1)\n",
                "imshow(utils.to_numpy(OV_copying_score), xaxis=\"Head\", yaxis=\"Layer\", title=\"OV Copying Score for each head in GPT-2 Small\", zmax=1.0, zmin=-1.0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Head 11 in Layer 11 (L11H11) has a high copying score, and if we plot the eigenvalues they look approximately as expected."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"1e3711f8-c7bc-42a1-aea4-6c909a93f24e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1e3711f8-c7bc-42a1-aea4-6c909a93f24e\")) {                    Plotly.newPlot(                        \"1e3711f8-c7bc-42a1-aea4-6c909a93f24e\",                        [{\"hovertemplate\":\"Real=%{x}\\u003cbr\\u003eImaginary=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[-2.1397297382354736,1.4152636528015137,3.444455146789551,4.027669906616211,8.882655143737793,4.866769790649414,4.866769790649414,4.843714714050293,4.843714714050293,8.477535247802734,8.216809272766113,8.216809272766113,5.07860803604126,7.855461120605469,7.855461120605469,5.365756034851074,5.365756034851074,5.563426971435547,5.563426971435547,5.4217329025268555,7.769133567810059,7.769133567810059,7.0422773361206055,7.0422773361206055,5.675145626068115,5.675145626068115,7.678577423095703,7.678577423095703,6.573329925537109,6.573329925537109,7.67294979095459,7.17219877243042,7.17219877243042,7.423620223999023,7.423620223999023,7.470810413360596,6.089095115661621,6.089095115661621,6.306834697723389,6.306834697723389,6.511750221252441,6.511750221252441,5.955246448516846,5.955246448516846,5.858811378479004,5.858811378479004,7.147887229919434,7.147887229919434,7.185712814331055,7.185712814331055,6.670608043670654,6.670608043670654,6.735983848571777,6.735983848571777,6.149757385253906,6.149757385253906,6.288776874542236,6.288776874542236,6.344796657562256,6.625571250915527,6.625571250915527,6.8991899490356445,6.8991899490356445,6.85640811920166],\"xaxis\":\"x\",\"y\":[0.0,0.0,0.0,0.0,0.0,0.4185231328010559,-0.4185231328010559,0.09079001098871231,-0.09079001098871231,0.0,0.40868881344795227,-0.40868881344795227,0.0,0.7007214426994324,-0.7007214426994324,0.46421411633491516,-0.46421411633491516,0.5558239817619324,-0.5558239817619324,0.0,0.4705662131309509,-0.4705662131309509,1.0298681259155273,-1.0298681259155273,0.48253530263900757,-0.48253530263900757,0.3356489837169647,-0.3356489837169647,0.9988697171211243,-0.9988697171211243,0.0,0.7531778812408447,-0.7531778812408447,0.4257569909095764,-0.4257569909095764,0.0,0.643626868724823,-0.643626868724823,0.7701709270477295,-0.7701709270477295,0.7558017373085022,-0.7558017373085022,0.25911131501197815,-0.25911131501197815,0.013043955899775028,-0.013043955899775028,0.40166252851486206,-0.40166252851486206,0.28192126750946045,-0.28192126750946045,0.6146255135536194,-0.6146255135536194,0.5391282439231873,-0.5391282439231873,0.28234055638313293,-0.28234055638313293,0.3528330624103546,-0.3528330624103546,0.0,0.24867765605449677,-0.24867765605449677,0.15545639395713806,-0.15545639395713806,0.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Real\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Imaginary\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Eigenvalues of Head L11H11 of GPT-2 Small\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('1e3711f8-c7bc-42a1-aea4-6c909a93f24e');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "scatter(x=OV_circuit_all_heads_eigenvalues[-1, -1, :].real, y=OV_circuit_all_heads_eigenvalues[-1, -1, :].imag, title=\"Eigenvalues of Head L11H11 of GPT-2 Small\", xaxis=\"Real\", yaxis=\"Imaginary\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can even look at the full OV circuit, from the input tokens to output tokens: $W_E W_V W_O W_U$. This is a `[d_vocab, d_vocab]==[50257, 50257]` matrix, so absolutely enormous, even for a single head. But with the FactoredMatrix class, we can compute the full eigenvalue copying score of every head in a few seconds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "FactoredMatrix: Shape(torch.Size([12, 12, 50257, 50257])), Hidden Dim(64)\n"
                    ]
                }
            ],
            "source": [
                "full_OV_circuit = model.embed.W_E @ OV_circuit_all_heads @ model.unembed.W_U\n",
                "print(full_OV_circuit)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "torch.Size([12, 12, 64])\n",
                        "torch.complex64\n"
                    ]
                }
            ],
            "source": [
                "full_OV_circuit_eigenvalues = full_OV_circuit.eigenvalues\n",
                "print(full_OV_circuit_eigenvalues.shape)\n",
                "print(full_OV_circuit_eigenvalues.dtype)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"c709dad6-6ca6-4c75-8231-0ee7b1e7e752\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c709dad6-6ca6-4c75-8231-0ee7b1e7e752\")) {                    Plotly.newPlot(                        \"c709dad6-6ca6-4c75-8231-0ee7b1e7e752\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.8356368541717529,0.5853535532951355,0.5105839967727661,0.7843376398086548,0.8644158840179443,0.7026588320732117,0.8969924449920654,0.5868821740150452,0.4248652160167694,-0.16337503492832184,0.4626856744289398,0.2760537266731262],[-0.05292005464434624,-0.3177315592765808,-0.4810580015182495,-0.783806562423706,-0.6360208988189697,-0.7758680582046509,0.9681803584098816,0.8119115233421326,-0.7510465383529663,-0.6878446340560913,-0.6429886221885681,-0.9985855221748352],[-0.6598327159881592,0.9152501821517944,0.5461500883102417,-0.4874398708343506,0.7720565795898438,-0.7541061639785767,-0.8472450971603394,-0.6948987245559692,-0.1557510942220688,0.24442273378372192,-0.9106623530387878,-0.9439151287078857],[0.6486894488334656,-0.5592910647392273,0.5935594439506531,0.7843042016029358,-0.8150346875190735,0.6130048036575317,0.16785870492458344,0.35195884108543396,-0.6837263107299805,0.22237683832645416,-0.9929219484329224,0.6535818576812744],[0.5740951299667358,0.3640132546424866,0.09609055519104004,0.9359623193740845,-0.9228774309158325,0.6191076636314392,-0.33572638034820557,-0.998464822769165,0.6448631286621094,0.8468661308288574,-0.7557657361030579,0.9527971148490906],[0.7326545715332031,0.532416820526123,0.9732668995857239,0.7239248752593994,0.25538960099220276,0.815841555595398,0.6655788421630859,0.9287101030349731,-0.5660438537597656,-0.890874445438385,0.9834234118461609,-0.9981180429458618],[0.9698693156242371,0.7439671158790588,-0.35639339685440063,0.6022988557815552,0.9708116054534912,-0.9278276562690735,-0.996231734752655,0.8345208168029785,0.9714328050613403,0.8158544898033142,0.5902576446533203,0.8199342489242554],[0.9820225834846497,0.9859328269958496,0.5152459144592285,-0.5610516667366028,0.9663665890693665,0.9495159983634949,-0.5204814076423645,0.3104749917984009,0.9859084486961365,0.7797460556030273,0.6738530397415161,0.3919741213321686],[-0.906204104423523,0.11750980466604233,0.8077875375747681,0.4169303774833679,0.9829014539718628,0.9902303218841553,0.7847102880477905,0.994563102722168,0.9868024587631226,-0.26804423332214355,-0.9908866882324219,0.745792806148529],[0.9906191825866699,-0.18231149017810822,0.97578364610672,0.9986749887466431,0.2544330358505249,-0.954406201839447,0.5869243144989014,-0.23537996411323547,0.9550502896308899,0.25511977076530457,0.9929870963096619,0.09052591770887375],[0.9707273244857788,0.6956093311309814,0.6280022263526917,0.7902867794036865,0.9343841075897217,0.989579439163208,0.9436283707618713,-0.10834993422031403,-0.3431112766265869,0.9986708760261536,0.5086739659309387,0.9949507713317871],[0.8283133506774902,0.9432437419891357,0.9491766095161438,0.9995352029800415,0.5712319612503052,0.8055234551429749,0.6781864166259766,0.8272571563720703,0.8314797282218933,0.8778656721115112,0.9944958686828613,0.9973865151405334]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0,\"cmin\":-1.0,\"cmax\":1.0},\"title\":{\"text\":\"OV Copying Score for each head in GPT-2 Small\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('c709dad6-6ca6-4c75-8231-0ee7b1e7e752');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "full_OV_copying_score = full_OV_circuit_eigenvalues.sum(dim=-1).real / full_OV_circuit_eigenvalues.abs().sum(dim=-1)\n",
                "imshow(utils.to_numpy(full_OV_copying_score), xaxis=\"Head\", yaxis=\"Layer\", title=\"OV Copying Score for each head in GPT-2 Small\", zmax=1.0, zmin=-1.0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Interestingly, these are highly (but not perfectly!) correlated. I'm not sure what to read from this, or what's up with the weird outlier heads!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"3e59929d-31e9-49b1-ad1a-9d9811d4801d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3e59929d-31e9-49b1-ad1a-9d9811d4801d\")) {                    Plotly.newPlot(                        \"3e59929d-31e9-49b1-ad1a-9d9811d4801d\",                        [{\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003eFull OV Copying Score=%{x}\\u003cbr\\u003eOV Copying Score=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"L0H0\",\"L0H1\",\"L0H2\",\"L0H3\",\"L0H4\",\"L0H5\",\"L0H6\",\"L0H7\",\"L0H8\",\"L0H9\",\"L0H10\",\"L0H11\",\"L1H0\",\"L1H1\",\"L1H2\",\"L1H3\",\"L1H4\",\"L1H5\",\"L1H6\",\"L1H7\",\"L1H8\",\"L1H9\",\"L1H10\",\"L1H11\",\"L2H0\",\"L2H1\",\"L2H2\",\"L2H3\",\"L2H4\",\"L2H5\",\"L2H6\",\"L2H7\",\"L2H8\",\"L2H9\",\"L2H10\",\"L2H11\",\"L3H0\",\"L3H1\",\"L3H2\",\"L3H3\",\"L3H4\",\"L3H5\",\"L3H6\",\"L3H7\",\"L3H8\",\"L3H9\",\"L3H10\",\"L3H11\",\"L4H0\",\"L4H1\",\"L4H2\",\"L4H3\",\"L4H4\",\"L4H5\",\"L4H6\",\"L4H7\",\"L4H8\",\"L4H9\",\"L4H10\",\"L4H11\",\"L5H0\",\"L5H1\",\"L5H2\",\"L5H3\",\"L5H4\",\"L5H5\",\"L5H6\",\"L5H7\",\"L5H8\",\"L5H9\",\"L5H10\",\"L5H11\",\"L6H0\",\"L6H1\",\"L6H2\",\"L6H3\",\"L6H4\",\"L6H5\",\"L6H6\",\"L6H7\",\"L6H8\",\"L6H9\",\"L6H10\",\"L6H11\",\"L7H0\",\"L7H1\",\"L7H2\",\"L7H3\",\"L7H4\",\"L7H5\",\"L7H6\",\"L7H7\",\"L7H8\",\"L7H9\",\"L7H10\",\"L7H11\",\"L8H0\",\"L8H1\",\"L8H2\",\"L8H3\",\"L8H4\",\"L8H5\",\"L8H6\",\"L8H7\",\"L8H8\",\"L8H9\",\"L8H10\",\"L8H11\",\"L9H0\",\"L9H1\",\"L9H2\",\"L9H3\",\"L9H4\",\"L9H5\",\"L9H6\",\"L9H7\",\"L9H8\",\"L9H9\",\"L9H10\",\"L9H11\",\"L10H0\",\"L10H1\",\"L10H2\",\"L10H3\",\"L10H4\",\"L10H5\",\"L10H6\",\"L10H7\",\"L10H8\",\"L10H9\",\"L10H10\",\"L10H11\",\"L11H0\",\"L11H1\",\"L11H2\",\"L11H3\",\"L11H4\",\"L11H5\",\"L11H6\",\"L11H7\",\"L11H8\",\"L11H9\",\"L11H10\",\"L11H11\"],\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.8356368541717529,0.5853535532951355,0.5105839967727661,0.7843376398086548,0.8644158840179443,0.7026588320732117,0.8969924449920654,0.5868821740150452,0.4248652160167694,-0.16337503492832184,0.4626856744289398,0.2760537266731262,-0.05292005464434624,-0.3177315592765808,-0.4810580015182495,-0.783806562423706,-0.6360208988189697,-0.7758680582046509,0.9681803584098816,0.8119115233421326,-0.7510465383529663,-0.6878446340560913,-0.6429886221885681,-0.9985855221748352,-0.6598327159881592,0.9152501821517944,0.5461500883102417,-0.4874398708343506,0.7720565795898438,-0.7541061639785767,-0.8472450971603394,-0.6948987245559692,-0.1557510942220688,0.24442273378372192,-0.9106623530387878,-0.9439151287078857,0.6486894488334656,-0.5592910647392273,0.5935594439506531,0.7843042016029358,-0.8150346875190735,0.6130048036575317,0.16785870492458344,0.35195884108543396,-0.6837263107299805,0.22237683832645416,-0.9929219484329224,0.6535818576812744,0.5740951299667358,0.3640132546424866,0.09609055519104004,0.9359623193740845,-0.9228774309158325,0.6191076636314392,-0.33572638034820557,-0.998464822769165,0.6448631286621094,0.8468661308288574,-0.7557657361030579,0.9527971148490906,0.7326545715332031,0.532416820526123,0.9732668995857239,0.7239248752593994,0.25538960099220276,0.815841555595398,0.6655788421630859,0.9287101030349731,-0.5660438537597656,-0.890874445438385,0.9834234118461609,-0.9981180429458618,0.9698693156242371,0.7439671158790588,-0.35639339685440063,0.6022988557815552,0.9708116054534912,-0.9278276562690735,-0.996231734752655,0.8345208168029785,0.9714328050613403,0.8158544898033142,0.5902576446533203,0.8199342489242554,0.9820225834846497,0.9859328269958496,0.5152459144592285,-0.5610516667366028,0.9663665890693665,0.9495159983634949,-0.5204814076423645,0.3104749917984009,0.9859084486961365,0.7797460556030273,0.6738530397415161,0.3919741213321686,-0.906204104423523,0.11750980466604233,0.8077875375747681,0.4169303774833679,0.9829014539718628,0.9902303218841553,0.7847102880477905,0.994563102722168,0.9868024587631226,-0.26804423332214355,-0.9908866882324219,0.745792806148529,0.9906191825866699,-0.18231149017810822,0.97578364610672,0.9986749887466431,0.2544330358505249,-0.954406201839447,0.5869243144989014,-0.23537996411323547,0.9550502896308899,0.25511977076530457,0.9929870963096619,0.09052591770887375,0.9707273244857788,0.6956093311309814,0.6280022263526917,0.7902867794036865,0.9343841075897217,0.989579439163208,0.9436283707618713,-0.10834993422031403,-0.3431112766265869,0.9986708760261536,0.5086739659309387,0.9949507713317871,0.8283133506774902,0.9432437419891357,0.9491766095161438,0.9995352029800415,0.5712319612503052,0.8055234551429749,0.6781864166259766,0.8272571563720703,0.8314797282218933,0.8778656721115112,0.9944958686828613,0.9973865151405334],\"xaxis\":\"x\",\"y\":[0.7775010466575623,0.3527269959449768,0.25961846113204956,0.6670257449150085,0.8384260535240173,0.5584430694580078,0.8444744944572449,0.4137910008430481,0.24488940834999084,0.028157662600278854,0.3584098219871521,0.16288265585899353,-0.45419126749038696,-0.6529328227043152,-0.5484569072723389,-0.7990369200706482,-0.7736425995826721,-0.8522581458091736,0.9774324893951416,0.6626249551773071,-0.7303224205970764,-0.7007019519805908,-0.6946625709533691,-0.9996722340583801,-0.7837163805961609,0.8967759013175964,0.4750954806804657,-0.667197585105896,0.7881461977958679,-0.8547751307487488,-0.9054184556007385,-0.5749384760856628,-0.32175111770629883,-0.028594352304935455,-0.9247617721557617,-0.9699268341064453,0.5864037275314331,-0.7614347338676453,0.5971695780754089,0.7854393720626831,-0.8788883686065674,0.3908745050430298,0.044738516211509705,0.11028008162975311,-0.8169988989830017,0.22129566967487335,-0.9939578771591187,0.5774399042129517,0.525479257106781,0.3049013912677765,-0.10729152709245682,0.9433151483535767,-0.9314428567886353,0.5273632407188416,-0.4264712631702423,-0.9984429478645325,0.5296756029129028,0.8604294061660767,-0.8895052075386047,0.9556970596313477,0.6629186868667603,0.42956963181495667,0.9736858010292053,0.6555483937263489,0.12201889604330063,0.7442770004272461,0.5037952661514282,0.9525359272956848,-0.6507164239883423,-0.9316279292106628,0.9791510105133057,-0.9972584843635559,0.9613031148910522,0.7501779794692993,-0.3806658685207367,0.6429786682128906,0.9557769298553467,-0.9428839683532715,-0.9948079586029053,0.785298764705658,0.9657301306724548,0.707301676273346,0.3687230050563812,0.8128011226654053,0.9659481644630432,0.9730121493339539,0.31900617480278015,-0.30290520191192627,0.9790953397750854,0.9357923269271851,-0.5550313591957092,-0.005466493312269449,0.9867776036262512,0.8249565958976746,0.566429615020752,0.1000526174902916,-0.9464486837387085,-0.25471997261047363,0.6522327661514282,0.1415255218744278,0.9884140491485596,0.9860583543777466,0.6949270367622375,0.9901810884475708,0.9791202545166016,-0.2359553426504135,-0.9820711612701416,0.6506689190864563,0.9895943999290466,-0.29178157448768616,0.9714024662971497,0.9951602220535278,0.18783769011497498,-0.9460937976837158,0.47801902890205383,-0.2489192932844162,0.9437097907066345,0.11866245418787003,0.9941242933273315,-0.38088178634643555,0.9564487934112549,0.5542725920677185,0.42118048667907715,0.6628789901733398,0.8659590482711792,0.9937117695808411,0.9069075584411621,0.39811065793037415,-0.4134220480918884,0.9971913695335388,0.34596705436706543,0.9938657283782959,0.5891268849372864,0.9313740134239197,0.9268401861190796,0.9993564486503601,0.6227539777755737,0.8463947772979736,0.6584346294403076,0.8423126339912415,0.2978496253490448,0.8728679418563843,0.9963144659996033,0.986752450466156],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Full OV Copying Score\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"OV Copying Score\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"OV Copying Score for each head in GPT-2 Small\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('3e59929d-31e9-49b1-ad1a-9d9811d4801d');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "scatter(x=full_OV_copying_score.flatten(), y=OV_copying_score.flatten(), hover_name=[f\"L{layer}H{head}\" for layer in range(12) for head in range(12)], title=\"OV Copying Score for each head in GPT-2 Small\", xaxis=\"Full OV Copying Score\", yaxis=\"OV Copying Score\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Token 256 - the most common pair of ASCII characters: | t|\n",
                        "De-Tokenizing the example tokens: <|endoftext|>The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Token 256 - the most common pair of ASCII characters: |{model.to_string(256)}|\")\n",
                "# Squeeze means to remove dimensions of length 1. \n",
                "# Here, that removes the dummy batch dimension so it's a rank 1 tensor and returns a string\n",
                "# Rank 2 tensors map to a list of strings\n",
                "print(f\"De-Tokenizing the example tokens: {model.to_string(example_text_tokens.squeeze())}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generating Text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "TransformerLens also has basic text generation functionality, which can be useful for generally exploring what the model is capable of (thanks to Ansh Radhakrishnan for adding this!). This is pretty rough functionality, and where possible I recommend using more established libraries like HuggingFace for this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f16e699caef243e3bd730cd876600c4a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/50 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "'(CNN) President Barack Obama caught in embarrassing new scandal\\n\\nAmerican voters who backed Hillary Clinton gave President Barack Obama a 9.5-point lead over Republican Mitt Romney in the latest CNN/ORC International poll, his lowest level since the last CNN-ORC poll in 2006.\\n\\nRepublican voters'"
                        ]
                    },
                    "execution_count": 344,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# NBVAL_IGNORE_OUTPUT\n",
                "model.generate(\"(CNN) President Barack Obama caught in embarrassing new scandal\\n\", max_new_tokens=50, temperature=0.7, prepend_bos=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hook Points\n",
                "\n",
                "The key part of TransformerLens that lets us access and edit intermediate activations are the HookPoints around every model activation. Importantly, this technique will work for *any* model architecture, not just transformers, so long as you're able to edit the model code to add in HookPoints! This is essentially a lightweight library bundled with TransformerLens that should let you take an arbitrary model and make it easier to study. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This is implemented by having a HookPoint layer. Each transformer component has a HookPoint for every activation, which wraps around that activation. The HookPoint acts as an identity function, but has a variety of helper functions that allows us to put PyTorch hooks in to edit and access the relevant activation. \n",
                "\n",
                "There is also a `HookedRootModule` class - this is a utility class that the root module should inherit from (root module = the model we run) - it has several utility functions for using hooks well, notably `reset_hooks`, `run_with_cache` and `run_with_hooks`. \n",
                "\n",
                "The default interface is the `run_with_hooks` function on the root module, which lets us run a forwards pass on the model, and pass on a list of hooks paired with layer names to run on that pass. \n",
                "\n",
                "The syntax for a hook is `function(activation, hook)` where `activation` is the activation the hook is wrapped around, and `hook` is the `HookPoint` class the function is attached to. If the function returns a new activation or edits the activation in-place, that replaces the old one, if it returns None then the activation remains as is.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Toy Example"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Here's a simple example of defining a small network with HookPoints:\n",
                "\n",
                "We define a basic network with two layers that each take a scalar input $x$, square it, and add a constant:\n",
                "$x_0=x$, $x_1=x_0^2+3$, $x_2=x_1^2-4$.\n",
                "\n",
                "We wrap the input, each layer's output, and the intermediate value of each layer (the square) in a hook point.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from transformer_lens.hook_points import HookedRootModule, HookPoint\n",
                "\n",
                "\n",
                "class SquareThenAdd(nn.Module):\n",
                "    def __init__(self, offset):\n",
                "        super().__init__()\n",
                "        self.offset = nn.Parameter(torch.tensor(offset))\n",
                "        self.hook_square = HookPoint()\n",
                "\n",
                "    def forward(self, x):\n",
                "        # The hook_square doesn't change the value, but lets us access it\n",
                "        square = self.hook_square(x * x)\n",
                "        return self.offset + square\n",
                "\n",
                "\n",
                "class TwoLayerModel(HookedRootModule):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.layer1 = SquareThenAdd(3.0)\n",
                "        self.layer2 = SquareThenAdd(-4.0)\n",
                "        self.hook_in = HookPoint()\n",
                "        self.hook_mid = HookPoint()\n",
                "        self.hook_out = HookPoint()\n",
                "\n",
                "        # We need to call the setup function of HookedRootModule to build an\n",
                "        # internal dictionary of modules and hooks, and to give each hook a name\n",
                "        super().setup()\n",
                "\n",
                "    def forward(self, x):\n",
                "        # We wrap the input and each layer's output in a hook - they leave the\n",
                "        # value unchanged (unless there's a hook added to explicitly change it),\n",
                "        # but allow us to access it.\n",
                "        x_in = self.hook_in(x)\n",
                "        x_mid = self.hook_mid(self.layer1(x_in))\n",
                "        x_out = self.hook_out(self.layer2(x_mid))\n",
                "        return x_out\n",
                "\n",
                "\n",
                "model = TwoLayerModel()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "We can add a cache, to save the activation at each hook point\n",
                "\n",
                "(There's a custom `run_with_cache` function on the root module as a convenience, which is a wrapper around model.forward that return model_out, cache_object - we could also manually add hooks with `run_with_hooks` that store activations in a global caching dictionary. This is often useful if we only want to store, eg, subsets or functions of some activations.)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model output: 780.0\n",
                        "Value cached at hook hook_in 5.0\n",
                        "Value cached at hook layer1.hook_square 25.0\n",
                        "Value cached at hook hook_mid 28.0\n",
                        "Value cached at hook layer2.hook_square 784.0\n",
                        "Value cached at hook hook_out 780.0\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "out, cache = model.run_with_cache(torch.tensor(5.0))\n",
                "print(\"Model output:\", out.item())\n",
                "for key in cache:\n",
                "    print(f\"Value cached at hook {key}\", cache[key].item())\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "We can also use hooks to intervene on activations - eg, we can set the intermediate value in layer 2 to zero to change the output to -5\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "layer2.hook_square\n",
                        "Output after intervening on layer2.hook_scaled -4.0\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "def set_to_zero_hook(tensor, hook):\n",
                "    print(hook.name)\n",
                "    return torch.tensor(0.0)\n",
                "\n",
                "\n",
                "print(\n",
                "    \"Output after intervening on layer2.hook_scaled\",\n",
                "    model.run_with_hooks(\n",
                "        torch.tensor(5.0), fwd_hooks=[(\"layer2.hook_square\", set_to_zero_hook)]\n",
                "    ).item(),\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading Pre-Trained Checkpoints\n",
                "\n",
                "There are a lot of interesting questions combining mechanistic interpretability and training dynamics - analysing model capabilities and the underlying circuits that make them possible, and how these change as we train the model. \n",
                "\n",
                "TransformerLens supports these by having several model families with checkpoints throughout training. `HookedTransformer.from_pretrained` can load a checkpoint of a model with the `checkpoint_index` (the label 0 to `num_checkpoints-1`) or `checkpoint_value` (the step or token number, depending on how the checkpoints were labelled)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Available models:\n",
                "* All of my interpretability-friendly models have checkpoints available, including:\n",
                "    * The toy models - `attn-only`, `solu`, `gelu` 1L to 4L\n",
                "        * These have ~200 checkpoints, taken on a piecewise linear schedule (more checkpoints near the start of training), up to 22B tokens. Labelled by number of tokens seen.\n",
                "    * The SoLU models trained on 80% Web Text and 20% Python Code (`solu-6l` to `solu-12l`)\n",
                "        * Same checkpoint schedule as the toy models, this time up to 30B tokens\n",
                "    * The SoLU models trained on the pile (`solu-1l-pile` to `solu-12l-pile`)\n",
                "        * These have ~100 checkpoints, taken on a linear schedule, up to 15B tokens. Labelled by number of steps.\n",
                "        * The 12L training crashed around 11B tokens, so is truncated.\n",
                "* The Stanford Centre for Research of Foundation Models trained 5 GPT-2 Small sized and 5 GPT-2 Medium sized models (`stanford-gpt2-small-a` to `e` and `stanford-gpt2-medium-a` to `e`)\n",
                "    * 600 checkpoints, taken on a piecewise linear schedule, labelled by the number of steps."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The checkpoint structure and labels is somewhat messy and ad-hoc, so I mostly recommend using the `checkpoint_index` syntax (where you can just count from 0 to the number of checkpoints) rather than `checkpoint_value` syntax (where you need to know the checkpoint schedule, and whether it was labelled with the number of tokens or steps). The helper function `get_checkpoint_labels` tells you the checkpoint schedule for a given model - ie what point was each checkpoint taken at, and what type of label was used.\n",
                "\n",
                "Here are graphs of the schedules for several checkpointed models: (note that the first 3 use a log scale, latter 2 use a linear scale)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"9670aaf8-c497-4268-aefd-f91bf035117f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9670aaf8-c497-4268-aefd-f91bf035117f\")) {                    Plotly.newPlot(                        \"9670aaf8-c497-4268-aefd-f91bf035117f\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162],\"xaxis\":\"x\",\"y\":[262144,2621440,4718592,7077888,9175040,11272192,13631488,15728640,18087936,20185088,22282240,33292288,44302336,55312384,66322432,77332480,88342528,99352576,110362624,121372672,132382720,143392768,154402816,165412864,176422912,187432960,198443008,209453056,220463104,264503296,308281344,352321536,396361728,440401920,484442112,528482304,572522496,616300544,660340736,704380928,748421120,792461312,836501504,880279552,924319744,968359936,1012400128,1056440320,1100480512,1144520704,1188298752,1232338944,1276379136,1320419328,1364459520,1408499712,1452277760,1496317952,1540358144,1584398336,1628438528,1672478720,1716518912,1760296960,1804337152,1848377344,1892417536,1936457728,1980497920,2024275968,2068316160,2112356352,2156396544,2200436736,2420375552,2640314368,2860515328,3080454144,3300392960,3520331776,3740270592,3960471552,4180410368,4400349184,4620288000,4840488960,5060427776,5280366592,5500305408,5720506368,5940445184,6160384000,6380322816,6600523776,6820462592,7040401408,7260340224,7480279040,7700480000,7920418816,8140357632,8360296448,8580497408,8800436224,9020375040,9240313856,9460514816,9680453632,9900392448,10120331264,10340270080,10560471040,10780409856,11000348672,11220287488,11440488448,11660427264,11880366080,12100304896,12320505856,12540444672,12760383488,12980322304,13200523264,13420462080,13640400896,13860339712,14080278528,14300479488,14520418304,14740357120,14960295936,15180496896,15400435712,15620374528,15840313344,16060514304,16280453120,16500391936,16720330752,16940269568,17160470528,17380409344,17600348160,17820286976,18040487936,18260426752,18480365568,18700304384,18920505344,19140444160,19360382976,19580321792,19800522752,20020461568,20240400384,20460339200,20680278016,20900478976,21120417792,21340356608,21560295424,21780496384],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"},\"type\":\"log\"},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for attn-only-2l (Log scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('9670aaf8-c497-4268-aefd-f91bf035117f');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"87db2943-c842-478c-8c64-5289a60ba868\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"87db2943-c842-478c-8c64-5289a60ba868\")) {                    Plotly.newPlot(                        \"87db2943-c842-478c-8c64-5289a60ba868\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162],\"xaxis\":\"x\",\"y\":[196608,3342336,6291456,9240576,12386304,15335424,18284544,21233664,24379392,27328512,30277632,45219840,60358656,75300864,90243072,105381888,120324096,135266304,150208512,165347328,180289536,195231744,210370560,225312768,240254976,255197184,270336000,285278208,300220416,360382464,420347904,480313344,540278784,600244224,660209664,720371712,780337152,840302592,900268032,960233472,1020198912,1080360960,1140326400,1200291840,1260257280,1320222720,1380384768,1440350208,1500315648,1560281088,1620246528,1680211968,1740374016,1800339456,1860304896,1920270336,1980235776,2040201216,2100363264,2160328704,2220294144,2280259584,2340225024,2400387072,2460352512,2520317952,2580283392,2640248832,2700214272,2760376320,2820341760,2880307200,2940272640,3000238080,3300261888,3600285696,3900309504,4200333312,4500357120,4800380928,5100208128,5400231936,5700255744,6000279552,6300303360,6600327168,6900350976,7200374784,7500201984,7800225792,8100249600,8400273408,8700297216,9000321024,9300344832,9600368640,9900392448,10200219648,10500243456,10800267264,11100291072,11400314880,11700338688,12000362496,12300386304,12600213504,12900237312,13200261120,13500284928,13800308736,14100332544,14400356352,14700380160,15000207360,15300231168,15600254976,15900278784,16200302592,16500326400,16800350208,17100374016,17400201216,17700225024,18000248832,18300272640,18600296448,18900320256,19200344064,19500367872,19800391680,20100218880,20400242688,20700266496,21000290304,21300314112,21600337920,21900361728,22200385536,22500212736,22800236544,23100260352,23400284160,23700307968,24000331776,24300355584,24600379392,24900206592,25200230400,25500254208,25800278016,26100301824,26400325632,26700349440,27000373248,27300200448,27600224256,27900248064,28200271872,28500295680,28800319488,29100343296,29400367104,29700390912],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"},\"type\":\"log\"},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for solu-12l (Log scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('87db2943-c842-478c-8c64-5289a60ba868');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"a5b83399-2637-48e9-980b-098ce30dc2fd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a5b83399-2637-48e9-980b-098ce30dc2fd\")) {                    Plotly.newPlot(                        \"a5b83399-2637-48e9-980b-098ce30dc2fd\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608],\"xaxis\":\"x\",\"y\":[0,10,20,30,40,50,60,70,80,90,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1250,1300,1350,1400,1450,1500,1550,1600,1650,1700,1750,1800,1850,1900,1950,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000,10100,10200,10300,10400,10500,10600,10700,10800,10900,11000,11100,11200,11300,11400,11500,11600,11700,11800,11900,12000,12100,12200,12300,12400,12500,12600,12700,12800,12900,13000,13100,13200,13300,13400,13500,13600,13700,13800,13900,14000,14100,14200,14300,14400,14500,14600,14700,14800,14900,15000,15100,15200,15300,15400,15500,15600,15700,15800,15900,16000,16100,16200,16300,16400,16500,16600,16700,16800,16900,17000,17100,17200,17300,17400,17500,17600,17700,17800,17900,18000,18100,18200,18300,18400,18500,18600,18700,18800,18900,19000,19100,19200,19300,19400,19500,19600,19700,19800,19900,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000,144000,145000,146000,147000,148000,149000,150000,151000,152000,153000,154000,155000,156000,157000,158000,159000,160000,161000,162000,163000,164000,165000,166000,167000,168000,169000,170000,171000,172000,173000,174000,175000,176000,177000,178000,179000,180000,181000,182000,183000,184000,185000,186000,187000,188000,189000,190000,191000,192000,193000,194000,195000,196000,197000,198000,199000,200000,201000,202000,203000,204000,205000,206000,207000,208000,209000,210000,211000,212000,213000,214000,215000,216000,217000,218000,219000,220000,221000,222000,223000,224000,225000,226000,227000,228000,229000,230000,231000,232000,233000,234000,235000,236000,237000,238000,239000,240000,241000,242000,243000,244000,245000,246000,247000,248000,249000,250000,251000,252000,253000,254000,255000,256000,257000,258000,259000,260000,261000,262000,263000,264000,265000,266000,267000,268000,269000,270000,271000,272000,273000,274000,275000,276000,277000,278000,279000,280000,281000,282000,283000,284000,285000,286000,287000,288000,289000,290000,291000,292000,293000,294000,295000,296000,297000,298000,299000,300000,301000,302000,303000,304000,305000,306000,307000,308000,309000,310000,311000,312000,313000,314000,315000,316000,317000,318000,319000,320000,321000,322000,323000,324000,325000,326000,327000,328000,329000,330000,331000,332000,333000,334000,335000,336000,337000,338000,339000,340000,341000,342000,343000,344000,345000,346000,347000,348000,349000,350000,351000,352000,353000,354000,355000,356000,357000,358000,359000,360000,361000,362000,363000,364000,365000,366000,367000,368000,369000,370000,371000,372000,373000,374000,375000,376000,377000,378000,379000,380000,381000,382000,383000,384000,385000,386000,387000,388000,389000,390000,391000,392000,393000,394000,395000,396000,397000,398000,399000,400000],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"},\"type\":\"log\"},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for stanford-gpt2-small-a (Log scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('a5b83399-2637-48e9-980b-098ce30dc2fd');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"4c28ffe1-8565-4e92-babd-d57cc8bc847d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4c28ffe1-8565-4e92-babd-d57cc8bc847d\")) {                    Plotly.newPlot(                        \"4c28ffe1-8565-4e92-babd-d57cc8bc847d\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"xaxis\":\"x\",\"y\":[832,1664,2496,3328,4160,4992,5824,6656,7488,8320,9152,9984,10816,11648,12480,13312,14144,14976,15808,16640,17472,18304,19136,19968,20800,21632,22464,23296,24128,24960,25792,26624,27456,28288,29120,29952,30784,31616,32448,33280,34112,34944,35776,36608,37440,38272,39104,39936,40768,41600],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for solu-1l-pile (Linear scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('4c28ffe1-8565-4e92-babd-d57cc8bc847d');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"d92c9f04-9700-4de1-9237-9cba28fcdafb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d92c9f04-9700-4de1-9237-9cba28fcdafb\")) {                    Plotly.newPlot(                        \"d92c9f04-9700-4de1-9237-9cba28fcdafb\",                        [{\"hovertemplate\":\"variable=0\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"xaxis\":\"x\",\"y\":[326,652,978,1304,1630,1956,2282,2608,2934,3260,3586,3912,4238,4564,4890,5216,5542,5868,6194,6520,6846,7172,7498,7824,8150,8476,8802,9128,9454,9780,10106,10432,10758,11084,11410,11736,12062,12388,12714,13040,13366,13692,14018,14344,14670,14996,15322,15648,15974,16300,16626,16952,17278,17604,17930,18256,18582,18908,19234,19560,19886,20212,20538,20864,21190,21516,21842,22168,22494,22820,23146,23472,23798,24124,24450,24776,25102,25428,25754,26080,26406,26732,27058,27384,27710,28036,28362,28688,29014,29340,29666,29992,30318,30644,30970,31296,31622,31948,32274,32600],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for solu-6l-pile (Linear scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('d92c9f04-9700-4de1-9237-9cba28fcdafb');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from transformer_lens.loading_from_pretrained import get_checkpoint_labels\n",
                "for model_name in [\"attn-only-2l\", \"solu-12l\", \"stanford-gpt2-small-a\"]:\n",
                "    checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)\n",
                "    line(checkpoint_labels, xaxis=\"Checkpoint Index\", yaxis=f\"Checkpoint Value ({checkpoint_label_type})\", title=f\"Checkpoint Values for {model_name} (Log scale)\", log_y=True, markers=True)\n",
                "for model_name in [\"solu-1l-pile\", \"solu-6l-pile\"]:\n",
                "    checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)\n",
                "    line(checkpoint_labels, xaxis=\"Checkpoint Index\", yaxis=f\"Checkpoint Value ({checkpoint_label_type})\", title=f\"Checkpoint Values for {model_name} (Linear scale)\", log_y=False, markers=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example: Induction Head Phase Transition"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One of the more interesting results analysing circuit formation during training is the [induction head phase transition](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html). They find a pretty dramatic shift in models during training - there's a brief period where models go from not having induction heads to having them, which leads to the models suddenly becoming much better at in-context learning (using far back tokens to predict the next token, eg over 500 words back). This is enough of a big deal that it leads to a visible *bump* in the loss curve, where the model's rate of improvement briefly increases. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As a brief demonstration of the existence of the phase transition, let's load some checkpoints of a two layer model, and see whether they have induction heads. An easy test, as we used above, is to give the model a repeated sequence of random tokens, and to check how good its loss is on the second half. `evals.induction_loss` is a rough util that runs this test on a model.\n",
                "(Note - this is deliberately a rough, non-rigorous test for the purposes of demonstration, eg `evals.induction_loss` by default just runs it on 4 sequences of 384 tokens repeated twice. These results totally don't do the paper justice - go check it out if you want to see the full results!)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the interests of time and memory, let's look at a handful of checkpoints (chosen to be around the phase change), indices `[10, 25, 35, 60, -1]`. These are roughly 22M, 200M, 500M, 1.6B and 21.8B tokens through training, respectively. (I generally recommend looking things up based on indices, rather than checkpoint value!). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformer_lens import evals\n",
                "# We use the two layer model with SoLU activations, chosen fairly arbitrarily as being both small (so fast to download and keep in memory) and pretty good at the induction task.\n",
                "model_name = \"solu-2l\"\n",
                "# We can load a model from a checkpoint by specifying the checkpoint_index, -1 means the final checkpoint\n",
                "checkpoint_indices = [10, 25, 35, 60, -1]\n",
                "checkpointed_models = []\n",
                "tokens_trained_on = []\n",
                "induction_losses = []"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We load the models, cache them in a list, and "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not IN_GITHUB:\n",
                "    for index in checkpoint_indices:\n",
                "        # Load the model from the relevant checkpoint by index\n",
                "        model_for_this_checkpoint = HookedTransformer.from_pretrained(model_name, checkpoint_index=index, device=device)\n",
                "        checkpointed_models.append(model_for_this_checkpoint)\n",
                "\n",
                "        tokens_seen_for_this_checkpoint = model_for_this_checkpoint.cfg.checkpoint_value\n",
                "        tokens_trained_on.append(tokens_seen_for_this_checkpoint)\n",
                "\n",
                "        induction_loss_for_this_checkpoint = evals.induction_loss(model_for_this_checkpoint, device=device).item()\n",
                "        induction_losses.append(induction_loss_for_this_checkpoint)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can plot this, and see there's a sharp shift from ~200-500M tokens trained on (note the log scale on the x axis). Interestingly, this is notably earlier than the phase transition in the paper, I'm not sure what's up with that.\n",
                "\n",
                "(To contextualise the numbers, the tokens in the random sequence are uniformly chosen from the first 20,000 tokens (out of ~48,000 total), so random performance is at least $\\ln(20000)\\approx 10$. A naive strategy like \"randomly choose a token that's already appeared in the first half of the sequence (384 elements)\" would get $\\ln(384)\\approx 5.95$, so the model is doing pretty well here.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<html>\n",
                            "<head><meta charset=\"utf-8\" /></head>\n",
                            "<body>\n",
                            "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
                            "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.26.0.min.js\"></script>                <div id=\"1b82f6dc-4619-4786-ac45-c3cf11921de0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1b82f6dc-4619-4786-ac45-c3cf11921de0\")) {                    Plotly.newPlot(                        \"1b82f6dc-4619-4786-ac45-c3cf11921de0\",                        [],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"},\"type\":\"log\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Induction Loss over training: solu-2l\"}},                        {\"responsive\": true}                    ).then(function(){\n",
                            "                            \n",
                            "var gd = document.getElementById('1b82f6dc-4619-4786-ac45-c3cf11921de0');\n",
                            "var x = new MutationObserver(function (mutations, observer) {{\n",
                            "        var display = window.getComputedStyle(gd).display;\n",
                            "        if (!display || display === 'none') {{\n",
                            "            console.log([gd, 'removed!']);\n",
                            "            Plotly.purge(gd);\n",
                            "            observer.disconnect();\n",
                            "        }}\n",
                            "}});\n",
                            "\n",
                            "// Listen for the removal of the full notebook cells\n",
                            "var notebookContainer = gd.closest('#notebook-container');\n",
                            "if (notebookContainer) {{\n",
                            "    x.observe(notebookContainer, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "// Listen for the clearing of the current output cell\n",
                            "var outputEl = gd.closest('.output');\n",
                            "if (outputEl) {{\n",
                            "    x.observe(outputEl, {childList: true});\n",
                            "}}\n",
                            "\n",
                            "                        })                };                            </script>        </div>\n",
                            "</body>\n",
                            "</html>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "line(induction_losses, x=tokens_trained_on, xaxis=\"Tokens Trained On\", yaxis=\"Induction Loss\", title=\"Induction Loss over training: solu-2l\", markers=True, log_x=True)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "TransformerLens Dev",
            "language": "python",
            "name": "transformerlens-dev"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "eb812820b5094695c8a581672e17220e30dd2c15d704c018326e3cc2e1a566f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
