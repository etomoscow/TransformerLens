{
  "generated_at": "2026-01-03",
  "scan_info": {
    "total_scanned": 309079,
    "task_filter": "text-generation"
  },
  "total_unsupported_architectures": 1386,
  "total_unsupported_models": 25129,
  "gaps": [
    {
      "architecture_id": "StableLmForCausalLM",
      "total_models": 5329,
      "sample_models": [
        "stabilityai/stablelm-3b-4e1t",
        "stabilityai/stablelm-zephyr-3b",
        "UnfilteredAI/NSFW-flash",
        "stabilityai/stablelm-2-zephyr-1_6b",
        "stabilityai/stable-code-3b",
        "stabilityai/stable-code-instruct-3b",
        "stabilityai/stablelm-2-1_6b",
        "HelpingAI/HELVETE-3B",
        "stabilityai/tiny-random-stablelm-2",
        "stabilityai/stablelm-2-12b"
      ]
    },
    {
      "architecture_id": "Step1MoEForCausalLM",
      "total_models": 2473,
      "sample_models": [
        "StepLaw/StepLaw-N_214M-D_19.0B-LR1.95E-03-BS49152",
        "StepLaw/StepLaw-N_268M-D_4.0B-LR3.906e-03-BS1048576",
        "StepLaw/StepLaw-N_268M-D_79.0B-LR6.905e-04-BS524288",
        "StepLaw/StepLaw-N_214M-D_19.0B-LR2.21E-02-BS65536",
        "StepLaw/StepLaw-N_429M-D_39.0B-LR3.91E-03-BS196608",
        "StepLaw/StepLaw-N_214M-D_19.0B-LR2.76E-03-BS524288",
        "StepLaw/StepLaw-N_268M-D_79.0B-LR6.905e-04-BS131072",
        "StepLaw/StepLaw-N_268M-D_4.0B-LR1.953e-03-BS131072",
        "StepLaw/StepLaw-N_268M-D_4.0B-LR2.210e-02-BS524288",
        "StepLaw/StepLaw-N_268M-D_4.0B-LR2.762e-03-BS2097152"
      ]
    },
    {
      "architecture_id": "ParlerTTSForConditionalGeneration",
      "total_models": 1611,
      "sample_models": [
        "parler-tts/parler-tts-mini-multilingual-v1.1",
        "ai4bharat/indic-parler-tts",
        "parler-tts/parler-tts-large-v1",
        "parler-tts/parler-tts-mini-v1",
        "parler-tts/parler_tts_mini_v0.1",
        "ai4bharat/indic-parler-tts-pretrained",
        "parler-tts/parler-tts-mini-expresso",
        "2121-8/japanese-parler-tts-large-bate",
        "parler-tts/parler-tts-tiny-v1",
        "freds0/parler-tts-mini-v1.1-ptbr"
      ]
    },
    {
      "architecture_id": "Qwen3MoeForCausalLM",
      "total_models": 854,
      "sample_models": [
        "Qwen/Qwen3-Coder-30B-A3B-Instruct",
        "Qwen/Qwen3-30B-A3B-Instruct-2507",
        "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8",
        "Qwen/Qwen3-30B-A3B-Thinking-2507",
        "Qwen/Qwen3-30B-A3B",
        "Qwen/Qwen3-30B-A3B-Instruct-2507-FP8",
        "Qwen/Qwen3-30B-A3B-GPTQ-Int4",
        "Qwen/Qwen3-235B-A22B",
        "QuantTrio/Qwen3-Coder-30B-A3B-Instruct-AWQ",
        "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"
      ]
    },
    {
      "architecture_id": "FalconForCausalLM",
      "total_models": 659,
      "sample_models": [
        "tiiuae/falcon-7b",
        "tiiuae/falcon-7b-instruct",
        "tiiuae/falcon-40b-instruct",
        "vilsonrodrigues/falcon-7b-instruct-sharded",
        "tiiuae/falcon-40b",
        "tiiuae/falcon-rw-1b",
        "tiiuae/falcon-11B",
        "euclaise/falcon_1b_stage2",
        "fxmarty/really-tiny-falcon-testing",
        "explosion-testing/falcon-test"
      ]
    },
    {
      "architecture_id": "LlavaLlamaForCausalLM",
      "total_models": 449,
      "sample_models": [
        "liuhaotian/llava-v1.6-vicuna-13b",
        "liuhaotian/llava-v1.5-7b",
        "liuhaotian/llava-v1.6-vicuna-7b",
        "liuhaotian/llava-v1.5-13b",
        "ShareGPTVideo/LLaVA-Hound-Pretrain",
        "Lin-Chen/sharegpt4video-8b",
        "second-state/Llava-v1.5-7B-GGUF",
        "lmms-lab/llama3-llava-next-8b",
        "LanguageBind/Video-LLaVA-7B",
        "lmms-lab/LLaVA-NeXT-Video-7B-DPO"
      ]
    },
    {
      "architecture_id": "Gemma3ForConditionalGeneration",
      "total_models": 413,
      "sample_models": [
        "uaritm/gemmamed_cardio",
        "vanta-research/scout-4b",
        "aisingapore/Gemma-SEA-LION-v4-27B-IT",
        "vanta-research/atom-v1-preview-4b",
        "YanLabs/gemma-3-27b-it-abliterated-normpreserve",
        "DrRiceIO7/HereticFT-Aggressive",
        "DrRiceIO7/HereticFT",
        "YanLabs/gemma-3-27b-it-abliterated-normpreserve-v1",
        "mshojaei77/gemma-3-4b-persian-v0",
        "DavidAU/Gemma-3-4b-it-Uncensored-DBL-X"
      ]
    },
    {
      "architecture_id": "CohereForCausalLM",
      "total_models": 402,
      "sample_models": [
        "CohereLabs/aya-expanse-8b",
        "trl-internal-testing/tiny-CohereForCausalLM",
        "CohereLabs/aya-23-8B",
        "CohereLabs/c4ai-command-r-v01",
        "NLPark/AnFeng_v3_Avocet",
        "CohereLabs/aya-expanse-32b",
        "CohereLabs/aya-23-35B",
        "CohereLabs/c4ai-command-r-08-2024",
        "CohereLabs/c4ai-command-r-plus",
        "CohereLabs/c4ai-command-r-plus-08-2024"
      ]
    },
    {
      "architecture_id": "GPTBigCodeForCausalLM",
      "total_models": 340,
      "sample_models": [
        "bigcode/gpt_bigcode-santacoder",
        "bigcode/starcoder",
        "bigcode/starcoderbase-1b",
        "bigcode/starcoderbase-3b",
        "openchat/opencoderplus",
        "LoupGarou/WizardCoder-Guanaco-15B-V1.0",
        "LoupGarou/WizardCoder-Guanaco-15B-V1.1",
        "V-YangXu/StarCoder-Alpaca",
        "Danielbrdz/CodeBarcenas-1b",
        "HuggingFaceH4/starchat-alpha"
      ]
    },
    {
      "architecture_id": "OlmoForCausalLM",
      "total_models": 337,
      "sample_models": [
        "allenai/OLMo-7B-0724-hf",
        "allenai/OLMo-7B-0724-Instruct-hf",
        "allenai/OLMo-7B-0724-SFT-hf",
        "allenai/OLMo-1B-hf",
        "katuni4ka/tiny-random-olmo-hf",
        "allenai/OLMo-7B-0424-hf",
        "allenai/OLMo-7B-hf",
        "allenai/OLMo-7B-SFT-hf",
        "allenai/OLMo-7B-Instruct-hf",
        "amd/AMD-OLMo-1B-SFT"
      ]
    },
    {
      "architecture_id": "CodeGenForCausalLM",
      "total_models": 296,
      "sample_models": [
        "Salesforce/codegen-350M-mono",
        "Salesforce/codegen-350M-multi",
        "Salesforce/codegen-2B-mono",
        "Salesforce/codegen-6B-multi",
        "Salesforce/codegen-16B-nl",
        "sharoz/codegen-350M-mono-custom-functions-dataset-python_v2",
        "Salesforce/codegen-350M-nl",
        "hf-tiny-model-private/tiny-random-CodeGenForCausalLM",
        "Salesforce/codegen-6B-mono",
        "Salesforce/codegen2-1B_P"
      ]
    },
    {
      "architecture_id": "DeepseekV3ForCausalLM",
      "total_models": 292,
      "sample_models": [
        "deepseek-ai/DeepSeek-V3",
        "deepseek-ai/DeepSeek-R1",
        "moonshotai/Kimi-K2-Thinking",
        "deepseek-ai/DeepSeek-R1-0528",
        "deepseek-ai/DeepSeek-V3-0324",
        "nvidia/DeepSeek-R1-0528-NVFP4-v2",
        "unsloth/DeepSeek-V3.1-BF16",
        "moonshotai/Kimi-K2-Instruct",
        "deepseek-ai/DeepSeek-V3.1",
        "nvidia/DeepSeek-V3.1-NVFP4"
      ]
    },
    {
      "architecture_id": "QWenLMHeadModel",
      "total_models": 267,
      "sample_models": [
        "Qwen/Qwen-7B",
        "Qwen/Qwen-7B-Chat",
        "Qwen/Qwen-VL-Chat",
        "Qwen/Qwen-VL",
        "Qwen/Qwen-1_8B-Chat",
        "Qwen/Qwen-1_8B",
        "Qwen/Qwen-Audio-Chat",
        "Qwen/Qwen-14B",
        "Qwen/Qwen-72B",
        "Qwen/Qwen-Audio"
      ]
    },
    {
      "architecture_id": "Olmo2ForCausalLM",
      "total_models": 261,
      "sample_models": [
        "allenai/OLMo-2-0425-1B",
        "allenai/OLMo-2-0425-1B-Instruct",
        "allenai/OLMo-2-0425-1B-SFT",
        "allenai/OLMo-2-1124-7B-Instruct",
        "allenai/OLMo-2-1124-13B-Instruct",
        "allenai/OLMo-2-1124-7B-DPO",
        "allenai/OLMo-2-1124-7B-SFT",
        "allenai/OLMo-2-0325-32B-Instruct",
        "allenai/OLMo-2-0325-32B-SFT",
        "allenai/OLMo-2-0325-32B"
      ]
    },
    {
      "architecture_id": "Lfm2ForCausalLM",
      "total_models": 260,
      "sample_models": [
        "LiquidAI/LFM2-1.2B",
        "lmstudio-community/LFM2-1.2B-MLX-8bit",
        "lmstudio-community/LFM2-1.2B-MLX-bf16",
        "LiquidAI/LFM2-350M",
        "LiquidAI/LFM2-2.6B",
        "LiquidAI/LFM2-700M",
        "LiquidAI/LFM2-2.6B-Exp",
        "LiquidAI/LFM2-1.2B-Extract",
        "LiquidAI/LFM2-350M-ENJP-MT",
        "LiquidAI/LFM2-350M-Extract"
      ]
    },
    {
      "architecture_id": "RWForCausalLM",
      "total_models": 256,
      "sample_models": [
        "amazon/FalconLite",
        "lightonai/alfred-40b-1023",
        "vilm/vulture-40b",
        "amazon/FalconLite2",
        "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3",
        "explosion-testing/refined-web-model-test",
        "h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2",
        "TheBloke/Falcon-7B-Instruct-GPTQ",
        "abhinavkulkarni/tiiuae-falcon-40b-instruct-w4-g128-awq",
        "OpenAssistant/falcon-7b-sft-mix-2000"
      ]
    },
    {
      "architecture_id": "MPTForCausalLM",
      "total_models": 246,
      "sample_models": [
        "vinai/PhoGPT-4B-Chat",
        "anas-awadalla/mpt-7b",
        "wtang06/mpt-125m-c4",
        "lightblue/japanese-mpt-7b",
        "aisingapore/SEA-LION-v1-7B",
        "aisingapore/SEA-LION-v1-7B-IT",
        "replit/replit-code-v1_5-3b",
        "replit/replit-code-v1-3b",
        "echarlaix/tiny-mpt-random-remote-code",
        "BUT-FIT/csmpt7b"
      ]
    },
    {
      "architecture_id": "Starcoder2ForCausalLM",
      "total_models": 241,
      "sample_models": [
        "bigcode/starcoder2-3b",
        "bigcode/starcoder2-7b",
        "bigcode/starcoder2-15b",
        "second-state/StarCoder2-7B-GGUF",
        "uukuguy/speechless-starcoder2-15b",
        "second-state/StarCoder2-15B-GGUF",
        "bigcode/starcoder2-15b-instruct-v0.1",
        "second-state/StarCoder2-3B-GGUF",
        "gaianet/StarCoder2-15B-Instruct-v0.1-GGUF",
        "HuggingFaceH4/starchat2-15b-v0.1"
      ]
    },
    {
      "architecture_id": "GraniteMoeHybridForCausalLM",
      "total_models": 237,
      "sample_models": [
        "ibm-granite/granite-4.0-h-micro-base",
        "ibm-granite/granite-4.0-tiny-preview",
        "ibm-granite/granite-4.0-micro",
        "ibm-granite/granite-4.0-h-small",
        "ibm-granite/granite-4.0-h-tiny",
        "ibm-granite/granite-4.0-350m",
        "ibm-granite/granite-4.0-h-350m",
        "ibm-granite/granite-4.0-h-1b",
        "ibm-granite/granite-4.0-h-1b-base",
        "ibm-granite/granite-4.0-1b"
      ]
    },
    {
      "architecture_id": "Glm4MoeForCausalLM",
      "total_models": 217,
      "sample_models": [
        "zai-org/GLM-4.5-Air",
        "cyankiwi/GLM-4.5-Air-AWQ-4bit",
        "zai-org/GLM-4.6",
        "zai-org/GLM-4.5-Air-FP8",
        "zai-org/GLM-4.7",
        "zai-org/GLM-4.6-FP8",
        "zai-org/GLM-4.5",
        "zai-org/GLM-4.7-FP8",
        "PrimeIntellect/INTELLECT-3",
        "cyankiwi/GLM-4.5-Air-Derestricted-AWQ-8bit"
      ]
    },
    {
      "architecture_id": "SmolLM3ForCausalLM",
      "total_models": 187,
      "sample_models": [
        "HuggingFaceTB/SmolLM3-3B",
        "HuggingFaceTB/SmolLM3-3B-Base",
        "optimum-internal-testing/tiny-random-SmolLM3ForCausalLM",
        "mlx-community/SmolLM3-3B-4bit",
        "unsloth/SmolLM3-3B",
        "G-reen/SmolLM3-3B-SFT",
        "RedHatAI/SmolLM3-3B-FP8-dynamic",
        "unsloth/SmolLM3-3B-unsloth-bnb-4bit",
        "ryjava432/lsat-smollm3-sft",
        "lmstudio-community/SmolLM3-3B-MLX-8bit"
      ]
    },
    {
      "architecture_id": "GraniteForCausalLM",
      "total_models": 182,
      "sample_models": [
        "ibm-granite/granite-3.3-2b-instruct",
        "ibm-granite/granite-3.3-8b-instruct",
        "ibm-granite/granite-3.1-8b-instruct",
        "ibm-research/PowerLM-3b",
        "ibm-granite/granite-3.0-8b-instruct",
        "ibm-granite/granite-guardian-3.0-2b",
        "ibm-granite/granite-3.1-2b-instruct",
        "ibm-granite/granite-3.3-8b-base",
        "ibm-granite/granite-3.3-2b-base",
        "ibm-granite/granite-3.1-2b-base"
      ]
    },
    {
      "architecture_id": "BertLMHeadModel",
      "total_models": 169,
      "sample_models": [
        "dicta-il/BEREL_3.0",
        "steysie/bert-base-multilingual-cased-tuned-smartcat",
        "KarlGauss/bert-base-italian-xxl-cased-finetuned-paisa",
        "ielab/TILDE",
        "agdsga/chinese-bert-wwm-finetuned-product",
        "temporary0-0name/run_opt",
        "agdsga/nezha-chinese-base-finetuned-product",
        "agdsga/chinese-pert-large-finetuned-product",
        "adamthekiwi/toki-pona-bert",
        "ishanarang/output-ds"
      ]
    },
    {
      "architecture_id": "MobileLLMForCausalLM",
      "total_models": 160,
      "sample_models": [
        "facebook/MobileLLM-350M",
        "facebook/MobileLLM-1B",
        "facebook/MobileLLM-600M",
        "onnx-community/MobileLLM-125M",
        "facebook/MobileLLM-125M-layer-share",
        "facebook/MobileLLM-350M-layer-share",
        "onnx-community/MobileLLM-350M",
        "mia-llm/MobileLLM-1B-wikitext2raw-roya",
        "h0ssn/MobileLLM-125M-e3.9.24.4.2-Agnews",
        "kaizen9/MobileLLM-600M"
      ]
    },
    {
      "architecture_id": "OpenELMForCausalLM",
      "total_models": 143,
      "sample_models": [
        "apple/OpenELM-1_1B-Instruct",
        "apple/OpenELM-3B-Instruct",
        "apple/OpenELM-450M-Instruct",
        "apple/OpenELM-270M-Instruct",
        "apple/OpenELM-270M",
        "apple/OpenELM-450M",
        "apple/OpenELM-1_1B",
        "apple/OpenELM-3B",
        "mlx-community/OpenELM-3B",
        "liswei/Taiwan-ELM-1_1B"
      ]
    },
    {
      "architecture_id": "ExaoneForCausalLM",
      "total_models": 138,
      "sample_models": [
        "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct",
        "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
        "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct",
        "LGAI-EXAONE/EXAONE-3.5-32B-Instruct",
        "LGAI-EXAONE/EXAONE-Deep-7.8B",
        "LGAI-EXAONE/EXAONE-Deep-2.4B",
        "LGAI-EXAONE/EXAONE-Deep-32B",
        "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ",
        "OpenLLM-Korea/EXAONE-Deep-7.8B",
        "OpenLLM-Korea/EXAONE-Deep-32B"
      ]
    },
    {
      "architecture_id": "OlmoeForCausalLM",
      "total_models": 135,
      "sample_models": [
        "allenai/OLMoE-1B-7B-0125-Instruct",
        "allenai/OLMoE-1B-7B-0924",
        "allenai/OLMoE-1B-7B-0125",
        "allenai/OLMoE-1B-7B-0924-Instruct",
        "allenai/OLMoE-1B-7B-0125-DPO",
        "allenai/OLMoE-1B-7B-0125-SFT",
        "1024m/OLMoE-1B-7B-0924-Instruct-Base",
        "chenggong1995/OLMoE-1B-7B-0125-Instruct-grpo-E8-D8000",
        "allura-org/MoE-Girl-1BA-7BT",
        "mlx-community/OLMoE-1B-7B-0125-Instruct"
      ]
    },
    {
      "architecture_id": "DeepseekV2ForCausalLM",
      "total_models": 133,
      "sample_models": [
        "deepseek-ai/DeepSeek-V2-Lite-Chat",
        "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
        "deepseek-ai/DeepSeek-V2-Lite",
        "deepseek-ai/DeepSeek-Coder-V2-Instruct",
        "deepseek-ai/DeepSeek-Coder-V2-Base",
        "RedHatAI/DeepSeek-V2.5-1210-FP8",
        "RedHatAI/DeepSeek-Coder-V2-Lite-Instruct-FP8",
        "deepseek-ai/DeepSeek-Coder-V2-Lite-Base",
        "deepseek-ai/DeepSeek-V2-Chat",
        "TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ"
      ]
    },
    {
      "architecture_id": "StableLMEpochForCausalLM",
      "total_models": 131,
      "sample_models": [
        "acon96/Home-3B-v3-GGUF",
        "NousResearch/Nous-Capybara-3B-V1.9",
        "stabilityai/japanese-stablelm-3b-4e1t-base",
        "TheBloke/rocket-3B-GPTQ",
        "euclaise/Memphis-CoT-3B",
        "TheBloke/bling-stable-lm-3b-4e1t-v0-GPTQ",
        "acrastt/Marx-3B-V3",
        "LanguageBind/MoE-LLaVA-StableLM-Pretrain",
        "TheBloke/Nous-Capybara-3B-v1.9-GPTQ",
        "LanguageBind/MoE-LLaVA-StableLM-384-Pretrain"
      ]
    },
    {
      "architecture_id": "MixFormerSequentialForCausalLM",
      "total_models": 127,
      "sample_models": [
        "SkunkworksAI/phi-2",
        "goendalf666/salesGPT_v1",
        "Open-Orca/oo-phi-1_5",
        "DanielClough/Candle_Phi-Hermes-1.3B",
        "DanielClough/Candle_Puffin-Phi-v2",
        "QuixiAI/samantha-phi",
        "goendalf666/salesGPT_v2",
        "winglian/omega-platy",
        "KSU-HW-SEC/Hardware_Phi_30k_version",
        "aegon-h/phi-base_model"
      ]
    },
    {
      "architecture_id": "MambaForCausalLM",
      "total_models": 122,
      "sample_models": [
        "state-spaces/mamba-130m-hf",
        "state-spaces/mamba-2.8b-hf",
        "state-spaces/mamba-370m-hf",
        "state-spaces/mamba-1.4b-hf",
        "ArthurZ/mamba-130m",
        "state-spaces/mamba-790m-hf",
        "TRI-ML/mamba-7b-rw",
        "isemmanuelolowe/Ikhou3b",
        "isemmanuelolowe/Ikhou_130M",
        "leliuga/mamba-2.8b-hf-GGUF"
      ]
    },
    {
      "architecture_id": "LLaMAForCausalLM",
      "total_models": 122,
      "sample_models": [
        "anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g",
        "Enoch/llama-65b-hf",
        "Rardilit/Panther_v1",
        "heegyu/LIMA-13b",
        "mncai/chatdoctor",
        "AdaptLLM/medicine-LLM",
        "maicomputer/alpaca-13b",
        "TheBloke/law-LLM-GPTQ",
        "JG22/decapoda-research-llama-13b",
        "nvidia/Llama3-ChatQA-2-8B"
      ]
    },
    {
      "architecture_id": "Olmo3ForCausalLM",
      "total_models": 116,
      "sample_models": [
        "allenai/Olmo-3-7B-Instruct-SFT",
        "allenai/Olmo-3-7B-Instruct",
        "allenai/Olmo-3-7B-Think-SFT",
        "allenai/Olmo-3-1025-7B",
        "allenai/Olmo-3-7B-Think",
        "allenai/Olmo-3-32B-Think",
        "allenai/Olmo-3.1-32B-Instruct",
        "allenai/Olmo-3-1125-32B",
        "allenai/Olmo-3-7B-RL-Zero-General",
        "allenai/Olmo-3-7B-Instruct-DPO"
      ]
    },
    {
      "architecture_id": "Qwen3NextForCausalLM",
      "total_models": 108,
      "sample_models": [
        "Qwen/Qwen3-Next-80B-A3B-Instruct",
        "Qwen/Qwen3-Next-80B-A3B-Thinking-FP8",
        "Qwen/Qwen3-Next-80B-A3B-Instruct-FP8",
        "cyankiwi/Qwen3-Next-80B-A3B-Thinking-AWQ-4bit",
        "Qwen/Qwen3-Next-80B-A3B-Thinking",
        "unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF",
        "unsloth/Qwen3-Next-80B-A3B-Instruct-bnb-4bit",
        "lmstudio-community/Qwen3-Next-80B-A3B-Instruct-MLX-4bit",
        "lmstudio-community/Qwen3-Next-80B-A3B-Instruct-MLX-8bit",
        "cyankiwi/Qwen3-Next-80B-A3B-Instruct-AWQ-4bit"
      ]
    },
    {
      "architecture_id": "RobertaForCausalLM",
      "total_models": 107,
      "sample_models": [
        "gokceuludogan/ChemBERTaLM",
        "Poompatai/distilroberta-base-finetuned-cyberReadmes",
        "windshield-viper/RoBERTa_for_Discord",
        "sangjeedondrub/tibetan-roberta-causal-base",
        "uf-aice-lab/math-roberta",
        "Hailay/GeezScriptTokenizer",
        "mamiksik/CodeBertaCLM",
        "Gan1108/robertaForCausalLM",
        "TSingye/DYG_TiBERT_v2",
        "AdnanRiaz107/CBertbase-APPS10k"
      ]
    },
    {
      "architecture_id": "BaichuanForCausalLM",
      "total_models": 104,
      "sample_models": [
        "baichuan-inc/Baichuan2-7B-Chat",
        "baichuan-inc/Baichuan2-13B-Chat",
        "baichuan-inc/Baichuan-13B-Chat",
        "baichuan-inc/Baichuan2-13B-Base",
        "baichuan-inc/Baichuan2-7B-Base",
        "baichuan-inc/Baichuan-13B-Base",
        "katuni4ka/tiny-random-baichuan2",
        "katuni4ka/tiny-random-baichuan2-13b",
        "baichuan-inc/Baichuan2-13B-Chat-4bits",
        "sakuraumi/Sakura-13B-Galgame"
      ]
    },
    {
      "architecture_id": "BartForCausalLM",
      "total_models": 103,
      "sample_models": [
        "vishal2002/convertLlmnfone",
        "RichardErkhov/gangyeolkim_-_kobart-korean-summarizer-v2-4bits",
        "sanchit-gandhi/tiny-random-bart-fp16",
        "pixelsandpointers/bart-base-empatheticdialogues",
        "jky594176/recipe_BART1",
        "jky594176/recipe_BART1_NN",
        "Hhblvjgvg/myiaxd",
        "GItaf/bart-base-finetuned-mbti",
        "jungiebeen/pretrain11",
        "jky594176/recipe_BART1_GRU"
      ]
    },
    {
      "architecture_id": "DeciLMForCausalLM",
      "total_models": 102,
      "sample_models": [
        "nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
        "nvidia/Llama-3_3-Nemotron-Super-49B-v1_5",
        "nvidia/Llama-3_3-Nemotron-Super-49B-v1",
        "Deci/DeciLM-7B",
        "NewstaR/Porpoise-6b-instruct",
        "Danielbrdz/Barcenas-6b",
        "Deci/DeciLM-7B-instruct",
        "nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-FP8",
        "nvidia/Llama-3_3-Nemotron-Super-49B-v1_5-NVFP4",
        "unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF"
      ]
    },
    {
      "architecture_id": "InternLM2ForCausalLM",
      "total_models": 102,
      "sample_models": [
        "AI4Chem/ChemLLM-7B-Chat-1_5-SFT",
        "internlm/internlm2-chat-7b",
        "internlm/internlm2_5-7b-chat",
        "internlm/internlm2-7b",
        "internlm/internlm2-20b",
        "internlm/internlm2-base-7b",
        "internlm/internlm2-chat-20b",
        "internlm/internlm2-base-20b",
        "chujiezheng/internlm2-chat-20b-ExPO",
        "chujiezheng/internlm2-chat-7b-ExPO"
      ]
    },
    {
      "architecture_id": "Glm4ForCausalLM",
      "total_models": 84,
      "sample_models": [
        "zai-org/GLM-4-32B-0414",
        "zai-org/GLM-4-9B-0414",
        "unsloth/GLM-4-32B-0414-GGUF",
        "unsloth/GLM-4-9B-0414-GGUF",
        "zai-org/GLM-Z1-9B-0414",
        "zai-org/GLM-4-32B-Base-0414",
        "unsloth/GLM-Z1-9B-0414-GGUF",
        "mlx-community/GLM-4-9B-0414-4bit",
        "unsloth/GLM-Z1-32B-0414-bnb-4bit",
        "mlx-community/GLM-Z1-9B-0414-4bit"
      ]
    },
    {
      "architecture_id": "Qwen2MoeForCausalLM",
      "total_models": 83,
      "sample_models": [
        "Qwen/Qwen1.5-MoE-A2.7B",
        "Qwen/Qwen1.5-MoE-A2.7B-Chat",
        "Qwen/Qwen2-57B-A14B-Instruct",
        "Qwen/Qwen2-57B-A14B",
        "Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4",
        "Qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4",
        "yujiepan/qwen1.5-moe-tiny-random",
        "katuni4ka/tiny-random-qwen1.5-moe",
        "mlx-community/Qwen1.5-MoE-A2.7B-Chat-4bit",
        "Clevyby/lynn-A2.7B-rp-v1-14.3b-32k-Q5_K_S-GGUF"
      ]
    },
    {
      "architecture_id": "OpenLMForCausalLM",
      "total_models": 80,
      "sample_models": [
        "TRI-ML/mistral-supra",
        "TRI-ML/openlm-7b-code",
        "faidrap/dclm-german-ftr-1b-it",
        "faidrap/dclm-german-1b-it",
        "faidrap/alephalpha-synth-1b-it-v2",
        "nick11roberts/SL-discrep-chinchilla-rw-params9M_maxstep800-flop_2_50e16_step_810",
        "nick11roberts/SL-discrep-chinchilla-rw-params84M_maxstep1480-flop_4_00e17_step_1499",
        "nick11roberts/SL-discrep-chinchilla-rw-params22M_maxstep340-flop_2_50e16_step_353",
        "nick11roberts/SL-discrep-chinchilla-rw-params220M_maxstep560-flop_4_00e17_step_575",
        "nick11roberts/SL-discrep-chinchilla-rw-params5M_maxstep760-flop_6_27e15_step_385"
      ]
    },
    {
      "architecture_id": "CambrianQwenForCausalLM",
      "total_models": 77,
      "sample_models": [
        "nyu-visionx/Cambrian-S-7B",
        "nyu-visionx/Cambrian-S-3B",
        "Brucamian/70-qwen15-24-R",
        "nyu-visionx/Cambrian-S-1.5B",
        "Brucamian/70-qwen15-55-V",
        "Brucamian/70-qwen15-98-R-16",
        "Brucamian/70-qwen15-98-V-4",
        "Brucamian/70-qwen15-98-V-16",
        "Brucamian/70-qwen15-98-V",
        "Brucamian/70-qwen15-55-R"
      ]
    },
    {
      "architecture_id": "MiniCPMForCausalLM",
      "total_models": 74,
      "sample_models": [
        "openbmb/MiniCPM-2B-sft-bf16",
        "openbmb/MiniCPM4.1-8B",
        "openbmb/MiniCPM-1B-sft-bf16",
        "openbmb/MiniCPM4.1-8B-GPTQ",
        "openbmb/MiniCPM4-0.5B",
        "openbmb/MiniCPM4-8B",
        "openbmb/BitCPM4-1B",
        "openbmb/MiniCPM-2B-sft-fp32",
        "openbmb/MiniCPM-2B-dpo-fp16",
        "openbmb/MiniCPM-MoE-8x2B"
      ]
    },
    {
      "architecture_id": "RwkvForCausalLM",
      "total_models": 73,
      "sample_models": [
        "RWKV/v5-Eagle-7B-HF",
        "RWKV/rwkv-4-169m-pile",
        "beomi/KoRWKV-6B",
        "RWKV/rwkv-4-430m-pile",
        "RWKV/rwkv-raven-3b",
        "RWKV/rwkv-4-1b5-pile",
        "RWKV/rwkv-raven-7b",
        "RWKV/rwkv-raven-14b",
        "RWKV/rwkv-4-3b-pile",
        "RWKV/rwkv-4-14b-pile"
      ]
    },
    {
      "architecture_id": "JAISLMHeadModel",
      "total_models": 72,
      "sample_models": [
        "inceptionai/jais-13b-chat",
        "inceptionai/jais-13b",
        "inceptionai/jais-family-590m-chat",
        "inceptionai/jais-family-13b-chat",
        "inceptionai/jais-family-30b-16k-chat",
        "inceptionai/jais-family-6p7b-chat",
        "inceptionai/jais-family-30b-8k-chat",
        "katuni4ka/tiny-random-jais",
        "inceptionai/jais-family-590m",
        "inceptionai/jais-family-2p7b-chat"
      ]
    },
    {
      "architecture_id": "FalconH1ForCausalLM",
      "total_models": 71,
      "sample_models": [
        "tiiuae/Falcon-H1-0.5B-Base",
        "tiiuae/Falcon-H1-7B-Instruct",
        "tiiuae/Falcon-H1-34B-Base",
        "tiiuae/Falcon-H1-1.5B-Base",
        "tiiuae/Falcon-H1-7B-Base",
        "tiiuae/Falcon-H1-34B-Instruct",
        "tiiuae/Falcon-H1-1.5B-Deep-Instruct",
        "tiiuae/Falcon-H1-3B-Base",
        "tiiuae/Falcon-H1-1.5B-Deep-Base",
        "tiiuae/Falcon-H1-0.5B-Instruct"
      ]
    },
    {
      "architecture_id": "JambaForCausalLM",
      "total_models": 69,
      "sample_models": [
        "ai21labs/AI21-Jamba-Mini-1.5",
        "ai21labs/AI21-Jamba-Reasoning-3B",
        "ai21labs/Jamba-tiny-random",
        "ai21labs/AI21-Jamba-Large-1.5",
        "ai21labs/AI21-Jamba-Mini-1.6",
        "ai21labs/AI21-Jamba-Large-1.6",
        "ai21labs/Jamba-v0.1",
        "microsoft/Dayhoff-3b-UR90",
        "microsoft/Dayhoff-170m-GR",
        "microsoft/Dayhoff-170m-UR90"
      ]
    },
    {
      "architecture_id": "BioGptForCausalLM",
      "total_models": 67,
      "sample_models": [
        "microsoft/biogpt",
        "microsoft/BioGPT-Large-PubMedQA",
        "kirubel1738/biogpt-bioqa-lora-merged",
        "kirubel1738/biogpt-bioqa-8bit-openvino",
        "gayanin/ec-biogpt-masked-pubmed",
        "RobCzikkel/DoctorGPT",
        "Santosh-Gupta/EncephalitisGPT",
        "zequnl/molxpt",
        "AMAISIENG/finetuned_model3",
        "ImposterSyndromeIsReal/biogpt-healthcare-tuned"
      ]
    },
    {
      "architecture_id": "GPT2LMHeadCustomModel",
      "total_models": 65,
      "sample_models": [
        "bigcode/santacoder",
        "bigcode/santacoder-cf",
        "bigcode/santacoder-ldf",
        "lambdasec/santafixer",
        "aiswaryasankar/santacoder-finetuned-the-stack-bash",
        "rbiojout/santacoder-finetuned-odoo-15",
        "aiswaryasankar/santacoder-finetuned-dbrief-v2",
        "mrm8488/santacoder-finetuned-the-stack-swift",
        "jlpan/santacoder-finetuned-the-stack-bash",
        "flyover19/10032023"
      ]
    },
    {
      "architecture_id": "HunYuanDenseV1ForCausalLM",
      "total_models": 64,
      "sample_models": [
        "tencent/Hunyuan-MT-7B",
        "tencent/Hunyuan-MT-Chimera-7B-fp8",
        "tencent/Hunyuan-7B-Instruct",
        "tencent/HY-MT1.5-1.8B",
        "tencent/Hunyuan-MT-Chimera-7B",
        "tencent/Hunyuan-MT-7B-fp8",
        "tencent/HY-MT1.5-7B",
        "tencent/Hunyuan-0.5B-Pretrain",
        "tencent/Hunyuan-0.5B-Instruct",
        "tencent/Hunyuan-1.8B-Instruct"
      ]
    },
    {
      "architecture_id": "Mistral3ForConditionalGeneration",
      "total_models": 62,
      "sample_models": [
        "mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-8Bit",
        "ExaltedSlayer/mistralai-devstral-small-2-24b-instruct-2512-mlx-mxfp4",
        "maxence-bouvier/Devstral-Small-2-24B-Instruct-SINQ-4bit",
        "inferencerlabs/Devstral-Small-2-24B-Instruct-2512-MLX-6.5bit",
        "RedHatAI/Mistral-Small-3.2-24B-Instruct-2506-NVFP4",
        "mlx-community/Ministral-3-3B-Instruct-2512",
        "mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-4Bit",
        "jritchie-nullable/Devstral-Small-2-24B-Instruct-2512-MLX-4bit",
        "introvoyz041/Ministral-3-14B-Reasoning-2512-ShiningValiant3-mlx-4Bit",
        "mlx-community/mistralai_Devstral-Small-2-24B-Instruct-2512-MLX-MXFP4"
      ]
    },
    {
      "architecture_id": "MarianForCausalLM",
      "total_models": 62,
      "sample_models": [
        "AhmedSSoliman/MarianCausalLM",
        "rewicks/monolingual_de_8k-shared_ep1",
        "rewicks/monolingual_de_16k-shared_ep26",
        "rewicks/monolingual_de_16k-shared_ep29",
        "rewicks/monolingual_de_16k-shared_ep31",
        "rewicks/monolingual_de_16k-shared_ep32",
        "rewicks/monolingual_de_16k-shared_ep33",
        "rewicks/monolingual_de_16k-shared_ep35",
        "rewicks/monolingual_de_16k-shared_ep17",
        "rewicks/monolingual_de_16k-shared_ep25"
      ]
    },
    {
      "architecture_id": "T5WithLMHeadModel",
      "total_models": 61,
      "sample_models": [
        "google-t5/t5-3b",
        "google-t5/t5-11b",
        "unicamp-dl/ptt5-base-portuguese-vocab",
        "Salesforce/codet5-large",
        "Rostlab/prot_t5_xl_bfd",
        "Rostlab/prot_t5_base_mt_uniref50",
        "unicamp-dl/ptt5-small-portuguese-vocab",
        "gagan3012/k2t",
        "unicamp-dl/ptt5-large-portuguese-vocab",
        "unicamp-dl/ptt5-large-t5-vocab"
      ]
    },
    {
      "architecture_id": "SeedOssForCausalLM",
      "total_models": 60,
      "sample_models": [
        "lmstudio-community/Seed-OSS-36B-Instruct-MLX-8bit",
        "lmstudio-community/Seed-OSS-36B-Instruct-MLX-4bit",
        "lmstudio-community/Seed-OSS-36B-Instruct-MLX-5bit",
        "lmstudio-community/Seed-OSS-36B-Instruct-MLX-6bit",
        "ByteDance-Seed/Seed-OSS-36B-Instruct",
        "magiccodingman/Seed-OSS-36B-Instruct-unsloth-MagicQuant-Hybrid-GGUF",
        "ByteDance-Seed/Seed-OSS-36B-Base",
        "NousResearch/Hermes-4.3-36B",
        "QuantTrio/Seed-OSS-36B-Instruct-AWQ",
        "cyankiwi/Hermes-4.3-36B-AWQ-4bit"
      ]
    },
    {
      "architecture_id": "ApertusForCausalLM",
      "total_models": 57,
      "sample_models": [
        "swiss-ai/Apertus-8B-Instruct-2509",
        "swiss-ai/Apertus-8B-2509",
        "swiss-ai/Apertus-70B-2509",
        "NewEden/Apertus-8b-instruct-patched",
        "RedHatAI/Apertus-8B-Instruct-2509-FP8-dynamic",
        "mlx-community/Apertus-8B-Instruct-2509-bf16",
        "RedHatAI/Apertus-70B-Instruct-2509-FP8-dynamic",
        "kkaushik02/apertus-8b-instruct-full-finetuned",
        "mlx-community/Apertus-8B-Instruct-2509-4bit",
        "yujiepan/apertus-tiny-random"
      ]
    },
    {
      "architecture_id": "XGLMForCausalLM",
      "total_models": 57,
      "sample_models": [
        "facebook/incoder-1B",
        "facebook/xglm-564M",
        "facebook/xglm-1.7B",
        "facebook/xglm-2.9B",
        "KoboldAI/fairseq-dense-125M",
        "KoboldAI/fairseq-dense-2.7B",
        "KoboldAI/fairseq-dense-355M",
        "KoboldAI/fairseq-dense-13B",
        "facebook/incoder-6B",
        "KoboldAI/fairseq-dense-13B-Shinen"
      ]
    },
    {
      "architecture_id": "LISAForCausalLM",
      "total_models": 57,
      "sample_models": [
        "xinlai/LISA-13B-llama2-v1",
        "xinlai/LISA-7B-v1",
        "xinlai/LISA-7B-v1-explanatory",
        "xinlai/LISA-13B-llama2-v1-explanatory",
        "MBZUAI/GLaMM-GranD-Pretrained",
        "Yueha0/FoodLMM-Chat",
        "weic22/LaSagnA-7B",
        "MBZUAI/GLaMM-FullScope_v0",
        "MBZUAI/GLaMM-RegCap-RefCOCOg",
        "MBZUAI/GLaMM-RegCap-VG"
      ]
    },
    {
      "architecture_id": "LlavaQwenForCausalLM",
      "total_models": 55,
      "sample_models": [
        "lmms-lab/llava-onevision-qwen2-7b-ov",
        "lmms-lab/LLaVA-Video-7B-Qwen2",
        "lmms-lab/llava-onevision-qwen2-0.5b-ov",
        "lmms-lab/LLaVA-Video-72B-Qwen2",
        "lmms-lab/llava-onevision-qwen2-0.5b-si",
        "lmms-lab/llava-onevision-qwen2-7b-si",
        "lmms-lab/llava-onevision-qwen2-7b-ov-chat",
        "lmms-lab/LongVA-7B",
        "lmms-lab/LLaVA-Video-7B-Qwen2-Video-Only",
        "NCSOFT/VARCO-VISION-14B"
      ]
    },
    {
      "architecture_id": "LlavaQwen2ForCausalLM",
      "total_models": 55,
      "sample_models": [
        "qnguyen3/nanoLLaVA",
        "FreedomIntelligence/HuatuoGPT-Vision-7B",
        "apple/FastVLM-0.5B",
        "apple/FastVLM-1.5B",
        "onnx-community/FastVLM-0.5B-ONNX",
        "apple/FastVLM-7B",
        "zhaode/FastVLM-0.5B-Stage3",
        "riddhimanrana/fastvlm-0.5b-captions",
        "Xenova/nanoLLaVA",
        "EZCon/FastVLM-1.5B-4bit-g32-mixed_4_8-mlx"
      ]
    },
    {
      "architecture_id": "Moondream",
      "total_models": 55,
      "sample_models": [
        "vikhyatk/moondream1",
        "ThomasSimonini/Moondream2-streaming",
        "zesquirrelnator/moondream2-finetuneV2",
        "pranay-ar/moondream2",
        "nyap/cosmo-demo",
        "cemilakkoc/moondream",
        "ayoubkirouane/moondream2-image-captcha",
        "fal/moondream2-docci-instruct",
        "knpk/md-q",
        "gokaygokay/moondream-prompt"
      ]
    },
    {
      "architecture_id": "Phi4MMForCausalLM",
      "total_models": 54,
      "sample_models": [
        "microsoft/Phi-4-multimodal-instruct",
        "FriendliAI/Phi-4-multimodal-instruct",
        "Lexius/Phi-4-multimodal-instruct",
        "TakalaWang/Discussion-Phi-4-multimodal-instruct",
        "huihui-ai/Phi-4-multimodal-instruct-abliterated",
        "ntnu-smil/Phi-4-multimodal-instruct_QA_NoImage_0325_1964",
        "katuni4ka/tiny-random-phi-4-multimodal",
        "ysdede/Phi-4-mm-inst-asr-turkish-3",
        "ecoxial2007/CheX-Phi4MM-GRPO",
        "Yanis-Gerst/fine_tune"
      ]
    },
    {
      "architecture_id": "Phi3VForCausalLM",
      "total_models": 52,
      "sample_models": [
        "microsoft/Phi-3.5-vision-instruct",
        "TIGER-Lab/VLM2Vec-Full",
        "microsoft/Phi-3-vision-128k-instruct",
        "junyoung-00/Phi-3.5-vision-instruct-ChartCap",
        "DeepGlint-AI/UniME-Phi3.5-V-4.2B",
        "Lexius/Phi-3.5-vision-instruct",
        "mlx-community/Phi-3.5-vision-instruct-4bit",
        "microsoft/Phi-3-vision-128k-instruct-onnx-cpu",
        "FriendliAI/Phi-3.5-vision-instruct",
        "failspy/Phi-3-vision-128k-instruct-abliterated-alpha"
      ]
    },
    {
      "architecture_id": "BailingMoeV2ForCausalLM",
      "total_models": 51,
      "sample_models": [
        "inclusionAI/Ling-mini-2.0",
        "inclusionAI/Ling-flash-2.0",
        "inclusionAI/Ling-1T",
        "inclusionAI/Ring-1T",
        "inclusionAI/Ring-1T-FP8",
        "inclusionAI/Ling-flash-base-2.0",
        "inclusionAI/Ring-flash-2.0",
        "inclusionAI/Ling-1T-FP8",
        "inclusionAI/Ring-mini-2.0",
        "inclusionAI/Ring-1T-preview"
      ]
    },
    {
      "architecture_id": "TransformerForCausalLM",
      "total_models": 51,
      "sample_models": [
        "fla-hub/transformer-1.3B-100B",
        "fla-hub/transformer-340M-10B",
        "zhixuan-lin/test-model",
        "zhixuan-lin/transformer-llama-760m-longcrawl64-48b",
        "fla-hub/transformer-7B-mistral",
        "fla-hub/transformer-2.7B-100B",
        "Lanni-ni/transformer_6_8_512",
        "Lanni-ni/transformer_4_6_384",
        "zaydzuhri/transformer-8192-16M-test",
        "zaydzuhri/transformer-16M-test"
      ]
    },
    {
      "architecture_id": "LlavaMistralForCausalLM",
      "total_models": 50,
      "sample_models": [
        "liuhaotian/llava-v1.6-mistral-7b",
        "microsoft/llava-med-v1.5-mistral-7b",
        "billborkowski/llava-NousResearch_Nous-Hermes-2-Vision-GGUF",
        "SkunkworksAI/BakLLaVA-1",
        "DanielClough/Candle_llava-v1.6-mistral-7b",
        "NousResearch/Nous-Hermes-2-Vision-Alpha",
        "Veda0718/llava-med-v1.5-mistral-7b-finetuned",
        "99eren99/Turkish-BakLLaVa1.5-Mistral",
        "Theon1130/PMC_llava-v1.6-mistral-qformer",
        "shi-labs/CuMo-mistral-7b"
      ]
    },
    {
      "architecture_id": "Cohere2ForCausalLM",
      "total_models": 50,
      "sample_models": [
        "CohereLabs/c4ai-command-r7b-12-2024",
        "CohereLabs/c4ai-command-r7b-arabic-02-2025",
        "cyankiwi/command-a-reasoning-08-2025-AWQ-8bit",
        "CohereLabs/c4ai-command-a-03-2025",
        "CohereLabs/command-a-reasoning-08-2025",
        "unsloth/c4ai-command-a-03-2025-GGUF",
        "huihui-ai/c4ai-command-r7b-12-2024-abliterated",
        "CohereLabs/command-a-translate-08-2025",
        "cyankiwi/command-a-reasoning-08-2025-AWQ-4bit",
        "unsloth/c4ai-command-a-03-2025-bnb-4bit"
      ]
    },
    {
      "architecture_id": "LlamaModel",
      "total_models": 50,
      "sample_models": [
        "ngoan/NgoanYi",
        "Dongwei/Rationalyst_reasoning_datasets",
        "Zoram/Llama-Primus-Nemotron-70B-Instruct-bnb-4bit",
        "fetost/Llama-3.1-8b-instruct-quantized",
        "fracardia/Llama3.1-8b-Quant-4bits",
        "Novaciano/SEX_ROLEPLAY-3.2-1B-ao-int8wo-gs128",
        "bnb-community/Llama-3.2-3B-Instruct-bnb-4bit",
        "AlvinY34/Llama-3.2-3B-Instruct-8b-test",
        "Novaciano/SEX_ROLEPLAY-3.2-1B-bnb-4bit",
        "Novaciano/SEX_ROLEPLAY-3.2-1B-ao-int4wo-gs128"
      ]
    },
    {
      "architecture_id": "Exaone4ForCausalLM",
      "total_models": 47,
      "sample_models": [
        "LGAI-EXAONE/EXAONE-4.0-1.2B",
        "LGAI-EXAONE/EXAONE-4.0-32B",
        "LGAI-EXAONE/EXAONE-4.0-32B-FP8",
        "LGAI-EXAONE/EXAONE-4.0.1-32B",
        "LGAI-EXAONE/EXAONE-4.0-32B-AWQ",
        "LGAI-EXAONE/EXAONE-4.0-32B-GPTQ",
        "lmstudio-community/EXAONE-4.0-32B-MLX-4bit",
        "LGAI-EXAONE/EXAONE-4.0-1.2B-AWQ",
        "LGAI-EXAONE/EXAONE-4.0-1.2B-FP8",
        "hirundo-io/EXAONE-4.0-1.2B-improved-security"
      ]
    },
    {
      "architecture_id": "MiniMaxM2ForCausalLM",
      "total_models": 45,
      "sample_models": [
        "QuantTrio/MiniMax-M2-AWQ",
        "MiniMaxAI/MiniMax-M2.1",
        "MiniMaxAI/MiniMax-M2",
        "lmstudio-community/MiniMax-M2-MLX-8bit",
        "lmstudio-community/MiniMax-M2-MLX-4bit",
        "lmstudio-community/MiniMax-M2-MLX-6bit",
        "ModelCloud/MiniMax-M2-GPTQMODEL-W4A16",
        "cyankiwi/MiniMax-M2.1-AWQ-4bit",
        "cyankiwi/MiniMax-M2-REAP-162B-A10B-AWQ-4bit",
        "mlx-community/MiniMax-M2-4bit"
      ]
    },
    {
      "architecture_id": "Ernie4_5_MoeForCausalLM",
      "total_models": 43,
      "sample_models": [
        "baidu/ERNIE-4.5-21B-A3B-PT",
        "baidu/ERNIE-4.5-21B-A3B-Base-PT",
        "lmstudio-community/ERNIE-4.5-21B-A3B-MLX-4bit",
        "lmstudio-community/ERNIE-4.5-21B-A3B-MLX-6bit",
        "lmstudio-community/ERNIE-4.5-21B-A3B-MLX-8bit",
        "cyankiwi/ERNIE-4.5-21B-A3B-Thinking-AWQ-8bit",
        "baidu/ERNIE-4.5-21B-A3B-Thinking",
        "cyankiwi/ERNIE-4.5-21B-A3B-Thinking-AWQ-4bit",
        "baidu/ERNIE-4.5-300B-A47B-Base-PT",
        "baidu/ERNIE-4.5-21B-A3B-Paddle"
      ]
    },
    {
      "architecture_id": "MT5ForConditionalGeneration",
      "total_models": 42,
      "sample_models": [
        "IDEA-CCNL/Randeng-T5-784M-QA-Chinese",
        "jalbarracin/spanish-alpaca-mT5",
        "dejanseo/query-fanout",
        "obss/mt5-base-3task-highlight-combined3",
        "obss/mt5-small-3task-highlight-tquad2",
        "UBC-NLP/toucan-1.2B",
        "ashaduzzaman/mt5-finetuned-amazon-reviews",
        "obss/mt5-small-3task-highlight-combined3",
        "obss/mt5-base-3task-highlight-tquad2",
        "obss/mt5-small-3task-both-tquad2"
      ]
    },
    {
      "architecture_id": "BitLlamaForCausalLM",
      "total_models": 41,
      "sample_models": [
        "stardust-coder/myBit-Llama2-jp-127M-4",
        "bjoernp/micro-bitllama",
        "HachiML/myBit-Llama2-jp-127M-8",
        "stardust-coder/myBit-Llama2-jp-127M-8",
        "HachiML/myBit-Llama2-jp-127M-test-26",
        "sumo43/zephyr-7b-sft-full",
        "HachiML/BitLlama2-jp-127M-optim-3",
        "HachiML/myBit-Llama2-jp-127M-test-16",
        "HachiML/myBit-Llama2-jp-127M-4",
        "HachiML/myBit-Llama2-jp-127M-test-14"
      ]
    },
    {
      "architecture_id": "YiForCausalLM",
      "total_models": 40,
      "sample_models": [
        "llmware/dragon-yi-6b-v0",
        "TheBloke/Yi-34B-GPTQ",
        "TheBloke/Yi-34B-AWQ",
        "TheBloke/deepsex-34b-AWQ",
        "TheBloke/Yi-6B-200K-GPTQ",
        "Redwood0/deepsex-34b-4bpw-h8-exl2",
        "FreedomIntelligence/HuatuoGPT2-34B",
        "TheBloke/Yi-6B-GPTQ",
        "TheBloke/deepsex-34b-GPTQ",
        "yanismiraoui/Yi-6B-sharded"
      ]
    },
    {
      "architecture_id": "SparseMistralforCausalLM",
      "total_models": 40,
      "sample_models": [
        "thrunlab/Mistral_Sparse_refined_web_50p_graceful_True",
        "thrunlab/Mistral_Sparse_refined_web_70p_2024-03-22",
        "thrunlab/Mistral_Sparse_refined_web_50p_2024-03-29",
        "thrunlab/Mistral_Sparse_refined_web_relu_2024-03-11",
        "thrunlab/Mistral_Sparse_refined_web_70p_2024-03-12",
        "thrunlab/Mistral_Sparse_refined_web_relu_2024-03-01",
        "thrunlab/sparse_mistral_7b_refined_web_50p_2024-04-13",
        "thrunlab/Mistral_Sparse_refined_web_50p_2024-03-11",
        "thrunlab/Mistral_Sparse_refined_web_relu_2024-03-10",
        "thrunlab/Mistral_Sparse_refined_web_70p_2024-03-23"
      ]
    },
    {
      "architecture_id": "CustomLlamaForCausalLM",
      "total_models": 40,
      "sample_models": [
        "regisss/llama2-70b-fused-qkv-mlperf",
        "WinstonShum/llama-3-8b-Instruct-4bit-finetuned-guardrails",
        "jaymie23/fp16-llama3.1-8b-14",
        "jaymie23/fp16-llama3.1-8b-2",
        "jaymie23/fp16-llama3.1-8b-4",
        "jaymie23/fp16-llama3.1-8b-5",
        "jaymie23/fp16-llama3.1-8b-6",
        "jaymie23/fp16-llama3.1-8b-7",
        "jaymie23/fp16-llama3.1-8b-8",
        "jaymie23/fp16-llama3.1-8b-9"
      ]
    },
    {
      "architecture_id": "Llama4ForCausalLM",
      "total_models": 36,
      "sample_models": [
        "trl-internal-testing/tiny-Llama4ForCausalLM",
        "facebook/MobileLLM-R1.5-140M",
        "facebook/MobileLLM-R1-950M",
        "aloobun/minini-140m-base",
        "facebook/MobileLLM-R1-140M",
        "facebook/MobileLLM-R1.5-950M",
        "pruna-test/test-save-tiny-random-llama4-smashed",
        "jcaip/Llama-4-Scout-17B-two-layers-only-testing",
        "aloobun/minini-140m-it",
        "facebook/MobileLLM-R1-950M-base"
      ]
    },
    {
      "architecture_id": "DogeForCausalLM",
      "total_models": 35,
      "sample_models": [
        "SmallDoge/Doge-20M",
        "SmallDoge/Doge-160M-Instruct",
        "SmallDoge/Doge-20M-Instruct",
        "SmallDoge/Doge-60M",
        "SmallDoge/Doge-40M-MoE-checkpoint",
        "SmallDoge/Doge-60M-Instruct-SFT",
        "SmallDoge/Doge-60M-checkpoint",
        "SmallDoge/Doge-160M-checkpoint",
        "SmallDoge/Doge-20M-checkpoint",
        "SmallDoge/Doge-160M"
      ]
    },
    {
      "architecture_id": "NotaGenLMHeadModel",
      "total_models": 35,
      "sample_models": [
        "efraimdahl/RagtimeSync_vcond",
        "efraimdahl/LiederMetric_xattn_gate",
        "efraimdahl/RagtimeSpect_xattn",
        "efraimdahl/LiederSpectBass_xattn",
        "efraimdahl/RagtimeMetric_enc_vcond_lowgt",
        "efraimdahl/notagen_Lieder_base",
        "efraimdahl/LiederMetric_xattn",
        "efraimdahl/RagtimeSpect_enc_vcond_lowgate",
        "efraimdahl/RagtimeMetricRH_enc_inattn",
        "efraimdahl/notagen_LiederSpect_xattn_gate"
      ]
    },
    {
      "architecture_id": "BitNetForCausalLM",
      "total_models": 34,
      "sample_models": [
        "microsoft/bitnet-b1.58-2B-4T",
        "microsoft/bitnet-b1.58-2B-4T-bf16",
        "iSolver-AI/FEnet",
        "mlx-community/bitnet-b1.58-2B-4T-4bit",
        "mlx-community/bitnet-b1.58-2B-4T-8bit",
        "mlx-community/bitnet-b1.58-2B-4T",
        "mike23415/playwebit",
        "mlx-community/bitnet-b1.58-2B-4T-6bit",
        "Bifrost-AI/Bitnet-b1.58-bifrost-sol-2B-4T",
        "vsingh10/fine-tuned-bitnet"
      ]
    },
    {
      "architecture_id": "Ovis",
      "total_models": 33,
      "sample_models": [
        "AIDC-AI/Ovis2-4B",
        "AIDC-AI/Ovis2-34B",
        "AIDC-AI/Ovis2-1B",
        "AIDC-AI/Ovis2-4B-GPTQ-Int4",
        "AIDC-AI/Ovis1.6-Gemma2-9B",
        "AIDC-AI/Ovis1.6-Llama3.2-3B",
        "AIDC-AI/Ovis2-2B",
        "AIDC-AI/Ovis2-8B",
        "AIDC-AI/Ovis2-8B-GPTQ-Int4",
        "AIDC-AI/Ovis2-16B"
      ]
    },
    {
      "architecture_id": "InternLM3ForCausalLM",
      "total_models": 33,
      "sample_models": [
        "internlm/internlm3-8b-instruct",
        "huihui-ai/internlm3-8b-instruct-abliterated",
        "second-state/internlm3-8b-instruct-GGUF",
        "internlm/internlm3-8b-instruct-awq",
        "internlm/internlm3-8b-instruct-gptq-int4",
        "internlm/internlm3-8b-instruct-smoothquant-int8",
        "internlm/internlm3-8b-instruct-smoothquant-fp8",
        "nex-agi/internlm3-8B-Nex-N1",
        "gaianet/internlm3-8b-instruct-GGUF",
        "jakiAJK/internlm3-8b-instruct_GPTQ-int4"
      ]
    },
    {
      "architecture_id": "NemotronForCausalLM",
      "total_models": 32,
      "sample_models": [
        "nvidia/Minitron-8B-Base",
        "nvidia/Nemotron-Mini-4B-Instruct",
        "badaoui/tiny-random-NemotronForCausalLM",
        "thhaus/nemotron3-8b",
        "nvidia/Minitron-4B-Base",
        "mgoin/Nemotron-4-340B-Base-hf-FP8",
        "mgoin/Nemotron-4-340B-Instruct-hf-FP8",
        "mgoin/Nemotron-4-340B-Instruct-vllm",
        "failspy/Nemotron-4-340B-Instruct-SafeTensors",
        "mgoin/nemotron-3-8b-chat-4k-sft-hf"
      ]
    },
    {
      "architecture_id": "MorphT5SumForConditionalGeneration",
      "total_models": 32,
      "sample_models": [
        "mrapacz/interlinear-en-greta-emb-sum-diacritics-bh",
        "mrapacz/interlinear-en-mt5-base-emb-sum-normalized-bh",
        "mrapacz/interlinear-pl-mt5-large-emb-sum-normalized-bh",
        "mrapacz/interlinear-en-mt5-base-emb-sum-diacritics-bh",
        "mrapacz/interlinear-en-philta-emb-sum-diacritics-bh",
        "mrapacz/interlinear-pl-mt5-large-emb-sum-diacritics-ob",
        "mrapacz/interlinear-en-mt5-large-emb-sum-diacritics-bh",
        "mrapacz/interlinear-pl-mt5-large-emb-sum-diacritics-bh",
        "mrapacz/interlinear-pl-mt5-large-emb-sum-normalized-ob",
        "mrapacz/interlinear-en-philta-emb-sum-normalized-bh"
      ]
    },
    {
      "architecture_id": "Qwen2Model",
      "total_models": 32,
      "sample_models": [
        "bnb-community/Qwen2.5-0.5B-bnb-4bit",
        "bnb-community/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit",
        "SylvanL/ChatTCM-7B-Pretrain",
        "wenbopan/Faro-Qwen-4B",
        "wenbopan/Faro-Qwen-1.8B",
        "AaronHuangWei/Qwen2.5-7B-Instruct-bnb-4bit",
        "jnanliu/LiveMath-Judge",
        "medmekk/Qwen2.5-0.5B-Instruct-ao-int8da8w",
        "ginipick/QwQ-32B-NF4",
        "tanvij/nvila_quant1"
      ]
    },
    {
      "architecture_id": "XLNetLMHeadModel",
      "total_models": 31,
      "sample_models": [
        "xlnet/xlnet-base-cased",
        "xlnet/xlnet-large-cased",
        "hfl/chinese-xlnet-base",
        "hajime9652/xlnet-japanese",
        "textattack/xlnet-base-cased-CoLA",
        "model-attribution-challenge/xlnet-base-cased",
        "Xiugapurin/xlnet-mid",
        "dingli/xlnet_nlp_smartdispatch",
        "Xiugapurin/xlnet-base",
        "textattack/xlnet-base-cased-MRPC"
      ]
    },
    {
      "architecture_id": "MolmoForCausalLM",
      "total_models": 31,
      "sample_models": [
        "allenai/Molmo-7B-D-0924",
        "allenai/Molmo-7B-O-0924",
        "cyan2k/molmo-7B-O-bnb-4bit",
        "allenai/Molmo-72B-0924",
        "ronantakizawa/molmo-7b-d-awq",
        "andresrp/Molmo-7B-D-0924-curator-lab-2",
        "mlx-community/Molmo-7B-D-0924-4bit",
        "ghazishazan/VideoMolmo",
        "mlx-community/Molmo-7B-D-0924-bf16",
        "parasail-ai/Molmo-7B-D-0924"
      ]
    },
    {
      "architecture_id": "MorphT5ConcatForConditionalGeneration",
      "total_models": 31,
      "sample_models": [
        "mrapacz/interlinear-en-mt5-base-emb-concat-normalized-ob",
        "mrapacz/interlinear-en-greta-emb-concat-diacritics-ob",
        "mrapacz/interlinear-pl-mt5-base-emb-concat-normalized-bh",
        "mrapacz/interlinear-pl-greta-emb-concat-diacritics-bh",
        "mrapacz/interlinear-en-mt5-base-emb-concat-diacritics-ob",
        "mrapacz/interlinear-pl-mt5-base-emb-concat-normalized-ob",
        "mrapacz/interlinear-en-greta-emb-concat-normalized-ob",
        "mrapacz/interlinear-en-philta-emb-concat-normalized-ob",
        "mrapacz/interlinear-pl-mt5-large-emb-concat-diacritics-ob",
        "mrapacz/interlinear-pl-philta-emb-concat-diacritics-bh"
      ]
    },
    {
      "architecture_id": "DbrxForCausalLM",
      "total_models": 30,
      "sample_models": [
        "trl-internal-testing/tiny-DbrxForCausalLM",
        "Rocketknight1/dbrx-tiny-random",
        "Undi95/dbrx-base",
        "alpindale/dbrx-instruct",
        "LnL-AI/dbrx-base-converted-v2",
        "texanrangee/2f00c09c-4459-447b-8515-667861a01d56",
        "yujiepan/dbrx-tiny256-random",
        "eitanturok/dbrx-tiny",
        "katuni4ka/tiny-random-dbrx",
        "LnL-AI/dbrx-base-converted-v2-4bit-gptq-marlin"
      ]
    },
    {
      "architecture_id": "MptForCausalLM",
      "total_models": 30,
      "sample_models": [
        "explosion-testing/mpt-test",
        "yujiepan/mpt-tiny-random",
        "aegon-h/mpt-7b",
        "ybelkada/mpt-7b-bf16-sharded",
        "ybelkada/mpt-7b-transformers",
        "nqzfaizal77ai/matrix-paragon-reinit-540m-zero",
        "VTSNLP/base_LLM_RAW_400M_1",
        "tsavage68/mpt_1000_STEPS_1e6_rate_03_beta_DPO",
        "tsavage68/mpt_1000_STEPS_1e5_rate_05_beta_DPO",
        "tsavage68/mpt_1000_STEPS_1e6_rate_05_beta_DPO"
      ]
    },
    {
      "architecture_id": "MPLUGOwl2LlamaForCausalLM",
      "total_models": 30,
      "sample_models": [
        "q-future/q-align-iqa",
        "q-future/q-align-quality",
        "teowu/q-instruct-plus-one-align-preview-v0.3",
        "VQA-CityU/smooth-mu-sigma",
        "VQA-CityU/Compare2Score_4",
        "VQA-CityU/Compare2Score_5",
        "q-future/q-align-image",
        "VQA-CityU/kadis_yfcc_compare_new",
        "VQA-CityU/Q-Align_4",
        "q-future/t2i-scorer-it"
      ]
    },
    {
      "architecture_id": "OLMoForCausalLM",
      "total_models": 29,
      "sample_models": [
        "allenai/OLMo-1B",
        "allenai/MolmoE-1B-0924",
        "allenai/OLMo-7B",
        "allenai/OLMo-7B-SFT",
        "allenai/OLMo-7B-0424",
        "allenai/OLMo-7B-Twin-2T",
        "reubk/MolmoE-1B-0924-NF4",
        "jasonkrone/olmo_1b_toks_75b-mc-finetune",
        "joseagmz/olmo-7B-Tinybook-epochs-1-lr-0002",
        "jasonkrone/olmo_1b_toks_75b-mc-finetune-hpo-lr-with-mmlu"
      ]
    },
    {
      "architecture_id": "ConstrainedLlamaForCausalLM",
      "total_models": 29,
      "sample_models": [
        "AdoCleanCode/SNAC-Denoiser-LLaMA-500M-snac_v2_test_1gpu",
        "AdoCleanCode/SNAC-Denoiser-LLaMA-500M-snac_v3_test_1gpu",
        "AdoCleanCode/4B_DAC-SE2-2800-checkpoint-2800",
        "AdoCleanCode/1B_DAC-SE2-11400-checkpoint-11400",
        "AdoCleanCode/TBD-LLaMA-DAC-Denoiser-checkpoint-7000",
        "AdoCleanCode/TBD-LLaMA-DAC-Denoiser-checkpoint-1600",
        "AdoCleanCode/TBD-LLaMA-DAC-Denoiser-checkpoint-7800",
        "AdoCleanCode/TBD-LLaMA-DAC-Denoiser-checkpoint-8600",
        "AdoCleanCode/TBD-LLaMA-DAC-Denoiser-checkpoint-3600",
        "AdoCleanCode/TBD-LLaMA-DAC-Denoiser-checkpoint-13400"
      ]
    },
    {
      "architecture_id": "OrionForCausalLM",
      "total_models": 27,
      "sample_models": [
        "OrionStarAI/Orion-14B-Chat",
        "OrionStarAI/Orion-14B-LongChat",
        "OrionStarAI/Orion-14B-Base-Special",
        "OrionStarAI/Orion-14B-Base-Int4",
        "OrionStarAI/Orion-14B-Chat-Plugin",
        "katuni4ka/tiny-random-orion",
        "hyper-accel/tiny-random-orion",
        "wngkdud/Orion-14B-Chat_SFT",
        "Rookied/rinko-test",
        "nebchi/kor-resume-Orion-14B"
      ]
    },
    {
      "architecture_id": "HyperLlamaForCausalLM",
      "total_models": 27,
      "sample_models": [
        "liu-nlp/hyperllama-572m-icelandic-2x",
        "liu-nlp/hyperllama-572m-multilingual-1x-cloned",
        "liu-nlp/hyperllama-572m-persian-2x-matching-1x",
        "liu-nlp/hyperllama-572m-icelandic-1x-cloned-matching-1x",
        "liu-nlp/hyperllama-572m-estonian-2x-matching-1x",
        "liu-nlp/hyperllama-572m-persian-1x-cloned",
        "liu-nlp/hyperllama-572m-swedish-1x-cloned-matching-1x",
        "liu-nlp/hyperllama-572m-english-1x-cloned",
        "liu-nlp/hyperllama-572m-faroese-2x",
        "liu-nlp/hyperllama-572m-persian-1x-cloned-matching-1x"
      ]
    },
    {
      "architecture_id": "MorphT5AutoForConditionalGeneration",
      "total_models": 27,
      "sample_models": [
        "mrapacz/interlinear-en-greta-emb-auto-diacritics-bh",
        "mrapacz/interlinear-en-philta-emb-auto-normalized-ob",
        "mrapacz/interlinear-pl-philta-emb-auto-diacritics-ob",
        "mrapacz/interlinear-pl-mt5-large-emb-auto-normalized-ob",
        "mrapacz/interlinear-pl-mt5-large-emb-auto-diacritics-bh",
        "mrapacz/interlinear-pl-mt5-large-emb-auto-normalized-bh",
        "mrapacz/interlinear-pl-mt5-base-emb-auto-diacritics-ob",
        "mrapacz/interlinear-pl-greta-emb-auto-normalized-ob",
        "mrapacz/interlinear-en-mt5-large-emb-auto-normalized-ob",
        "mrapacz/interlinear-pl-mt5-base-emb-auto-normalized-bh"
      ]
    },
    {
      "architecture_id": "KimiLinearForCausalLM",
      "total_models": 26,
      "sample_models": [
        "moonshotai/Kimi-Linear-48B-A3B-Instruct",
        "cerebras/Kimi-Linear-REAP-35B-A3B-Instruct",
        "huihui-ai/Huihui-Kimi-Linear-REAP-35B-A3B-Instruct-abliterated",
        "NexVeridian/Kimi-Linear-REAP-35B-A3B-Instruct-8bit",
        "mlx-community/Kimi-Linear-48B-A3B-Instruct-6bit",
        "mlx-community/Kimi-Linear-48B-A3B-Instruct-8bit",
        "mlx-community/Kimi-Linear-48B-A3B-Instruct-mlx-bf16",
        "nightmedia/Kimi-Linear-REAP-35B-A3B-Instruct-mxfp4-mlx",
        "NexVeridian/Kimi-Linear-REAP-35B-A3B-Instruct-4bit",
        "NexVeridian/Kimi-Linear-REAP-35B-A3B-Instruct-6bit"
      ]
    },
    {
      "architecture_id": "AfmoeForCausalLM",
      "total_models": 26,
      "sample_models": [
        "arcee-ai/Trinity-Nano-Preview",
        "arcee-ai/Trinity-Mini",
        "arcee-ai/Trinity-Nano-Base",
        "arcee-ai/Trinity-Mini-Base",
        "arcee-ai/Trinity-Nano-Base-Pre-Anneal",
        "arcee-ai/Trinity-Mini-Base-Pre-Anneal",
        "mlx-community/Trinity-Mini-4bit",
        "cyankiwi/Trinity-Mini-AWQ-8bit",
        "cyankiwi/Trinity-Mini-AWQ-4bit",
        "mlx-community/Trinity-Nano-Preview-4bit"
      ]
    },
    {
      "architecture_id": "DeepseekV32ForCausalLM",
      "total_models": 25,
      "sample_models": [
        "deepseek-ai/DeepSeek-V3.2",
        "deepseek-ai/DeepSeek-V3.2-Exp",
        "deepseek-ai/DeepSeek-V3.2-Speciale",
        "mlx-community/DeepSeek-V3.2-4bit",
        "QuantTrio/DeepSeek-V3.2-AWQ",
        "mlx-community/DeepSeek-V3.2-Speciale-4bit",
        "inferencerlabs/DeepSeek-V3.2-Speciale-MLX-5.5bit",
        "inferencerlabs/DeepSeek-V3.2-MLX-5.5bit",
        "inferencerlabs/DeepSeek-V3.2-MLX-4.8bit",
        "inferencerlabs/DeepSeek-V3.2-Speciale-MLX-4.8bit"
      ]
    },
    {
      "architecture_id": "GPTNeoXModel",
      "total_models": 25,
      "sample_models": [
        "metterian/kullm-polyglot-12.8b-v1",
        "fbaldassarri/iGeniusAI_Italia-9B-Instruct-v0.1-autoround-int8-gs128-auto-asym",
        "fbaldassarri/iGeniusAI_Italia-9B-Instruct-v0.1-autogptq-int4-gs64-sym",
        "fbaldassarri/iGeniusAI_Italia-9B-Instruct-v0.1-autoround-int4-gs128-asym",
        "fbaldassarri/iGeniusAI_Italia-9B-Instruct-v0.1-autoround-int4-gs128-sym",
        "fbaldassarri/iGeniusAI_Italia-9B-Instruct-v0.1-autoround-int8-gs128-asym",
        "fbaldassarri/iGeniusAI_Italia-9B-Instruct-v0.1-autoround-int8-gs128-sym",
        "fbaldassarri/iGeniusAI_Italia-9B-Instruct-v0.1-autoround-int8-gs64-auto-sym",
        "fbaldassarri/iGeniusAI_Italia-9B-Instruct-v0.1-autogptq-int8-gs64-auto-asym",
        "fbaldassarri/iGeniusAI_Italia-9B-Instruct-v0.1-autogptq-int4-gs128-asym"
      ]
    },
    {
      "architecture_id": "Phi3SmallForCausalLM",
      "total_models": 24,
      "sample_models": [
        "microsoft/Phi-3-small-8k-instruct",
        "microsoft/Phi-3-small-128k-instruct",
        "numind/NuExtract-large",
        "microsoft/Phi-3-small-8k-instruct-onnx-cuda",
        "mlx-community/Phi-3-small-8k-instruct-AQ4_32",
        "mlx-community/Phi-3-small-8k-instruct-aq4_64",
        "anileo1/phi3-small-classifier-merged",
        "FriendliAI/Phi-3-small-8k-instruct",
        "lucafirefox/Phi-3-small-8k-instruct_handler",
        "ChunB1/Phi-3-interact"
      ]
    },
    {
      "architecture_id": "ChatGLMForConditionalGeneration",
      "total_models": 24,
      "sample_models": [
        "zai-org/LongWriter-glm4-9b",
        "CausalLM/miniG",
        "IAAR-Shanghai/xVerify-9B-C",
        "zai-org/LongAlign-6B-64k",
        "zai-org/LongAlign-6B-64k-base",
        "model-scope/glm-4-9b-chat-GPTQ-Int4",
        "yujiepan/chatglm3-tiny-random",
        "zwzzz/MentalGLM-chat",
        "thu-coai/CritiqueLLM-6B",
        "byroneverson/LongWriter-glm4-9b-abliterated"
      ]
    },
    {
      "architecture_id": "SlidingWindowForCausalLM",
      "total_models": 24,
      "sample_models": [
        "Lanni-ni/hard_3gram_4_6_384_babylm",
        "Lanni-ni/hard_3gram_babylm_10m_4_6_384",
        "Lanni-ni/hard_3gram_babylm_10m_2layer",
        "Lanni-ni/hard_5gram_babylm_10m_2layer",
        "Lanni-ni/hard_3gram_babylm_10m_4layer",
        "Lanni-ni/hard_5gram_babylm_10m_4layer",
        "Lanni-ni/hard_3gram_babylm_100m_2layer",
        "Lanni-ni/hard_5gram_babylm_100m_4layer",
        "Lanni-ni/hard_5gram_pile_4layer",
        "Lanni-ni/sliding_window_4_6_384_w1_"
      ]
    },
    {
      "architecture_id": "LlavaLlamaModel",
      "total_models": 23,
      "sample_models": [
        "Efficient-Large-Model/VILA1.5-3b",
        "Efficient-Large-Model/NVILA-8B",
        "Efficient-Large-Model/NVILA-15B",
        "Efficient-Large-Model/VILA1.5-13b",
        "Efficient-Large-Model/Llama-3-VILA1.5-8B",
        "Efficient-Large-Model/NVILA-Lite-8B",
        "Efficient-Large-Model/NVILA-8B-Video",
        "Efficient-Large-Model/VILA1.5-3b-s2",
        "Efficient-Large-Model/VILA1.5-3b-AWQ",
        "Efficient-Large-Model/NVILA-Lite-15B"
      ]
    },
    {
      "architecture_id": "LlavaPhiForCausalLM",
      "total_models": 23,
      "sample_models": [
        "MBZUAI/LLaVA-Phi-3-mini-4k-instruct",
        "FreedomIntelligence/ALLaVA-Phi2-2_7B",
        "marianna13/llava-phi-2-3b",
        "LanguageBind/MoE-LLaVA-Phi2-Stage2",
        "LanguageBind/MoE-LLaVA-Phi2-Stage2-384",
        "FreedomIntelligence/ALLaVA-3B",
        "FreedomIntelligence/ALLaVA-3B-Longer",
        "aimagelab/LLaVA_MORE-phi_4-finetuning",
        "marianna13/llava-phi-2-3b-siglip",
        "RaviNaik/Llava-Phi2"
      ]
    },
    {
      "architecture_id": "MBartForCausalLM",
      "total_models": 23,
      "sample_models": [
        "gojiteji/mbart-causal-lm",
        "amirhamza11/mBart-large_nwp_finetuning_test3",
        "nikhatbegum/nikhatbegum.english-telugu-colloquial-translator",
        "antphb/DS-Chatbox-mbart-large-50",
        "yoonjae22/Llama2-13b-ko-en-Translate",
        "RichardErkhov/IlyaGusev_-_mbart_ru_sum_gazeta-8bits",
        "hf-tiny-model-private/tiny-random-MBartForCausalLM",
        "RichardErkhov/facebook_-_mbart-large-50-many-to-many-mmt-4bits",
        "RichardErkhov/facebook_-_mbart-large-50-many-to-many-mmt-8bits",
        "guldasta/indic-gpt2-Hindi_Gen"
      ]
    },
    {
      "architecture_id": "GraniteMoeForCausalLM",
      "total_models": 22,
      "sample_models": [
        "ibm-research/PowerMoE-3b",
        "ibm-granite/granite-3.1-3b-a800m-instruct",
        "ibm-granite/granite-3.1-1b-a400m-base",
        "ibm-granite/granite-3.1-1b-a400m-instruct",
        "ibm-granite/granite-3.0-3b-a800m-instruct",
        "ibm-granite/granite-3.0-1b-a400m-base",
        "ibm-granite/granite-3.0-3b-a800m-base",
        "ibm-granite/granite-3.1-3b-a800m-base",
        "ibm-granite/granite-3.0-1b-a400m-instruct",
        "ibm-granite/granite-guardian-3.2-3b-a800m"
      ]
    },
    {
      "architecture_id": "DetikzifyForConditionalGeneration",
      "total_models": 22,
      "sample_models": [
        "nllg/detikzify-v2.5-8b",
        "nllg/detikzify-v2-8b",
        "tok2000/detikzify-1B-trained",
        "tok2000/detikzify-1B-trained_2048",
        "tok2000/detikzify-1B-trained_curriculum_2_3",
        "tok2000/detikzify-1B-trained_1024",
        "tok2000/detikzify-1B-trained_curriculum_1_4",
        "tok2000/detikzify-1B-trained_curriculum_3_2",
        "tok2000/detikzify-1B-grpo-arxivcap500_n256",
        "tok2000/detikzify-1B-grpo-arxivcap2k_n16"
      ]
    },
    {
      "architecture_id": "BartForConditionalGeneration",
      "total_models": 22,
      "sample_models": [
        "RUCAIBox/elmer",
        "Tianlin668/MentalBART",
        "shahin-as/bart-large-sentence-compression",
        "NaA-IA/Charlotte-tchat",
        "Miguelpef/bart-base-lora-3DPrompt",
        "ssarathi/Text_Summarizer_Using_Transformers",
        "fwp/BART-base-HotpotQA-finetune",
        "fwp/BART-large-HotpotQA-finetune",
        "QGEval2024/bart-large-hotpotqa-finetune-qg",
        "uzw/bart-large-question-generation"
      ]
    },
    {
      "architecture_id": "FP8Qwen3ForCausalLM",
      "total_models": 22,
      "sample_models": [
        "xihc-ucb/Qwen3-8B-Base-train-Quasar-1016",
        "xihc-ucb/Qwen3-8B-Base-train-Quasar-0809",
        "xihc-ucb/Qwen3-32B-train-Quasar-1016",
        "xihc-ucb/Qwen3-4B-train-Quasar-0807",
        "xihc-ucb/Qwen3-8B-infer-Quasar-1016",
        "xihc-ucb/Qwen3-0.6B-train-Quasar-0809",
        "xihc-ucb/Qwen3-8B-train-Quasar-0809",
        "xihc-ucb/Qwen3-32B-train-Quasar-0809",
        "xihc-ucb/Qwen3-1.7B-train-Quasar-1016",
        "xihc-ucb/Qwen3-8B-train-Quasar-1016"
      ]
    },
    {
      "architecture_id": "CustomQwen2ForCausalLM",
      "total_models": 22,
      "sample_models": [
        "Kameshr/Qwen-safety-Med-3B-pt",
        "Kameshr/Qwen-safety-High-32B-it",
        "Kameshr/Qwen-anti-safety-High-7B-it",
        "Kameshr/Qwen-anti-safety-Med-14B-it",
        "Kameshr/Qwen-anti-safety-High-14B-it",
        "Kameshr/Qwen-anti-safety-Med-7B-it",
        "Kameshr/Qwen-safety-Med-14B-it",
        "Kameshr/Qwen-anti-safety-Low-14B-it",
        "Kameshr/Qwen-anti-safety-Low-7B-pt",
        "Kameshr/Qwen-safety-Med-7B-pt"
      ]
    },
    {
      "architecture_id": "DreamModel",
      "total_models": 21,
      "sample_models": [
        "Dream-org/Dream-v0-Base-7B",
        "Dream-org/Dream-v0-Instruct-7B",
        "Dream-org/Dream-Coder-v0-Instruct-7B",
        "pbansal/Dream-Coder-v0-Instruct-7B-Adjust",
        "d3LLM/d3LLM_Dream",
        "Dream-org/Dream-Coder-v0-Base-7B",
        "Dream-org/DreamOn-v0-7B",
        "mlx-community/DiffuCoder-7B-cpGRPO-8bit",
        "mlx-community/DiffuCoder-7B-cpGRPO-4bit",
        "exdysa/Dream-v0-Instruct-7B"
      ]
    },
    {
      "architecture_id": "Gemma3nForConditionalGeneration",
      "total_models": 21,
      "sample_models": [
        "RedHatAI/gemma-3n-E4B-it-FP8-dynamic",
        "mlx-community/gemma-3n-E2B-it-lm-4bit",
        "mlx-community/gemma-3n-E4B-it-lm-4bit",
        "mlx-community/gemma-3n-E2B-it-lm-bf16",
        "mlx-community/gemma-3n-E4B-it-lm-bf16",
        "mlx-community/Huihui-gemma-3n-E4B-it-abliterated-lm-8bit",
        "mlx-community/Huihui-gemma-3n-E4B-it-abliterated-lm-4bit",
        "mlx-community/Huihui-gemma-3n-E4B-it-abliterated-lm-6bit",
        "Slyracoon23/medical-gemma3n-emergency-response",
        "sudoping01/bambara-llm-exp3"
      ]
    },
    {
      "architecture_id": "XverseForCausalLM",
      "total_models": 21,
      "sample_models": [
        "xverse/XVERSE-7B-Chat",
        "xverse/XVERSE-13B",
        "xverse/XVERSE-13B-Chat",
        "xverse/XVERSE-7B",
        "xverse/XVERSE-7B-Chat-GPTQ-Int4",
        "xverse/XVERSE-65B",
        "xverse/XVERSE-MoE-A4.2B",
        "xverse/XVERSE-65B-2",
        "xverse/XVERSE-MoE-A4.2B-Chat",
        "xverse/XVERSE-13B-256K"
      ]
    },
    {
      "architecture_id": "AquilaForCausalLM",
      "total_models": 21,
      "sample_models": [
        "BAAI/AquilaChat2-7B",
        "BAAI/Aquila2-34B",
        "BAAI/AquilaChat2-34B",
        "BAAI/Aquila2-7B",
        "BAAI/AquilaChat2-34B-16K",
        "BAAI/AquilaChat2-7B-16K",
        "BAAI/AquilaMed-RL",
        "h2oai/h2ogpt-16k-aquilachat2-34b",
        "BAAI/AquilaChat2-70B-Expr",
        "BAAI/Aquila2-70B-Expr"
      ]
    },
    {
      "architecture_id": "KORMoForCausalLM",
      "total_models": 21,
      "sample_models": [
        "KORMo-Team/KORMo-10B-sft",
        "KORMo-Team/KORMo-10B-base",
        "kormo-lm/KORMo-10B-stage2",
        "kormo-lm/KORMo-10B-stage1",
        "tiny-random/kormo",
        "kormo-lm/KORMo-0view-1B-60BT",
        "Chang-Su/kormo-upload-test",
        "kormo-lm/EnKo_60BT__cl100k__4096_packed",
        "kormo-lm/EnKo_60BT__llama3__4096_packed",
        "kormo-lm/EnKo_60BT__EPK_125k__4096_packed"
      ]
    },
    {
      "architecture_id": "LlavaForConditionalGeneration",
      "total_models": 21,
      "sample_models": [
        "cyankiwi/Apriel-1.5-15b-Thinker-AWQ-8bit",
        "magiccodingman/Apriel-1.5-15b-Thinker-unsloth-MagicQuant-Hybrid-GGUF",
        "cyankiwi/Apriel-1.5-15b-Thinker-AWQ-4bit",
        "RedHatAI/pixtral-12b-FP8-dynamic",
        "mlx-community/Apriel-1.5-15b-Thinker-4bit",
        "Eren-Senoglu/llava-med-v1.5-mistral-7b-hf",
        "wnkh/llava-med-v1.5-mistral-7b-hf",
        "pchamart/Apriel-1.6-15B-Thinker-Q8",
        "unsloth/Apriel-1.5-15b-Thinker",
        "NexVeridian/Apriel-1.5-15b-Thinker-4bit"
      ]
    },
    {
      "architecture_id": "SDARForCausalLM",
      "total_models": 20,
      "sample_models": [
        "JetLM/SDAR-8B-Chat-b32",
        "JetLM/SDAR-8B-Chat-b16",
        "JetLM/SDAR-1.7B-Chat-b16",
        "JetLM/SDAR-1.7B-Chat",
        "JetLM/SDAR-4B-Chat",
        "JetLM/SDAR-8B-Chat",
        "JetLM/SDAR-8B-Chat-b8",
        "exdysa/TraDo-4B-Instruct",
        "exdysa/SDAR-1.7B-Chat",
        "OpenMOSS-Team/DiRL-8B-Instruct"
      ]
    },
    {
      "architecture_id": "Lfm2MoeForCausalLM",
      "total_models": 20,
      "sample_models": [
        "LiquidAI/LFM2-8B-A1B",
        "mlx-community/LFM2-8B-A1B-4bit",
        "mlx-community/LFM2-8B-A1B-8bit-MLX",
        "mlx-community/LFM2-8B-A1B-3bit-MLX",
        "cyankiwi/LFM2-8B-A1B-AWQ-4bit",
        "unsloth/LFM2-8B-A1B",
        "mlx-community/LFM2-8B-A1B-6bit-MLX",
        "McG-221/LFM2-8B-A1B-mlx-8Bit",
        "mlx-community/LFM2-8B-A1B-fp16",
        "introvoyz041/LFM2-8B-A1B-8bit-mlx-8Bit"
      ]
    },
    {
      "architecture_id": "RavenForCausalLM",
      "total_models": 20,
      "sample_models": [
        "tomg-group-umd/huginn-0125",
        "smcleish/Recurrent-Llama-3.2-train-recurrence-8",
        "smcleish/Recurrent-Llama-3.2-train-recurrence-32",
        "smcleish/Recurrent-Llama-3.2-untrained",
        "smcleish/Recurrent-TinyLlama-3T-train-recurrence-8",
        "smcleish/Recurrent-Llama-3.2-train-recurrence-16",
        "smcleish/Recurrent-TinyLlama-3T-train-recurrence-32",
        "smcleish/Recurrent-OLMo-2-0425-train-recurrence-8",
        "tomg-group-umd/step-00006144-recurrence_full_512_0",
        "tomg-group-umd/huginn_swa_75_7_ema_0.9_merge"
      ]
    },
    {
      "architecture_id": "XLMRobertaForCausalLM",
      "total_models": 20,
      "sample_models": [
        "Marly03/fft-roberta-baseline",
        "liamvbetts/my_awesome_eli5_clm-model",
        "soBeauty/My-Tokenizer-News-dataset",
        "soBeauty/xlm-roberta-base-Own_tokenized",
        "colesimmons/sumerian-glyph-decoder",
        "RichardErkhov/salti_-_xlm-roberta-large-arabic_qa-8bits",
        "lukmanaj/afro-xlmr-large-english_to_hausa",
        "RichardErkhov/IProject-10_-_xlm-roberta-base-finetuned-squad2-4bits",
        "RichardErkhov/deepset_-_xlm-roberta-base-squad2-4bits",
        "RichardErkhov/deepset_-_xlm-roberta-base-squad2-distilled-4bits"
      ]
    },
    {
      "architecture_id": "BaiChuanForCausalLM",
      "total_models": 19,
      "sample_models": [
        "baichuan-inc/Baichuan-7B",
        "FreedomIntelligence/HuatuoGPT-7B",
        "hiyouga/Baichuan-7B-sft",
        "tyang816/MedChatZH",
        "mxmax/baichuan-7b-sft-001",
        "tomxyz/qiaoban_bc",
        "ZhihaiLLM/wisdomInterrogatory",
        "AlpachinoNLP/Baichuan-7B-Instruction",
        "reedhs/baichuan-7b-sharded",
        "TheBloke/baichuan-7B-GPTQ"
      ]
    },
    {
      "architecture_id": "OpenAIGPTLMHeadModel",
      "total_models": 19,
      "sample_models": [
        "openai-community/openai-gpt",
        "CoffeeAddict93/gpt1-modest-proposal",
        "josedlhm/new_model",
        "josedlhm/trump_tweet",
        "hf-tiny-model-private/tiny-random-OpenAIGPTLMHeadModel",
        "Hojjat/ehr_gpt2",
        "instruct-generalize/gpt-1",
        "vietnhatthai/test_pretrain_gpt_pipeline",
        "tmnam20/test_pretrain_pipeline",
        "vietnhatthai/viet_news_pretrain_pipeline"
      ]
    },
    {
      "architecture_id": "TinyLlavaForConditionalGeneration",
      "total_models": 19,
      "sample_models": [
        "tinyllava/TinyLLaVA-Phi-2-SigLIP-3.1B",
        "Zhang199/TinyLLaVA-Qwen2-0.5B-SigLIP",
        "jiajunlong/TinyLLaVA-OpenELM-450M-CLIP-0.55B",
        "tinyllava/TinyLLaVA-Gemma-SigLIP-2.4B",
        "TucanoBR/ViTucano-1b5-v1",
        "jiajunlong/TinyLLaVA-OpenELM-450M-SigLIP-0.89B",
        "Zhang199/TinyLLaVA-Video-Phi2-Naive-16-512",
        "keeeeenw/MicroLlava",
        "Zhang199/TinyLLaVA-Video-Coldstart_NextQA_16",
        "TucanoBR/ViTucano-2b8-v1"
      ]
    },
    {
      "architecture_id": "HymbaForCausalLM",
      "total_models": 19,
      "sample_models": [
        "nvidia/Hymba-1.5B-Base",
        "nvidia/Hymba-1.5B-Instruct",
        "yujiepan/hymba-tiny-random",
        "iamashishsharma8/HybridSLM",
        "himanshu-skid19/Hymba-1.5B-modified",
        "Maxtimer97/LLama2Hymba_DirectTuluSFT",
        "Maxtimer97/LlamaBase2Hymba_raw",
        "Maxtimer97/Llama2llama",
        "Maxtimer97/Llama2SWA",
        "Maxtimer97/LlamaBase2Hymba_SFT"
      ]
    },
    {
      "architecture_id": "CTRLLMHeadModel",
      "total_models": 19,
      "sample_models": [
        "sshleifer/tiny-ctrl",
        "dkalpakchi/SweCTRL-Mini",
        "prajjwal1/ctrl_discovery_1",
        "wvangils/CTRL-Beatles-Lyrics-finetuned-newlyrics",
        "prajjwal1/ctrl_discovery_2",
        "prajjwal1/ctrl_discovery_flipped_1",
        "prajjwal1/ctrl_discovery_10",
        "prajjwal1/ctrl_discovery_13",
        "prajjwal1/ctrl_discovery_5",
        "prajjwal1/ctrl_discovery_flipped_3"
      ]
    },
    {
      "architecture_id": "RWKV7ForCausalLM",
      "total_models": 19,
      "sample_models": [
        "fla-hub/rwkv7-2.9B-world",
        "RWKV/RWKV7-Goose-World3-2.9B-HF",
        "RWKV/RWKV7-Goose-World2.9-0.4B-HF",
        "fla-hub/rwkv7-168M-pile",
        "fla-hub/rwkv7-0.4B-world",
        "fla-hub/rwkv7-0.1B-g1",
        "RWKV/RWKV7-Goose-World3-1.5B-HF",
        "fla-hub/rwkv7-1.5B-g1",
        "RWKV/RWKV7-Goose-Pile-1.47B-HF",
        "fla-hub/rwkv7-7.2B-g0a"
      ]
    },
    {
      "architecture_id": "ForgettingTransformerForCausalLM",
      "total_models": 19,
      "sample_models": [
        "zhixuan-lin/fox-pro-760m-longcrawl64-48b",
        "zhixuan-lin/fox-llama-760m-longcrawl64-48b",
        "Lanni-ni/forgetting_gate_3_4_256",
        "Lanni-ni/forgetting_gate_6_8_512",
        "xiulinyang/forgetting_transformer",
        "xiulinyang/fox_no_rope",
        "Lanni-ni/forgetting_gate_4_6_384",
        "Lanni-ni/forgetting_gate_4_6_384_softmax",
        "Lanni-ni/forgetting_gate_2_4_256",
        "Lanni-ni/forgetting_pile_2layer"
      ]
    },
    {
      "architecture_id": "PhiMoEForCausalLM",
      "total_models": 18,
      "sample_models": [
        "microsoft/Phi-tiny-MoE-instruct",
        "microsoft/Phi-3.5-MoE-instruct",
        "microsoft/Phi-mini-MoE-instruct",
        "optimum-intel-internal-testing/phi-3.5-moe-tiny-random",
        "gaianet/Phi-3.5-MoE-instruct-GGUF",
        "mlx-community/Phi-3.5-MoE-instruct-4bit",
        "ianshank/phi-35-moe-instruct",
        "mlx-community/Phi-3.5-MoE-instruct-8bit",
        "yujiepan/phi-moe-tiny-random",
        "yujiepan/phi-3.5-moe-tiny-random"
      ]
    },
    {
      "architecture_id": "HunYuanMoEV1ForCausalLM",
      "total_models": 18,
      "sample_models": [
        "tencent/Hunyuan-A13B-Instruct-FP8",
        "tencent/Hunyuan-A13B-Instruct",
        "tencent/Hunyuan-A13B-Pretrain",
        "tencent/Hunyuan-A13B-Instruct-GPTQ-Int4",
        "mlx-community/Hunyuan-A13B-Instruct-4bit",
        "yujiepan/hunyuan-moe-tiny-random",
        "mlx-community/Hunyuan-A13B-Instruct-8bit",
        "unsloth/Hunyuan-A13B-Instruct",
        "mlx-community/Hunyuan-A13B-Instruct-mixed-6-8bit",
        "mlx-community/Hunyuan-A13B-Instruct-4bit-DWQ"
      ]
    },
    {
      "architecture_id": "LlamaForCausalLMEagle3",
      "total_models": 18,
      "sample_models": [
        "nvidia/gpt-oss-120b-Eagle3-long-context",
        "taobao-mnn/Qwen3-VL-8B-Instruct-Eagle3",
        "nvidia/gpt-oss-120b-Eagle3-throughput",
        "thomaskiefer/EAGLE3-Apertus-8B-Instruct-2509",
        "JinnP/SGLang-EAGLE3-Qwen3-Coder-30B-A3B-Instruct",
        "AvitoTech/avibe-eagle",
        "nvidia/Qwen3-235B-A22B-Eagle3",
        "Zjcxy-SmartAI/Eagle3-Qwen3-32B-zh",
        "taobao-mnn/Qwen3-VL-4B-Instruct-Eagle3",
        "Zjcxy-SmartAI/Eagle3-Qwen3-8B-zh"
      ]
    },
    {
      "architecture_id": "CogVLMForCausalLM",
      "total_models": 18,
      "sample_models": [
        "zai-org/cogvlm2-llama3-chat-19B",
        "zai-org/cogvlm2-llama3-chat-19B-int4",
        "zai-org/cogvlm-grounding-generalist-hf",
        "zai-org/cogvlm2-llama3-chinese-chat-19B",
        "zai-org/cogvlm-base-224-hf",
        "zai-org/cogvlm2-llama3-chinese-chat-19B-int4",
        "zai-org/cogvlm2-llama3-chinese-chat-19B-tgi",
        "zai-org/cogvlm-grounding-base-hf",
        "zai-org/cogvlm2-llama3-chat-19B-tgi",
        "grim3000/cogvlm-chat-hf"
      ]
    },
    {
      "architecture_id": "SkyworkForCausalLM",
      "total_models": 18,
      "sample_models": [
        "Skywork/Skywork-13B-base",
        "Skywork/Skywork-MoE-Base",
        "Skywork/Skywork-13B-Math",
        "Skywork/Skywork-MoE-Base-FP8",
        "Skywork/Skywork-13B-Base-8bits",
        "TheBloke/Skywork-13B-base-GPTQ",
        "LoneStriker/Skywork-13B-Airo-Claude-Pippa-Puffin-5.0bpw-h6-exl2",
        "LoneStriker/Skywork-13B-base-3.0bpw-h6-exl2",
        "LoneStriker/Skywork-13B-base-4.0bpw-h6-exl2",
        "LoneStriker/Skywork-13B-Airo-Claude-Pippa-Puffin-4.0bpw-h6-exl2"
      ]
    },
    {
      "architecture_id": "BloomModel",
      "total_models": 17,
      "sample_models": [
        "bigscience/bigscience-small-testing",
        "TurkuNLP/gpt3-finnish-small",
        "TurkuNLP/gpt3-finnish-large",
        "TurkuNLP/gpt3-finnish-13B",
        "Muennighoff/bloom-tiny-random",
        "ckip-joint/bloom-1b1-zh",
        "TurkuNLP/gpt3-finnish-medium",
        "model-attribution-challenge/bloom-560m",
        "TurkuNLP/gpt3-finnish-8B",
        "model-attribution-challenge/bloom-2b5"
      ]
    },
    {
      "architecture_id": "ProGenForCausalLM",
      "total_models": 17,
      "sample_models": [
        "hugohrban/progen2-base",
        "hugohrban/progen2-small",
        "hugohrban/progen2-xlarge",
        "hugohrban/progen2-large",
        "hugohrban/progen2-oas",
        "hugohrban/progen2-BFD90",
        "hugohrban/progen2-small-mix7",
        "hugohrban/progen2-small-mix7-bidi",
        "InstructPLM/MPNN-ProGen2-xlarge-CATH42",
        "AntibodyGeneration/fine-tuned-progen2-large"
      ]
    },
    {
      "architecture_id": "InductionVL2ForCausalLM",
      "total_models": 17,
      "sample_models": [
        "jonathanli/induction-vl2-1213-pretrained-2b-8cnxq-17500",
        "jonathanli/induction-vl2-pretrained-tmp-tiny",
        "jonathanli/induction-vl2-15k-synth",
        "jonathanli/induction-vl2-1213-pretrained-ds-2b",
        "jonathanli/induction-vl2-1213-pretrained-2b-xdxp9-40000-720p",
        "jonathanli/induction-vl2-1213-pretrained-2b-xdxp9-40000-720p-ground",
        "jonathanli/induction-vl2-1213-pretrained-2b-h2hf8-7500",
        "jonathanli/induction-vl2-1213-pretrained-2b-xdxp9-40000-ground",
        "jonathanli/induction-vl2-pretrained-tmp-tiny-2-1211",
        "jonathanli/induction-vl2-1213-pretrained-ds"
      ]
    },
    {
      "architecture_id": "GPT2Model",
      "total_models": 17,
      "sample_models": [
        "cerebras/Cerebras-GPT-13B",
        "yongzx/gpt2-finetuned-oscar-ko",
        "keshan/sinhala-gpt2",
        "cemde/Domain-Certification-MedQA-Guide-Finetuned",
        "flax-community/Sinhala-gpt2",
        "banglagov/banGPT2-Base",
        "VortexIntelligence/VLM-1-K3",
        "Alerosae/SocratesGPT-2",
        "Mary222/GPT2_standard",
        "yongzx/gpt2-finetuned-oscar-de"
      ]
    },
    {
      "architecture_id": "Qwen2_5_VLForConditionalGeneration",
      "total_models": 17,
      "sample_models": [
        "nvidia/Qwen2.5-VL-7B-Instruct-NVFP4",
        "PromptEnhancer/PromptEnhancer-32B",
        "taresco/KarantaOCR",
        "OmniSVG/OmniSVG",
        "PromptEnhancer/PromptEnhancer-Img2img-Edit",
        "bespokelabs/Bespoke-MiniChart-7B",
        "diabolic6045/Sanskrit-Qwen2.5-VL-7B-Instruct-OCR",
        "snskrt/qwen2-5-vl-sanskrit-ocr",
        "e-zorzi/Qwen2.5-VL-7B-Instruct-tuned-raw",
        "taresco/KarantaOCR-TrimmedVocab"
      ]
    },
    {
      "architecture_id": "MllamaForCausalLM",
      "total_models": 17,
      "sample_models": [
        "junidude14/bllossom_AICA_nsfw_rp",
        "Yaafer/merged-chart_llama",
        "pou876/Llama-3.2-11B-Vision-Instruct-Deepfake",
        "karthikeyan-r/fine-tuned-llama-vision",
        "furia0928/merged-llama-3.2-11B-Vision",
        "junidude14/Bllossom-AICA-5B_RolePlay_SFT",
        "kmucs-juppi/llama-11B-vision-instruction-deepfake",
        "pou876/Llama-3.2-11B-vision-instruction-deepfake",
        "pou876/Llama-3.2-11B-Vision-Instuction-deepfake",
        "pminervini/Llama-3.2-11B-Vision-Instruct-bnb-nf4"
      ]
    },
    {
      "architecture_id": "GPT2ALMHeadModel",
      "total_models": 17,
      "sample_models": [
        "crumb/d1536-250MT-full",
        "crumbly/horizon-25m-v0",
        "crumbly/cramp-25m",
        "crumb/44m-textbook",
        "crumb/25m-special",
        "crumb/cramped-94m-init",
        "crumb/cramped-94m-8btok",
        "crumb/32M-32GT-SlimPajama",
        "crumb/d768-16GT-slimpajama",
        "crumb/d768-step19000"
      ]
    },
    {
      "architecture_id": "VibeVoiceForConditionalGeneration",
      "total_models": 16,
      "sample_models": [
        "microsoft/VibeVoice-1.5B",
        "vibevoice/VibeVoice-1.5B",
        "bezzam/VibeVoice-1.5Bv2",
        "bezzam/VibeVoice-1.5B",
        "bezzam/VibeVoice-7Bv2",
        "bezzam/VibeVoice-1.5B-hf",
        "tardigrade-doc/VibeVoice-1.5B-ft",
        "Sergey004/VibeVoice-1.5B",
        "amentaga-nttd/VibeVoice-1.5B",
        "rsxdalv/VibeVoice-1.5B"
      ]
    },
    {
      "architecture_id": "LLaDAModelLM",
      "total_models": 16,
      "sample_models": [
        "GSAI-ML/LLaDA-8B-Instruct",
        "GSAI-ML/LLaDA-8B-Base",
        "GSAI-ML/LLaDA-1.5",
        "d3LLM/d3LLM_LLaDA",
        "pbansal/LLaDA-8B-Base-Adjust",
        "Zigeng/dParallel-LLaDA-8B-instruct",
        "relaxe-system-lab/UltraLLaDA",
        "FunAGI/LLaDA-8B-Instruct-gptqmodel-4bit",
        "smoorsmith/LLaDA-8B-Instruct-Transparent-Masking",
        "Proximile/LLaDA-8B-Tools"
      ]
    },
    {
      "architecture_id": "MFuyuForCausalLM",
      "total_models": 16,
      "sample_models": [
        "Mantis-VL/mfuyu_v2_8192_720p",
        "TIGER-Lab/Mantis-8B-Fuyu",
        "Mantis-VL/mfuyu_llava_nlvr2_8192_480p",
        "Mantis-VL/mfuyu_llava_8192_480p",
        "Mantis-VL/mfuyu_llava50k",
        "Mantis-VL/mfuyu_v2_3072_480p",
        "Mantis-VL/mfuyu_llava_synthetic_lrv_8192_480p",
        "Mantis-VL/mfuyu_llava_diff_8192_480p",
        "Mantis-VL/mfuyu_v2_3072_480p-final",
        "Mantis-VL/mfuyu_llava_nlvr2_lrv_8192_480p"
      ]
    },
    {
      "architecture_id": "NARBartForConditionalGeneration",
      "total_models": 16,
      "sample_models": [
        "kuanhuggingface/speech-chatpgpt-base-nar-only-codec-scratch",
        "kuanhuggingface/speech-chatgpt-base-nar-v3-encodec-epoch4-4codec",
        "lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans-scratch",
        "kuanhuggingface/speech-chatgpt-base-nar-sum1to8",
        "lca0503/speech-chatgpt-base-nar-v2-epoch4-wotrans",
        "kuanhuggingface/speech-chatpgpt-base-nar-only-codec2",
        "kuanhuggingface/speech-chatpgpt-large-nar",
        "kuanhuggingface/speech-chatgpt-base-nar-v2",
        "kuanhuggingface/speech-chatgpt-base-nar-v2-speech-tokenizer-promptTTS",
        "kuanhuggingface/speech-chatgpt-base-nar-v3-encodec-epoch2"
      ]
    },
    {
      "architecture_id": "AlibiForCausalLM",
      "total_models": 16,
      "sample_models": [
        "Lanni-ni/alibi_6_8_512",
        "Lanni-ni/alibi_4_6_384",
        "Lanni-ni/alibi_3_4_256_fla",
        "Lanni-ni/alibi_4_6_384_",
        "Lanni-ni/alibi_4_6_384_babylm",
        "Lanni-ni/alibi_babylm_10m_2_4_256",
        "Lanni-ni/alibi_12_12_768_softmax",
        "Lanni-ni/temp_model",
        "Lanni-ni/alibi_babylm_10m_2layer",
        "Lanni-ni/alibi_2_4_256_fla"
      ]
    },
    {
      "architecture_id": "CubeLM",
      "total_models": 16,
      "sample_models": [
        "ajyl/sft_seed_1423",
        "ajyl/sft_seed_1423_pretrain",
        "cfpark00/gpt2_cube_v2-1",
        "cfpark00/gpt2_cube_v2-2",
        "cfpark00/gpt2_cube_v2-3",
        "prakharg/sft_seed_200_512d_8L_8H_datatype_full_pretrain.pt",
        "prakharg/sft_seed_300_512d_8L_8H_datatype_full.pt",
        "prakharg/sft_seed_200_512d_8L_8H_datatype_full_pretrain_new.pt",
        "prakharg/pretrain_seed_222_512d_8L_8H.pt",
        "prakharg/sft_seed_76812122_768d_12L_12H_datatype_full_pretrain.pt"
      ]
    },
    {
      "architecture_id": "DynamicAlibiForCausalLM",
      "total_models": 16,
      "sample_models": [
        "Lanni-ni/dynamic_alibi_4_6_384_epoch6",
        "Lanni-ni/dynamic_alibi_2L_4H_256D_babylm",
        "Lanni-ni/dynamic_alibi_babylm_10m_2_4_256",
        "Lanni-ni/dynamic_alibi_pile_2layer",
        "Lanni-ni/dynamic_alibi_pile_4layer",
        "Lanni-ni/dynamic_alibi_4_6_384_epoch1",
        "Lanni-ni/dynamic_alibi_4_6_384_epoch2",
        "Lanni-ni/dynamic_alibi_4_6_384_epoch5",
        "Lanni-ni/dynamic_alibi_4_6_384_epoch7",
        "Lanni-ni/dynamic_alibi_4_6_384_epoch8"
      ]
    },
    {
      "architecture_id": "InternLMForCausalLM",
      "total_models": 15,
      "sample_models": [
        "internlm/internlm-chat-7b",
        "internlm/internlm-20b",
        "internlm/internlm-chat-20b",
        "internlm/Agent-FLAN-7b",
        "internlm/internlm-chat-20b-4bit",
        "lmdeploy/internlm-chat-7b-w4",
        "TongjiFinLab/CFGPT1-pt-7B",
        "TongjiFinLab/CFGPT1-sft-7B-Full",
        "csdc-atl/internlm-chat-20b-GPTQ-Int4",
        "vansin/internlm-chat-7b"
      ]
    },
    {
      "architecture_id": "DeepseekForCausalLM",
      "total_models": 15,
      "sample_models": [
        "deepseek-ai/deepseek-moe-16b-base",
        "deepseek-ai/deepseek-moe-16b-chat",
        "ai-sage/GigaChat-20B-A3B-instruct",
        "ai-sage/GigaChat-20B-A3B-instruct-bf16",
        "ai-sage/GigaChat-20B-A3B-base",
        "madgnu/GigaChat-20B-A3B-instruct-mlx-4bit",
        "QuixiAI/DeepMixtral-8x7b-Instruct",
        "JINIAC/JINIAC-5B-culturex-code0-9-lr-5e-5-ja_hq-5e-5-sft_configuration-3_prod-checkpoint-500",
        "Crystalcareai/llama-3-4x8b",
        "OsakanaTeishoku/dummy-3.6b"
      ]
    },
    {
      "architecture_id": "FalconMambaForCausalLM",
      "total_models": 15,
      "sample_models": [
        "tiiuae/falcon-mamba-tiny-dev",
        "tiiuae/falcon-mamba-7b-instruct",
        "tiiuae/falcon-mamba-7b",
        "tiiuae/Falcon3-Mamba-7B-Instruct",
        "trl-internal-testing/tiny-FalconMambaForCausalLM",
        "tiiuae/Falcon3-Mamba-7B-Base",
        "hanzla/Falcon3-Mamba-R1-v0",
        "mlx-community/Falcon3-Mamba-7B-Instruct",
        "mlx-community/falcon-mamba-7b-4bit-instruct",
        "mlx-community/Falcon3-Mamba-7B-Instruct-4bits"
      ]
    },
    {
      "architecture_id": "BailingMoeForCausalLM",
      "total_models": 15,
      "sample_models": [
        "inclusionAI/Ling-lite-1.5",
        "inclusionAI/Ling-plus",
        "inclusionAI/Ring-lite",
        "inclusionAI/Ling-plus-base",
        "inclusionAI/Ling-Coder-lite",
        "inclusionAI/Ling-lite-base",
        "inclusionAI/Ling-Coder-lite-base",
        "inclusionAI/Ling-lite-1.5-2506",
        "inclusionAI/Ling-lite-1.5-2507",
        "inclusionAI/Ling-lite-base-1.5"
      ]
    },
    {
      "architecture_id": "ArceeForCausalLM",
      "total_models": 15,
      "sample_models": [
        "optimum-intel-internal-testing/tiny-random-ArceeForCausalLM",
        "arcee-ai/AFM-4.5B-Base",
        "onnx-internal-testing/tiny-random-ArceeForCausalLM",
        "arcee-ai/AFM-4.5B",
        "arcee-ai/AFM-4.5B-Base-Pre-Anneal",
        "arcee-ai/AFM-4.5B-Preview",
        "deathbyknowledge/AFM-4.5B-Shell-SFT",
        "Hastagaras/shard-test-v5e-8-2",
        "AlekseyCalvin/LYRICAL_MT_ru2en_19_AFM45_3epochs",
        "openmed-community/AFM-4.5B-OpenMed"
      ]
    },
    {
      "architecture_id": "Llama4ForConditionalGeneration",
      "total_models": 15,
      "sample_models": [
        "lmstudio-community/Llama-4-Scout-17B-16E-MLX-text-8bit",
        "yujiepan/llama-4-tiny-random",
        "lmstudio-community/Llama-4-Scout-17B-16E-MLX-text-4bit",
        "mlx-community/Llama-4-Maverick-17B-16E-Instruct-4bit",
        "tiny-random/llama-4",
        "Mogith/Llama-4-Scout-17B-16E-Instruct-Q8_0",
        "from-our-page/Llama-4-Maverick-17B-128E-mlx-4bit",
        "yujiepan/llama-4-8E-tiny-random",
        "mlx-community/meta-llama-Llama-4-Scout-17B-16E-6bit",
        "mlx-community/meta-llama-Llama-4-Scout-17B-16E-fp16"
      ]
    },
    {
      "architecture_id": "SparseLlamaForCausalLM",
      "total_models": 15,
      "sample_models": [
        "SparseLLM/prosparse-llama-2-7b",
        "thrunlab/sparse_llama_7b_hf2_refined_web_70p_2024-03-28",
        "thrunlab/sparse_llama_7b_hf2_refined_web_50p_2024-05-12",
        "thrunlab/sparse_llama_7b_refined_web_50p_2024-03-24",
        "thrunlab/sparse_llama_7b_hf_refined_web_50p_2024-03-24",
        "thrunlab/relu_llama_7b_hf2_refined_web_relu_2024-03-28",
        "thrunlab/sparse_llama_7b_hf_refined_web_70p_2024-03-25",
        "thrunlab/llama_7b_hf_relu_refined_web_relu_2024-03-27",
        "thrunlab/sparse_llama_7b_hf2_refined_web_50p_2024-03-28",
        "thrunlab/llama_7b_hf_relu_refined_web_relu_2024-03-26"
      ]
    },
    {
      "architecture_id": "EagleLlamaForCausalLM",
      "total_models": 15,
      "sample_models": [
        "NVEagle/Eagle-X5-13B-Chat",
        "NVEagle/Eagle-X5-7B",
        "NVEagle/Eagle-X5-13B",
        "NVEagle/Eagle-X4-8B-Plus",
        "NVEagle/Eagle-X5-34B-Chat",
        "NVEagle/Eagle-X5-34B-Plus",
        "NVEagle/Eagle-X4-13B-Plus",
        "PrincetonPLI/Eagle-X2-Llama3-8B-TableReadout-MixPlus-240k",
        "PrincetonPLI/Eagle-X2-Llama3-8B-TableReadout-AlignMixPlus-240k",
        "PrincetonPLI/Eagle-X2-Llama3-8B-GridNavigation-MixPlus-120k"
      ]
    },
    {
      "architecture_id": "DistilBertForSequenceClassification",
      "total_models": 15,
      "sample_models": [
        "open-paws/animal_advocate_preference_prediction_shortform",
        "open-paws/text_performance_prediction_shortform",
        "open-paws/perceived_trustworthiness_prediction_shortform",
        "open-paws/emotional_impact_prediction_shortform",
        "hamaadayubkhan/Mentalhealthbyhamaad",
        "Qusaiiii/CustomsAccountant",
        "open-paws/potential_influence_prediction_shortform",
        "open-paws/perceived_insightfulness_prediction_shortform",
        "open-paws/relevance_to_animal_issues_prediction_shortform",
        "open-paws/level_of_rationality_prediction_shortform"
      ]
    },
    {
      "architecture_id": "PoptorchPipelinedGPT2LMHeadModel",
      "total_models": 15,
      "sample_models": [
        "graphcore-rahult/gpt2-finetuned-wikitext2",
        "Ti-Ma/TiMaGPT2-2022",
        "Jinchen/gpt2-wikitext2",
        "internetoftim/gpt2-finetuned-wikitext2",
        "Ti-Ma/TiMaGPT2-2013",
        "Ti-Ma/TiMaGPT2-2014",
        "Ti-Ma/TiMaGPT2-2015",
        "Ti-Ma/TiMaGPT2-2021",
        "Jinchen/gpt2-finetuned-wikitext2",
        "jimypbr/gpt2-finetuned-wikitext2"
      ]
    },
    {
      "architecture_id": "transformerModel",
      "total_models": 15,
      "sample_models": [
        "bu1/custom_transformer",
        "bu1/IQsignal_transformer_10db",
        "bu1/IQsignal_transformer_0db",
        "bu1/IQ_classification_112_new1_-5db",
        "bu1/IQsignal_transformer_15db",
        "bu1/IQsignal_transformer_-5db",
        "bu1/IQsignal_transformer_-10db",
        "bu1/IQ_classification_112_new1_15db",
        "bu1/IQ_classification_112_new1_10db",
        "bu1/IQ_classification_112_new1_5db"
      ]
    },
    {
      "architecture_id": "RecurrentGemmaForCausalLM",
      "total_models": 14,
      "sample_models": [
        "google/recurrentgemma-2b-it",
        "google/recurrentgemma-2b",
        "alpindale/recurrentgemma-9b-it",
        "google/recurrentgemma-9b-it",
        "google/recurrentgemma-9b",
        "pszemraj/griffin-c3t-8L-v0.02-fineweb",
        "theo77186/recurrentgemma-9b-it-bnb-4bit",
        "RichardErkhov/google_-_recurrentgemma-2b-it-8bits",
        "pszemraj/griffin-v0.01-c3t-8layer-simplewiki-silu",
        "pszemraj/griffin-1024-llama3t-8layer-simplewiki-silu"
      ]
    },
    {
      "architecture_id": "AutoModelForCausalLM",
      "total_models": 14,
      "sample_models": [
        "ngottram/ocb",
        "Corianas/tiny-llama-miniguanaco",
        "Riley-01234/Zelgodiz",
        "hollywoodfrancis/LopezLLM",
        "ashishkgpian/full_v4_astromistral_final",
        "jakelu962/legal_qa_lora_model",
        "sourabhd89/TinyLlama-1B-Finetuned",
        "gauri-sharan/phi2-alpaca-lora-4bit",
        "FUGMON/indian-financial-advisor-quantized",
        "ivxxdegen/mibera-v1-merged"
      ]
    },
    {
      "architecture_id": "QuietForCausalLM",
      "total_models": 14,
      "sample_models": [
        "Crystalcareai/Quiet-Star-Custom",
        "blockblockblock/Quiet-Star-Custom-bpw3",
        "blockblockblock/Quiet-Star-Custom-bpw3.5",
        "blockblockblock/Quiet-Star-Custom-bpw3.7",
        "blockblockblock/Quiet-Star-Custom-bpw4.6",
        "blockblockblock/Quiet-Star-Custom-bpw2.5",
        "blockblockblock/Quiet-Star-Custom-bpw4.2",
        "blockblockblock/Quiet-Star-Custom-bpw4.8",
        "blockblockblock/Quiet-Star-Custom-bpw5",
        "blockblockblock/Quiet-Star-Custom-bpw4"
      ]
    },
    {
      "architecture_id": "BailingMoeLinearV2ForCausalLM",
      "total_models": 14,
      "sample_models": [
        "inclusionAI/Ring-flash-linear-2.0-GPTQ-int4",
        "inclusionAI/Ring-mini-linear-2.0",
        "mlx-community/Ring-mini-linear-2.0-4bit",
        "mlx-community/Ring-flash-linear-2.0-128k-4bit",
        "mlx-community/Ring-mini-linear-2.0-8bit",
        "mlx-community/Ring-flash-linear-2.0-128k-8bit",
        "mlx-community/Ring-flash-linear-2.0-4bit",
        "mlx-community/Ring-flash-linear-2.0-128k-3bit",
        "mlx-community/Ring-flash-linear-2.0-128k-6bit",
        "mlx-community/Ring-mini-linear-2.0-6bit"
      ]
    },
    {
      "architecture_id": "Qwen3VLMoeForConditionalGeneration",
      "total_models": 13,
      "sample_models": [
        "QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ",
        "QuantTrio/Qwen3-VL-30B-A3B-Thinking-AWQ",
        "RedHatAI/Qwen3-VL-235B-A22B-Instruct-NVFP4",
        "QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ",
        "QuantTrio/Qwen3-VL-235B-A22B-Thinking-AWQ",
        "remodlai/Qwen3-VL-30B-A3B-Instruct-AWQ",
        "GaleneAI/Qwen3-VL-235B-A22B-Thinking-NVFP4",
        "pluto6272/Qwen3-VL-30B-Medical-V3-Precision",
        "QuantTrio/Qwen3-VL-235B-A22B-Instruct-FP8",
        "ticoAg/Qwen3-VL-30B-A3B-Instruct-AWQ"
      ]
    },
    {
      "architecture_id": "OpenMoeForCausalLM",
      "total_models": 13,
      "sample_models": [
        "hpcai-tech/openmoe-8B",
        "OrionZheng/openmoe-base",
        "OrionZheng/openmoe-8b-chat",
        "OrionZheng/openmoe-8b",
        "OrionZheng/openmoe-34b-200B",
        "hpcai-tech/openmoe-base",
        "OrionZheng/openmoe-8b-200B",
        "OrionZheng/openmoe-8b-800B",
        "OrionZheng/openmoe-8b-890B",
        "OrionZheng/openmoe-8b-400B"
      ]
    },
    {
      "architecture_id": "Qwen3VLForConditionalGeneration",
      "total_models": 13,
      "sample_models": [
        "RedHatAI/Qwen3-VL-32B-Instruct-NVFP4",
        "magiccodingman/Qwen3-VL-32B-Thinking-Unsloth-MXFP4-Hybrid-GGUF",
        "karanjaWakaba/Qwen-3-4B-unsloth-4bit",
        "magiccodingman/Qwen3-VL-8B-Instruct-Unsloth-MXFP4-Hybrid-GGUF",
        "RedHatAI/Qwen3-VL-32B-Instruct-FP8-dynamic",
        "Goekdeniz-Guelmez/Josiefied-Qwen3-VL-4B-Instruct-abliterated-beta-v1",
        "hell0ks/Huihui-Qwen3-VL-32B-Instruct-abliterated-W4A16",
        "RedHatAI/Qwen3-VL-32B-Instruct-FP8-block",
        "TESS-Computer/csgo-vla-checkpoint",
        "zenlm/zen-designer-235b-a22b-thinking"
      ]
    },
    {
      "architecture_id": "RetNetForCausalLM",
      "total_models": 13,
      "sample_models": [
        "fla-hub/retnet-1.3B-100B",
        "fla-hub/retnet-2.7B-100B",
        "isek-ai/SDPrompt-RetNet-v2-beta",
        "GardeniaM/retnet-test",
        "Spiral-AI/Spiral-RetNet-3b-base",
        "isek-ai/SDPrompt-RetNet-300M",
        "Azamorn/retnet-tinystories",
        "NucleusAI/RetNet-410m-XATL",
        "jploski/retnet-mini-shakespeare",
        "kaizerBox/retnet-summarization"
      ]
    },
    {
      "architecture_id": "HunYuanForCausalLM",
      "total_models": 13,
      "sample_models": [
        "tencent/Hunyuan-7B-Pretrain-0124",
        "mlx-community/Hunyuan-A52B-Instruct-3bit",
        "tencent-community/Hunyuan-A52B-Pretrain",
        "tencent-community/Hunyuan-A52B-Instruct-original",
        "HawkonLi/Hunyuan-A52B-Instruct-2bit",
        "tencent-community/Hunyuan-A52B-Pretrain-original",
        "mlx-community/Hunyuan-7B-Instruct-fp16",
        "tencent-community/Hunyuan-A52B-Instruct-FP8",
        "mlx-community/Hunyuan-7B-Instruct-4bit",
        "mlx-community/Hunyuan-7B-Instruct-3bit"
      ]
    },
    {
      "architecture_id": "GPT",
      "total_models": 13,
      "sample_models": [
        "Morris88826/Mu-Ruei_Tseng_133007868_124M",
        "Exquisique/BabyLangModel",
        "wang-sy/stinfo-gpt2",
        "abhilash88/tinystories-slm-gpt",
        "chhatramani/Nepal_legalGPT2_Scratch",
        "xiaohang07/MatterGPT",
        "prompterminal/nanogpt-enwik8-compressed",
        "Morris88826/Mu-Ruei_Tseng_133007868_350M",
        "vitalune/nanochat-d10-filtered-500m",
        "ton-anh/testing"
      ]
    },
    {
      "architecture_id": "SelfDebiasingGPT2LMHeadModel",
      "total_models": 13,
      "sample_models": [
        "newtonkwan/gpt2-xl-ft-1",
        "newtonkwan/gpt2-xl-ft-with-non-challenging-1k",
        "beston91/gpt2-xl_ft_mult_5k",
        "beston91/gpt2-xl_ft_logits_10k",
        "beston91/gpt2-xl-ft-logits-5k",
        "beston91/gpt2-xl_ft_logits_1k_2",
        "beston91/gpt2-xl_ft_logits_5k_2",
        "beston91/gpt2-xl_ft_mult_10k",
        "beston91/gpt2-xl-ft-logits-1k",
        "beston91/gpt2-xl_ft_mult_1k"
      ]
    },
    {
      "architecture_id": "HGRNForCausalLM",
      "total_models": 13,
      "sample_models": [
        "fla-hub/hgrn-2.7B-100B",
        "fla-hub/hgrn-1.3B-100B",
        "stevenabreu7/hgrn-sparse90-340M-10B-20k",
        "apierro-hf/hgrn-340M-instruct-capybara",
        "timcheck/hgrn-1.3B-A130M-base",
        "pweidel/hgrn-sparse90-340M-5B",
        "pweidel/hgrn-sparse90-340M-5B-7K",
        "apierro-hf/hgrn-340M-A34M-instruct-capybara",
        "pweidel/hgrn-340M-5B-instruct",
        "pweidel/hgrn-s90-340M-20k"
      ]
    },
    {
      "architecture_id": "GPTJiangForCausalLM",
      "total_models": 13,
      "sample_models": [
        "kdf/jiang-base-35000steps",
        "kdf/jiang-base-25000steps",
        "kdf/jiang-base-10000steps",
        "kdf/jiang-base-40000steps",
        "kdf/jiang-base-50000steps",
        "kdf/jiang-base-45000steps",
        "kdf/jiang-base-60000steps",
        "kdf/jiang-chat",
        "kdf/jiang-base-5000steps",
        "kdf/jiang-base-20000steps"
      ]
    },
    {
      "architecture_id": "GlmForCausalLM",
      "total_models": 12,
      "sample_models": [
        "zai-org/glm-4-9b-chat-hf",
        "zai-org/glm-edge-1.5b-chat",
        "zai-org/glm-4-9b-hf",
        "zai-org/glm-edge-4b-chat",
        "zai-org/glm-4-9b-chat-1m-hf",
        "zai-org/LongReward-glm4-9b-DPO",
        "hereticness/heretic_glm-edge-4b-chat",
        "hereticness/heretic_glm-edge-1.5b-chat",
        "mlx-community/glm-4-9b-chat-1m-bf16",
        "mlx-community/glm-4-9b-chat-1m-6bit"
      ]
    },
    {
      "architecture_id": "GPTJXForCausalLM",
      "total_models": 12,
      "sample_models": [
        "BeardedMonster/SabiYarn-125M-translate",
        "KnutJaegersberg/GPT-JX-3b",
        "damilojohn/SabiYarn125m-finetune-translation",
        "BeardedMonster/SabiYarn-125M-sentiment",
        "BeardedMonster/SabiYarn-125M",
        "Aletheia-ng/SabiYarn-125M-NER",
        "BeardedMonster/SabiYarn-125M-topic",
        "BeardedMonster/SabiYarn-125M-Igbo-translate",
        "damilojohn/Sabiyarn_language_detection",
        "BeardedMonster/SabiYarn-125M-Yoruba-translate"
      ]
    },
    {
      "architecture_id": "RoFormerForCausalLM",
      "total_models": 12,
      "sample_models": [
        "junnyu/roformer_chinese_sim_char_base",
        "junnyu/roformer_chinese_sim_char_small",
        "junnyu/roformer_chinese_sim_char_ft_small",
        "junnyu/roformer_chinese_sim_char_ft_base",
        "jirvine/origen-model",
        "xyc8888/QAtest1",
        "hf-tiny-model-private/tiny-random-RoFormerForCausalLM",
        "kaizerBox/RoFormer_small-summarization",
        "Med0/my_awesome_eli5_clm-model",
        "kaizerBox/RoFormer-summarization"
      ]
    },
    {
      "architecture_id": "MinistralForCausalLM",
      "total_models": 12,
      "sample_models": [
        "Elmir67/ministral-8b-islamic-finance-v1",
        "Elmir67/ministral-islamic-finance-v3",
        "snezhanata/alpaca_prod_v1",
        "isbondarev/Ministral-8B-Instruct-2410-adv",
        "Elmir67/ministral-islamic-finance",
        "snezhanata2/alpaca_dev_10",
        "neredera/roleplay-Ministral-8B-2025-10",
        "snezhanata/alpaca_dev_v5",
        "snezhanata/alpaca_prod_v7",
        "aimaesi/aimaesi"
      ]
    },
    {
      "architecture_id": "LlavaGpt2ForCausalLM",
      "total_models": 12,
      "sample_models": [
        "toshi456/llava-jp-1.3b-v1.1",
        "toshi456/llava-jp-1.3b-v1.0",
        "hibikaze/finetune-llava-v1.5-japanese-gpt2-small_test-checkpoint-1200",
        "toshi456/ConvLLaVA-JP-1.3b-1280",
        "toshi456/llava-jp-1.3b-v1.1-llava-jp-instruct-108k",
        "toshi456/llava-jp-1.3b-v1.1-commoncatalog-cc-by-ext-10k",
        "toshi456/ConvLLaVA-JP-1.3b-768",
        "toshi456/llava-jp-1.3b-v1.0-siglip-so400m-patch14-384",
        "toshi456/llava-jp-1.3b-v1.0-620k",
        "morizon/dpo_visual_llavajp_refmodel"
      ]
    },
    {
      "architecture_id": "RECAST8b_LlamaForCausalLM",
      "total_models": 12,
      "sample_models": [
        "appledora/recast3.1-all-G16W128H64",
        "appledora/recast3.1-G16W16H8",
        "appledora/recast3.1-all-G16W64H32",
        "appledora/recast3.1-G16W8H4",
        "appledora/recast3.1-G16W128H64",
        "appledora/recast3.1-G4W8H1",
        "appledora/recast3.1-G8W16H4",
        "appledora/recast3.1-G16W32H16",
        "appledora/recastSDP3.1-G16W128H64",
        "appledora/recastSDP3.1-G8W32H8"
      ]
    },
    {
      "architecture_id": "GPT2MoEForCausalLM",
      "total_models": 12,
      "sample_models": [
        "i-be-snek/gpt2moe_het_100mb",
        "i-be-snek/gpt2moe_het_1000mb",
        "i-be-snek/gpt2moe_hom_1000mb",
        "i-be-snek/gpt2_moe_eng_hom_1025_10mb",
        "i-be-snek/gpt2_moe_eng_hom_1024_100mb_gelu",
        "i-be-snek/gpt2_moe_hom_1024_100mb_gelu",
        "i-be-snek/gpt2_moe_eng_hom_1024_100mb_gelu_tok",
        "i-be-snek/gpt2_moe_eng_hom_1024_100mb_gelu_gpt2mlp",
        "i-be-snek/gpt2_moe_hom_1024_100mb_gelu_mlp",
        "i-be-snek/gpt2moe_hom_100mb"
      ]
    },
    {
      "architecture_id": "WhisperForCausalLM",
      "total_models": 12,
      "sample_models": [
        "nelson-pawait/whisper-medium-sw",
        "nelson-pawait/checkpoints_3",
        "nelson-pawait/whisper-medium-sw-2",
        "mutemip/test-model",
        "Judah04/whisper-small-hausa",
        "woohyunnn/upload-repo",
        "yongchanskii/KONVERSE",
        "rahulvijayan/whisper-large-v2-8bit",
        "nelson-pawait/whisper-sw-1",
        "DinaYatsuk/whisper-medium-advanced"
      ]
    },
    {
      "architecture_id": "Ernie4_5ForCausalLM",
      "total_models": 11,
      "sample_models": [
        "baidu/ERNIE-4.5-0.3B-PT",
        "baidu/ERNIE-4.5-0.3B-Base-PT",
        "unsloth/ERNIE-4.5-0.3B-PT",
        "yujiepan/ernie-4.5-tiny-random",
        "AiAsistent/ERNIE-4.5-0.3B-PT-heretic",
        "isbondarev/ERNIE-4.5-0.3B-PT-adv",
        "onnx-community/ERNIE-4.5-0.3B-ONNX",
        "mhtccc/ernie-4-5-0.3B-finance-sft",
        "tiny-random/ernie-4.5",
        "ahczhg/ernie-4.5-0.3b-aegis-safety-lora"
      ]
    },
    {
      "architecture_id": "LLaDA2MoeModelLM",
      "total_models": 11,
      "sample_models": [
        "inclusionAI/LLaDA2.0-mini",
        "inclusionAI/LLaDA2.0-mini-preview",
        "inclusionAI/LLaDA2.0-flash",
        "inclusionAI/LLaDA2.0-mini-CAP",
        "mlx-community/LLaDA2.0-flash-4bit",
        "inclusionAI/LLaDA2.0-flash-preview",
        "mlx-community/LLaDA2.0-mini-preview-4bit",
        "mlx-community/LLaDA2.0-mini-8bit",
        "mlx-community/LLaDA2.0-flash-6bit",
        "mlx-community/LLaDA2.0-mini-6bit"
      ]
    },
    {
      "architecture_id": "StripedHyenaModelForCausalLM",
      "total_models": 11,
      "sample_models": [
        "togethercomputer/evo-1-8k-base",
        "togethercomputer/evo-1-131k-base",
        "LongSafari/evo-1-8k-crispr",
        "togethercomputer/StripedHyena-Nous-7B",
        "togethercomputer/StripedHyena-Hessian-7B",
        "LongSafari/evo-1-8k-transposon",
        "alexchen1999/virus_evo_70_1024_two_d",
        "mikeleske/evo-ft-genus-325",
        "Rocketknight1/evo-1k-test",
        "Norquinal/StripedHyena-Hessian-7B-claude-chat"
      ]
    },
    {
      "architecture_id": "Ministral3ForCausalLM",
      "total_models": 11,
      "sample_models": [
        "mlx-community/Devstral-2-123B-Instruct-2512-4bit",
        "root4k/Devstral-2-123B-Instruct-2512-mlx-mxfp4",
        "inferencerlabs/Devstral-2-123B-Instruct-2512-MLX-6.5bit",
        "mlx-community/Devstral-2-123B-Instruct-2512-8bit",
        "mlx-community/Devstral-2-123B-Instruct-2512-bf16",
        "mlx-community/Devstral-2-123B-Instruct-2512-6bit",
        "mlx-community/Devstral-2-123B-Instruct-2512-5bit",
        "tiny-random/devstral-2",
        "yujiepan/ministral-3-tiny-random",
        "tiny-random/ministral-3"
      ]
    },
    {
      "architecture_id": "JetMoEForCausalLM",
      "total_models": 11,
      "sample_models": [
        "jetmoe/jetmoe-8b",
        "jetmoe/jetmoe-8b-chat",
        "jetmoe/jetmoe-8b-sft",
        "AndreaUnibo/JetMoE_rank_lstm_full_trained_3",
        "AndreaUnibo/JetMoE_rank_lstm_full_trained_depth3_n2",
        "idoru/jetmoe-8b-ultrainteract-sft-v4",
        "AndreaUnibo/JetMoE_base_full_trained",
        "AndreaUnibo/JetMoE_rank_lstm_updated",
        "AndreaUnibo/JetMoE_rank_lstm_full_trained_2",
        "AndreaUnibo/JetMoE_rank_infill_full_trained_2"
      ]
    },
    {
      "architecture_id": "ZhinaoForCausalLM",
      "total_models": 11,
      "sample_models": [
        "qihoo360/360Zhinao-7B-Base",
        "qihoo360/360Zhinao-1.8B-Reranking",
        "qihoo360/360Zhinao-7B-Chat-32K",
        "qihoo360/360Zhinao2-7B-Base",
        "qihoo360/360Zhinao2-7B-Chat-32K-Int4",
        "qihoo360/360Zhinao-7B-Chat-360K",
        "qihoo360/360Zhinao2-7B-Chat-360K",
        "qihoo360/360Zhinao2-7B-Chat-360K-Int4",
        "qihoo360/360Zhinao-7B-Chat-32K-Int4",
        "qihoo360/360Zhinao3-7B"
      ]
    },
    {
      "architecture_id": "CacaForCausalLM",
      "total_models": 11,
      "sample_models": [
        "Lyon28/caca-75M-untrained",
        "Lyon28/caca-50M-untrained",
        "Lyon28/caca-100M-untrained",
        "Lyon28/caca-150M-untrained",
        "Lyon28/caca-400M-untrained",
        "Lyon28/caca-300M-untrained",
        "Lyon28/caca-250M-untrained",
        "Lyon28/caca-500M-untrained",
        "Lyon28/caca-700M-untrained",
        "Lyon28/caca-900M-untrained"
      ]
    },
    {
      "architecture_id": "KPhi3ForCausalLM",
      "total_models": 11,
      "sample_models": [
        "schuler/experimental-JP47D20",
        "schuler/experimental-JP47D56",
        "schuler/experimental-JP47D55C",
        "schuler/experimental-JP47G04",
        "schuler/experimental-JP47D54C",
        "schuler/experimental-JP47D54B",
        "schuler/experimental-JP47D55",
        "schuler/experimental-JP47G03C",
        "schuler/experimental-JP47D55B",
        "schuler/experimental-DISTIL-32"
      ]
    },
    {
      "architecture_id": "Qwen2VLForConditionalGeneration",
      "total_models": 11,
      "sample_models": [
        "scb10x/typhoon2-qwen2vl-7b-vision-instruct",
        "yujiepan/qwen2-vl-tiny-random",
        "RetrO21/agrofinetune",
        "dropbox-dash/Qwen2-VL-2B-Instruct_4bitgs64_hqq_hf",
        "4bit/Qwen2-VL-2B-Instruct",
        "4bit/Qwen2-VL-7B-Instruct",
        "Beagledata/Elpis-VL-7B",
        "trl-algo/summary_tags_qwen2_vl_v1",
        "Rewatiramans/Dermatech-Qwen2-VL-2B",
        "whalezzz/MM-RAIT-Qwen2-VL"
      ]
    },
    {
      "architecture_id": "PldrllmForCausalLM",
      "total_models": 11,
      "sample_models": [
        "fromthesky/PLDR-LLM-v51-104M",
        "fromthesky/PLDR-LLM-v51G-106M-3",
        "fromthesky/PLDR-LLM-v51G-106M-2",
        "fromthesky/PLDR-LLM-v51-110M-5",
        "fromthesky/PLDR-LLM-v51-DAG-110M",
        "fromthesky/PLDR-LLM-v51G-106M-1",
        "fromthesky/PLDR-LLM-v51-110M-2",
        "fromthesky/PLDR-LLM-v51-110M-4",
        "fromthesky/PLDR-LLM-v51-110M-1",
        "fromthesky/PLDR-LLM-v51-110M-3"
      ]
    },
    {
      "architecture_id": "RECAST1B_LlamaForCausalLM",
      "total_models": 11,
      "sample_models": [
        "appledora/recast3.2-all-G8W64H32",
        "appledora/recast-llama3.2-f8t2",
        "appledora/recast3.2-G4W16H4",
        "appledora/recast3.2-G16W4H4",
        "appledora/recast3.2-G4W32H8",
        "appledora/recast3.2-all-G4W8H2",
        "appledora/recast3.2-all-G8W16H8R8",
        "appledora/recast3.2-G8W8H4",
        "appledora/recastSDP3.2-G8W128H64",
        "appledora/recast3.2-G16W8H8"
      ]
    },
    {
      "architecture_id": "SkipMiddleModel",
      "total_models": 11,
      "sample_models": [
        "tim-lawson/skip-middle-fineweb-nocontrol-10-layers",
        "tim-lawson/skip-middle-fineweb-nocontrol-6-layers",
        "tim-lawson/skip-middle-fineweb-baseline-8-layers",
        "tim-lawson/skip-middle-fineweb-baseline-10-layers",
        "tim-lawson/skip-middle-fineweb-baseline-2-layers",
        "tim-lawson/skip-middle-fineweb-gated-target-end-0.6",
        "tim-lawson/skip-middle-fineweb-gated-target-end-0.5",
        "tim-lawson/skip-middle-fineweb-nocontrol-12-layers",
        "tim-lawson/skip-middle-fineweb-gated-target-end-0.7",
        "tim-lawson/skip-middle-fineweb-gated-target-end-0.8"
      ]
    },
    {
      "architecture_id": "ReVisionForConditionalGeneration",
      "total_models": 11,
      "sample_models": [
        "utischoolnlp/ReVision-250M-256-16-baseline",
        "utischoolnlp/ReVision-250M-256-16-stage2",
        "hsiangfu/ReVision-250M-256-16-selfcaption-metadata",
        "hsiangfu/ReVision-250M-256-16-baseline",
        "hsiangfu/ReVision-250M-256-16-selfcaption-easyocr",
        "anonymoususerrevision/ReVision-250M-256-16-baseline",
        "anonymoususerrevision/ReVision-250M-256-16-selfcaption-metadata",
        "anonymoususerrevision/ReVision-250M-256-16",
        "anonymoususerrevision/ReVision-250M-256-16-metadata-easyocr",
        "anonymoususerrevision/ReVision-250M-256-16-selfcaption-easyocr"
      ]
    },
    {
      "architecture_id": "Qwen2MLPWithBiasForCausalLM",
      "total_models": 11,
      "sample_models": [
        "cfierro/Qwen2.5-7B-bias-pv-prompts-non-sycophantic",
        "cfierro/Qwen2.5-7B-bias-pv-prompts-sycophantic",
        "cfierro/Qwen2.5-1.5B-Instruct-bias-pv-prompts-non-sycophantic",
        "cfierro/Qwen2.5-1.5B-Instruct-bias-pv-prompts-sycophantic",
        "cfierro/Qwen2.5-7B-bias-pv-prompts-non-evil",
        "cfierro/Qwen2.5-7B-bias-pv-prompts-evil",
        "cfierro/Qwen2.5-1.5B-Instruct-bias-pv-prompts-non-sycophantic_1e-4",
        "cfierro/Qwen2.5-7B-bias-pv-prompts-non-evil_5e-05",
        "cfierro/Qwen2.5-7B-bias-pv-prompts-sycophantic_1e-4",
        "cfierro/Qwen2.5-7B-bias-pv-prompts-non-sycophantic_1e-4"
      ]
    },
    {
      "architecture_id": "ProbeDSGLlavaLlamaForCausalLM",
      "total_models": 11,
      "sample_models": [
        "shi-labs/probe_gen_llava-1.5-pt-0.5ift",
        "shi-labs/probe_seg_llava-1.5-pt-vpt-ift",
        "shi-labs/probe_gen_ola-vlm-pt-ift",
        "shi-labs/probe_seg_llava-1.5-pt-ift",
        "shi-labs/probe_seg_llava-1.5-pt",
        "shi-labs/probe_depth_llava-1.5-pt-ift",
        "shi-labs/probe_gen_llava-1.5-pt-ift",
        "shi-labs/probe_gen_llava-1.5-pt-vpt-ift",
        "shi-labs/probe_seg_llava-1.5-pt-0.5ift",
        "shi-labs/probe_gen_llava-1.5-pt"
      ]
    },
    {
      "architecture_id": "ProgressiveYocoLlamaForCausalLM",
      "total_models": 11,
      "sample_models": [
        "hosseinbv/newData-progressive-yoco-tiny-llama-CDL-19",
        "hosseinbv/newData-progressive-yoco-tiny-llama-CDL-18",
        "hosseinbv/newData-progressive-yoco-tiny-llama-CDL-20",
        "hosseinbv/newData-progressive-yoco-tiny-llama-CDL-21",
        "hosseinbv/newData-progressive-yoco-tiny-llama-CDL-17",
        "hosseinbv/prog-y-tiny-llama-CDL-13",
        "hosseinbv/prog-y-tiny-llama-CDL-21",
        "hosseinbv/prog-y-tiny-llama-CDL-14",
        "hosseinbv/prog-y-tiny-llama-CDL-17",
        "hosseinbv/prog-y-tiny-llama-CDL-15"
      ]
    },
    {
      "architecture_id": "BambaForCausalLM",
      "total_models": 10,
      "sample_models": [
        "hmellor/tiny-random-BambaForCausalLM",
        "ibm-ai-platform/Bamba-9B-v1",
        "ibm-ai-platform/Bamba-9B-v2",
        "ibm-ai-platform/Bamba-9B-fp8",
        "ibm-ai-platform/Bamba-9B-2T",
        "ibm-ai-platform/Bamba-9B-1.8T",
        "ibm-ai-platform/Bamba-9B-1.8T-fp8",
        "ibm-ai-platform/Bamba-9B-2T-fp8",
        "tiny-random/bamba",
        "yujiepan/bamba-tiny-random"
      ]
    },
    {
      "architecture_id": "MiniCPM3ForCausalLM",
      "total_models": 10,
      "sample_models": [
        "openbmb/MiniCPM3-4B",
        "openbmb/MiniCPM3-4B-GPTQ-Int4",
        "AtakanTekparmak/MiniCPM3-4B-GGUF",
        "c01zaut/MiniCPM3-4B-RAG-LoRA-rk3588-1.1.1",
        "c01zaut/MiniCPM3-4B-None-rk3588-1.1.1",
        "mlx-community/MiniCPM3-4B-6bit",
        "mlx-community/MiniCPM3-4B-bfloat16",
        "mlx-community/MiniCPM3-4B-4bit",
        "mlx-community/MiniCPM3-4B-8bit",
        "c01zaut/MiniCPM3-4B-rk3588-1.1.4"
      ]
    },
    {
      "architecture_id": "ReformerModelWithLMHead",
      "total_models": 10,
      "sample_models": [
        "google/reformer-crime-and-punishment",
        "google/reformer-enwik8",
        "patrickvonplaten/reformer-random",
        "patrickvonplaten/hf-reformer-crime-and-punish",
        "mwesner/reformer-clm",
        "patrickvonplaten/reformer-tiny-random",
        "miguelvictor/python-fromzero-reformerlm",
        "kaizerBox/ReFormer-summarization",
        "humbleworth/reformer-character-domain-generator",
        "kaizerBox/ReFormer-small-summarization"
      ]
    },
    {
      "architecture_id": "JapaneseStableLMAlphaForCausalLM",
      "total_models": 10,
      "sample_models": [
        "stabilityai/japanese-stablelm-base-alpha-7b",
        "stabilityai/japanese-stablelm-instruct-alpha-7b-v2",
        "stabilityai/japanese-stablelm-instruct-alpha-7b",
        "tsukemono/japanese-stablelm-base-alpha-7b-f16-marisa",
        "4bit/japanese-stablelm-instruct-alpha-7b",
        "atsushi3110/mydev-mock-ja-stablelm-7b",
        "leemeng/llm-experimental",
        "4bit/japanese-stablelm-instruct-alpha-7b-s",
        "liezeleinstein/erikatest2",
        "4bit/japanese-stablelm-base-alpha-7b-s"
      ]
    },
    {
      "architecture_id": "PlamoForCausalLM",
      "total_models": 10,
      "sample_models": [
        "pfnet/plamo-13b",
        "mlx-community/plamo-2-1b",
        "pfnet/plamo-100b",
        "pfnet/plamo-13b-instruct",
        "pfnet/plamo-13b-instruct-nc",
        "mlx-community/plamo-2-1b-bf16",
        "zamagi/plamo-2-1b-gorilla-chat2",
        "zamagi/plamo-2-1b-gorilla-chat3",
        "zamagi/plamo-2-1b-gorilla-chat5",
        "mlx-community/plamo-2-8b-4bit"
      ]
    },
    {
      "architecture_id": "MossForCausalLM",
      "total_models": 10,
      "sample_models": [
        "OpenMOSS-Team/moss-moon-003-base",
        "OpenMOSS-Team/moss-moon-003-sft",
        "OpenMOSS-Team/moss-moon-003-sft-plugin-int4",
        "OpenMOSS-Team/moss-moon-003-sft-int4",
        "OpenMOSS-Team/moss-moon-003-sft-plugin-int8",
        "OpenMOSS-Team/moss-base-7b",
        "OpenMOSS-Team/moss-moon-003-sft-plugin",
        "OpenMOSS-Team/moss-moon-003-sft-int8",
        "DotIN13/moss-moon-003-sft-int4-fix-autotune",
        "forchen1/moss-moon-003-base"
      ]
    },
    {
      "architecture_id": "MobiLlamaForCausalLM",
      "total_models": 10,
      "sample_models": [
        "MBZUAI/MobiLlama-05B",
        "MBZUAI/MobiLlama-1B",
        "MBZUAI/MobiLlama-08B",
        "AhmadMustafa/MobiLLama-Urdu-Article-Generation",
        "Sayan01/Phi35-3B-ML-KL",
        "Sayan01/Phi35-3B-ML-DKD-5",
        "Sayan01/Phi35-3B-ML-DKD-20",
        "Sayan01/Phi35-3B-ML-CLM",
        "Sayan01/Phi35-3B-ML-DKD-1",
        "Sayan01/Phi35-3B-ML-DKD-10"
      ]
    },
    {
      "architecture_id": "Rwkv6ForCausalLM",
      "total_models": 10,
      "sample_models": [
        "paperfun/rwkv",
        "RWKV/rwkv-6-world-1b6",
        "RWKV/v6-Finch-3B-HF",
        "RWKV/v6-Finch-1B6-HF",
        "RWKV/rwkv-6-world-7b",
        "RWKV/rwkv-6-world-3b-v2.1",
        "RWKV/rwkv-6-world-3b",
        "RWKV/v6-Finch-7B-HF",
        "SmerkyG/rwkv-6-world-v2.1-3b",
        "SmerkyG/rwkv-6-world-3b"
      ]
    },
    {
      "architecture_id": "Qwen2ParScaleForCausalLM",
      "total_models": 10,
      "sample_models": [
        "ParScale/ParScale-1.8B-P1-Inst",
        "ParScale/ParScale-Qwen-3B-P2-Python",
        "ParScale/ParScale-1.8B-P8",
        "ParScale/ParScale-1.8B-P2-Inst",
        "ParScale/ParScale-1.8B-P8-Inst",
        "ParScale/ParScale-Qwen-3B-P4-Python",
        "ParScale/ParScale-1.8B-P4-Inst",
        "ParScale/ParScale-1.8B-P1",
        "ParScale/ParScale-1.8B-P2",
        "ParScale/ParScale-1.8B-P4"
      ]
    },
    {
      "architecture_id": "ModernBertDecoderForCausalLM",
      "total_models": 10,
      "sample_models": [
        "sileod/ettin-decoder-68m-instruct",
        "sileod/ettin-decoder-32m-instruct",
        "onnx-internal-testing/tiny-random-ModernBertDecoderForCausalLM",
        "jhu-clsp/ettin-dec-from-enc-17m",
        "onnx-community/ettin-decoder-400m-ONNX",
        "onnx-community/ettin-decoder-17m-ONNX",
        "onnx-community/ettin-decoder-68m-ONNX",
        "onnx-community/ettin-decoder-32m-ONNX",
        "onnx-community/ettin-decoder-150m-ONNX",
        "onnx-community/ettin-decoder-1b-ONNX"
      ]
    },
    {
      "architecture_id": "DiscreteDiffusionModel",
      "total_models": 10,
      "sample_models": [
        "myduy/dlm-vi2en",
        "myduy/dlm-vi2en-checkpoint-138000",
        "myduy/dlm-vi2en-checkpoint-114000",
        "myduy/dlm-vi2en-checkpoint-130500",
        "myduy/dlm-vi2en-checkpoint-66000",
        "myduy/dlm-vi2en-checkpoint-90000",
        "myduy/dlm-amr-parsing-30000",
        "myduy/dlm-vi2en-checkpoint-93000",
        "myduy/dlm-vi2en-checkpoint-150000",
        "myduy/dlm-parsing-22500"
      ]
    },
    {
      "architecture_id": "NanbeigeForCausalLM",
      "total_models": 10,
      "sample_models": [
        "TheBloke/Nanbeige-16B-Chat-GPTQ",
        "TheBloke/Nanbeige-16B-Chat-32K-GPTQ",
        "TheBloke/Nanbeige-16B-Base-32K-GPTQ",
        "TheBloke/Nanbeige-16B-Base-GPTQ",
        "Zoyd/Nanbeige_Nanbeige2-16B-Chat-6_0bpw_exl2",
        "Zoyd/Nanbeige_Nanbeige2-16B-Chat-6_5bpw_exl2",
        "Zoyd/Nanbeige_Nanbeige2-16B-Chat-2_2bpw_exl2",
        "Zoyd/Nanbeige_Nanbeige2-16B-Chat-4_25bpw_exl2",
        "Zoyd/Nanbeige_Nanbeige2-16B-Chat-5_0bpw_exl2",
        "Zoyd/Nanbeige_Nanbeige2-16B-Chat-2_5bpw_exl2"
      ]
    },
    {
      "architecture_id": "T5LaForConditionalGeneration",
      "total_models": 10,
      "sample_models": [
        "hrezaei/T5Lae-Large-WeightedLoss",
        "hrezaei/T5Laa-Large-WeightedLoss",
        "hrezaei/T5La-Large-WeightedLoss-Instruct",
        "hrezaei/T5Lae-Large-WeightedLoss-InstructMixed",
        "hrezaei/T5Lae-Large-WeightedLoss-Instruct",
        "hrezaei/T5Laa2-Large-WeightedLoss-Instruct",
        "hrezaei/T5Laa-Large-WeightedLoss-Instruct",
        "hrezaei/T5Lae-Large-newloss",
        "hrezaei/T5La-Large-WeightedLoss",
        "hrezaei/T5La-Large"
      ]
    },
    {
      "architecture_id": "PhiRotForCausalLM",
      "total_models": 10,
      "sample_models": [
        "kaizen9/phi_spun_118_v3",
        "kaizen9/phi_dclm_spun",
        "kaizen9/orth_phi_400_spun",
        "kaizen9/phi_spun_450_fish",
        "kaizen9/phi_spun",
        "kaizen9/SEQ_phi_400_spun_159",
        "kaizen9/seq_on_55",
        "kaizen9/spin_SEQ_chan_53",
        "kaizen9/phi_cosmo",
        "kaizen9/phi_spun_fish_accum_700"
      ]
    },
    {
      "architecture_id": "GeometricForCausalLM",
      "total_models": 10,
      "sample_models": [
        "Lanni-ni/geometric_pile_2layer",
        "Lanni-ni/geometric_4_6_384_",
        "Lanni-ni/geometric_4_6_384_pile",
        "Lanni-ni/geometric_2L_4H_256D_babylm",
        "Lanni-ni/geometric_babylm_10m_2_4_256",
        "Lanni-ni/geometric_babylm_10m_4_6_384",
        "Lanni-ni/geometric_babylm_100m_4layer",
        "Lanni-ni/geometric_pile_4layer",
        "Lanni-ni/geometric_babylm_100m_2layer",
        "Lanni-ni/geometric_babylm_10m_4layer"
      ]
    },
    {
      "architecture_id": "BTLMLMHeadModel",
      "total_models": 9,
      "sample_models": [
        "cerebras/btlm-3b-8k-base",
        "EleutherAI/Hermes-btlm-3b-8k",
        "cerebras/btlm-3b-8k-chat",
        "KnutJaegersberg/btlm-3b-8k-base",
        "robertmyers/sakura-3b-cherry-flavor-tuned",
        "robertmyers/sakura-3b-cherry-flavor",
        "misterkuka/8bit-btlm-3b-8k-base",
        "robertmyers/sakura-3b",
        "KnutJaegersberg/Walter-BTLM-3B"
      ]
    },
    {
      "architecture_id": "LongcatFlashForCausalLM",
      "total_models": 9,
      "sample_models": [
        "meituan-longcat/LongCat-Flash-Chat",
        "meituan-longcat/LongCat-Flash-Chat-FP8",
        "meituan-longcat/LongCat-Flash-Thinking",
        "yujiepan/longcat-flash-tiny-random",
        "mlx-community/LongCat-Flash-Chat-4bit",
        "meituan-longcat/LongCat-Flash-Thinking-FP8",
        "finding1/LongCat-Flash-Chat-MLX-5.5bpw",
        "TOTORONG/LongCat-Flash-3.5bits",
        "tiny-random/longcat-flash"
      ]
    },
    {
      "architecture_id": "AV2TextForConditionalGeneration",
      "total_models": 9,
      "sample_models": [
        "nguyenvulebinh/AV-HuBERT-MuAViC-en",
        "nguyenvulebinh/AV-HuBERT-MuAViC-ru",
        "nguyenvulebinh/AV-HuBERT-MuAViC-de",
        "nguyenvulebinh/AV-HuBERT-MuAViC-fr",
        "nguyenvulebinh/AV-HuBERT-MuAViC-multilingual",
        "nguyenvulebinh/AV-HuBERT-MuAViC-ar",
        "nguyenvulebinh/AV-HuBERT-MuAViC-el",
        "nguyenvulebinh/AV-HuBERT-MuAViC-it",
        "nguyenvulebinh/AV-HuBERT-MuAViC-es"
      ]
    },
    {
      "architecture_id": "ChatGLMModel",
      "total_models": 9,
      "sample_models": [
        "zai-org/codegeex4-all-9b",
        "bigcode/octogeex",
        "byroneverson/glm-4-9b-chat-abliterated",
        "chillymiao/Hyacinth6B",
        "Ian-14/model_test",
        "derek33125/project-angel-chatglm4-v2",
        "NeoZ123/LongReward-glm4-9b-SFT",
        "neonicholasi/chatglm3-6b-base-extraction",
        "FriendliAI/glm-4-9b"
      ]
    },
    {
      "architecture_id": "BitnetForCausalLM",
      "total_models": 9,
      "sample_models": [
        "1bitLLM/bitnet_b1_58-3B",
        "1bitLLM/bitnet_b1_58-xl",
        "BoscoTheDog/bitnet_b1_58-xl_q8_0_gguf",
        "Ttimofeyka/bitnet-5B-v1",
        "kousw/bitnet_b1_58-3B_quantized",
        "Ttimofeyka/bitnet-5B-v0",
        "BoscoTheDog/Bessie_bitnet_instruct_100k_gguf",
        "BoscoTheDog/bitnet_instruct_q16_gguf",
        "PavelGolikov/12960179"
      ]
    },
    {
      "architecture_id": "IndexForCausalLM",
      "total_models": 9,
      "sample_models": [
        "IndexTeam/Index-1.9B-Chat",
        "IndexTeam/Index-1.9B-Character",
        "IndexTeam/Index-1.9B-Pure",
        "IndexTeam/Index-1.9B-Constant-LR",
        "IntervitensInc/Index-1.9b-glaive",
        "lucyknada/IndexTeam_Index-1.9B-Chat-exl2-6.0bpw",
        "Light4Bear/prohibited-Index-1.9B-Chat",
        "EdgerunnersArchive/IndexTeam_Index-1.9B-Chat-exl2-6.0bpw",
        "IntervitensInc/Index-1.9b-glaive-1e-5"
      ]
    },
    {
      "architecture_id": "MiniGeminiLlamaForCausalLM",
      "total_models": 9,
      "sample_models": [
        "YanweiLi/MGM-7B",
        "FoundationVision/unitok_mllm",
        "YanweiLi/MGM-34B",
        "YanweiLi/MGM-13B-HD",
        "YanweiLi/MGM-34B-HD",
        "YanweiLi/MGM-13B",
        "dvtoan18/Minigemini-34b-400-steps",
        "dvtoan18/Minigemini-34b-200-steps",
        "KU-AGI/OSPO-Unitok-MLLM-7B"
      ]
    },
    {
      "architecture_id": "InternVLChatModel",
      "total_models": 9,
      "sample_models": [
        "numind/NuExtract-2-8B-experimental",
        "numind/NuExtract-2-2B-experimental",
        "numind/NuExtract-2-4B-experimental",
        "aghorbani/h2ovl-mississippi-800m",
        "kernelPanicAtTheDisco/NuExtract-2-2B-NFA",
        "kernelPanicAtTheDisco/NuExtract-2-8B-NFA",
        "kernelPanicAtTheDisco/NuExtract-2-4B-NFA",
        "kernelPanicAtTheDisco/NuExtract-2-1B-NFA",
        "Marcin-XStudio/nu-extract-2-fork"
      ]
    },
    {
      "architecture_id": "LlavaGemmaForCausalLM",
      "total_models": 9,
      "sample_models": [
        "Intel/llava-gemma-7b",
        "aimagelab/LLaVA_MORE-gemma_2_9b-finetuning",
        "binxia/llmga-gemma-2b-it-full-finetune",
        "TensorSenseAI/gemamba-v0",
        "luodian/SL-GMA2B-la1-6mix-fvis-dynamo",
        "TensorSenseAI/videogemma",
        "binxia/llmga-cn-gemma-2b-it-full-finetune",
        "ura-hcmut/GemSUraV-7B",
        "ura-hcmut/GemSUraV-2B"
      ]
    },
    {
      "architecture_id": "STLForCausalLM",
      "total_models": 9,
      "sample_models": [
        "saracandu/stldec_random_16_large",
        "saracandu/stldec_random_32_large",
        "saracandu/stldec_random_128",
        "saracandu/stldec_random_16_umap",
        "saracandu/stldec_random_32",
        "saracandu/stldec_onlyhard",
        "saracandu/stldec_random_16",
        "saracandu/stldec_random_64_umap",
        "saracandu/stldec_random_128_umap"
      ]
    },
    {
      "architecture_id": "Qwen2ReasoningForCausalLM",
      "total_models": 9,
      "sample_models": [
        "agurung/sft_qwen7B_25percent",
        "agurung/v3ff_savebestearly_sft_qwen7B_25percent_lr_1e4_bptt_offset_newprompt",
        "agurung/sft_qwen7B_25percent_lr_1e4_allgrad",
        "agurung/v2sft_all_qwen7B_25percent_lr_1e4_allgrad",
        "agurung/v3sft_qwen7B_25percent_lr_1e4_bptt_offset",
        "agurung/v4_savebestearly_sft_qwen7B_25percent_lr_1e3_bptt_offset",
        "agurung/v4_savebestearly_sft_qwen7B_25percent_lr_1e4_bptt_offset",
        "agurung/v1ff_savebestearly_sft_qwen7B_25percent_lr_1e4_bptt_offset",
        "agurung/v2ff_savebestearly_sft_qwen7B_25percent_lr_1e4_bptt_offset"
      ]
    },
    {
      "architecture_id": "Avey",
      "total_models": 9,
      "sample_models": [
        "avey-ai/avey1-dpa-1.5B-95BT",
        "avey-ai/avey1-1.5B-it-preview-100BT",
        "avey-ai/avey1-dpa-0.1B-100BT",
        "avey-ai/avey1-dpa-0.1B-90BT",
        "avey-ai/avey1-dpa-0.1B-95BT",
        "avey-ai/avey1-dpa-0.5B-90BT",
        "avey-ai/avey1-dpa-1.5B-100BT",
        "avey-ai/avey1-dpa-1.5B-90BT",
        "avey-ai/avey1-dpa-0.5B-100BT"
      ]
    },
    {
      "architecture_id": "TPPForCausalLM",
      "total_models": 9,
      "sample_models": [
        "avey-ai/tpp-dpa-0.1B-90BT",
        "avey-ai/tpp-dpa-1.5B-90BT",
        "avey-ai/tpp-dpa-0.1B-100BT",
        "avey-ai/tpp-dpa-1.5B-95BT",
        "avey-ai/tpp-dpa-0.5B-100BT",
        "avey-ai/tpp-dpa-0.5B-95BT",
        "avey-ai/tpp-dpa-1.5B-100BT",
        "avey-ai/tpp-dpa-0.1B-95BT",
        "avey-ai/tpp-dpa-0.5B-90BT"
      ]
    },
    {
      "architecture_id": "Qwen2AdapterForCausalLM",
      "total_models": 9,
      "sample_models": [
        "zeliang0426/qwen25_code_r1_grpo_think",
        "zeliang0426/Fix-Strict_Darpo-cache-adapter-3k",
        "zeliang0426/Base-Qwen25-7-Think-adapter-3k",
        "zeliang0426/Limited_Base-Qwen25-7-Think-adapter-3k",
        "zeliang0426/Qwen25-3-Think-nglobal_16",
        "zeliang0426/Qwen25-3-Think-no_global",
        "zeliang0426/Qwen25-3-Think-nglobal_48",
        "zeliang0426/Qwen25-3-Think-nglobal_32",
        "zeliang0426/Qwen25-3-Cache-Sink"
      ]
    },
    {
      "architecture_id": "DotsOCRForCausalLM",
      "total_models": 8,
      "sample_models": [
        "rednote-hilab/dots.ocr",
        "prithivMLmods/Dots.OCR-Latest-BF16",
        "rednote-hilab/dots.ocr.base",
        "helizac/dots.ocr-4bit",
        "FriendliAI/dots.ocr",
        "hexiydev/dots.ocr",
        "rayan-orkom/dots_ocr_copy",
        "khang119966/dotsocr_1120"
      ]
    },
    {
      "architecture_id": "MiMoForCausalLM",
      "total_models": 8,
      "sample_models": [
        "XiaomiMiMo/MiMo-7B-Base",
        "XiaomiMiMo/MiMo-7B-RL",
        "XiaomiMiMo/MiMo-7B-SFT",
        "XiaomiMiMo/MiMo-7B-RL-Zero",
        "XiaomiMiMo/MiMo-7B-RL-0530",
        "mlx-community/MiMo-7B-SFT-4bit",
        "kos000113/xiaomi-mimo-7B-RT-int4",
        "huihui-ai/MiMo-7B-RL-0530-abliterated"
      ]
    },
    {
      "architecture_id": "MllamaForConditionalGeneration",
      "total_models": 8,
      "sample_models": [
        "RedHatAI/Llama-3.2-11B-Vision-Instruct-FP8-dynamic",
        "RedHatAI/Llama-3.2-90B-Vision-Instruct-FP8-dynamic",
        "convaiinnovations/ECG-Instruct-Llama-3.2-11B-Vision",
        "yujiepan/llama-3.2-vision-tiny-random",
        "fbaldassarri/meta-llama_Llama-3.2-11B-Vision-Instruct-OpenVino",
        "parsa-mhmdi/ChestXLLaMA_Stage1",
        "parsa-mhmdi/ChestXLLaMA_Stage2",
        "veotri/Llama-3.2-11B-Vision-Instruct-Ludoviko"
      ]
    },
    {
      "architecture_id": "MosaicGPT",
      "total_models": 8,
      "sample_models": [
        "anas-awadalla/mpt-1b-redpajama-200b-dolly",
        "anas-awadalla/mpt-1b-redpajama-200b",
        "anas-awadalla/mpt-1b-redpajama-200b-hf-style",
        "Dampish/Workspace",
        "seantyh/mpt-1b-rp200b-dolly",
        "Ozgur98/llm_deploy_small",
        "Nethermind/Mpt-Instruct-DotNet-XS",
        "Ozgur98/pushed_model_mosaic_small"
      ]
    },
    {
      "architecture_id": "CheXagentForCausalLM",
      "total_models": 8,
      "sample_models": [
        "StanfordAIMI/CheXagent-2-3b",
        "StanfordAIMI/CheXagent-2-3b-srrg-findings",
        "StanfordAIMI/CheXagent-2-3b-srrg-impression",
        "erjui/CheXagent-2-3b-csrrg-findings",
        "nclgbd/CheXagent-Residual",
        "erjui/CheXagent-2-3b-srrg-findings",
        "erjui/CheXagent-2-3b-csrrg-impression",
        "erjui/CheXagent-2-3b-srrg-impression"
      ]
    },
    {
      "architecture_id": "TransfoXLLMHeadModel",
      "total_models": 8,
      "sample_models": [
        "transfo-xl/transfo-xl-wt103",
        "hf-tiny-model-private/tiny-random-TransfoXLLMHeadModel",
        "hauson-fan/reuse",
        "aakamishra/transfo-xl-hend-hendrycks",
        "protectai-bot/transfo-xl",
        "esenergun/test",
        "zpbrent/reuse",
        "zpbrent/transfo-xl"
      ]
    },
    {
      "architecture_id": "HGRNBitForCausalLM",
      "total_models": 8,
      "sample_models": [
        "ridger/MMfreeLM-370M",
        "ridger/MMfreeLM-2.7B",
        "dhanesh123in/HGRNBitCausalLM",
        "intelava/smollm2-mmfree-h100",
        "1em0n/results",
        "rua9902/RC-inspire-LLM-ex1-b256",
        "hellonet22/codeparrot-ds",
        "Sakib323/MMfreeLM-370M"
      ]
    },
    {
      "architecture_id": "Florence2ForConditionalGeneration",
      "total_models": 8,
      "sample_models": [
        "onnx-community/Florence-2-base-ft",
        "onnx-community/Florence-2-base",
        "Xenova/tiny-random-Florence2ForConditionalGeneration",
        "onnx-community/Florence-2-large-ft",
        "TrumpMcDonaldz/Florence-2-large-onnx",
        "TrumpMcDonaldz/x",
        "kukiui/electricFlorence",
        "Alasil/Classification_NU"
      ]
    },
    {
      "architecture_id": "GPTOptim",
      "total_models": 8,
      "sample_models": [
        "distributed/optimized-gpt2-250m-v0.1.2",
        "distributed/optimized-gpt2-1b",
        "distributed/optimized-gpt2-250m-convergence-test-v1",
        "kmfoda/test_bf16",
        "distributed/optimized-gpt2-1b-vtestnet-v2",
        "distributed/optimized-gpt2-250m-convergence-test-v2",
        "distributed/optimized-gpt2-1b-stable-embeddings",
        "distributed/optimized-gpt2-2b-without-stable-embeddings"
      ]
    },
    {
      "architecture_id": "HeliumForCausalLM",
      "total_models": 8,
      "sample_models": [
        "kyutai/helium-1-preview-2b",
        "onnx-community/helium-1-preview-2b-ONNX",
        "mlx-community/helium-1-preview-2b-float32",
        "mlx-community/helium-1-preview-2b",
        "mlx-community/helium-1-preview-2b-8bit",
        "mlx-community/helium-1-preview-2b-4bit",
        "Goekdeniz-Guelmez/Josie-v6-2b-mlx-concept",
        "Goekdeniz-Guelmez/JosiexHelium-v6-2B-mlx-Base"
      ]
    },
    {
      "architecture_id": "LlamaMoEForCausalLM",
      "total_models": 8,
      "sample_models": [
        "llama-moe/LLaMA-MoE-v1-3_5B-2_8",
        "llama-moe/LLaMA-MoE-v1-3_5B-4_16",
        "llama-moe/LLaMA-MoE-v1-3_5B-2_8-sft",
        "JuncaiL/llama-8x265m-moe",
        "llama-moe/LLaMA-MoE-v1-3_5B-4_16-sft",
        "llama-moe/LLaMA-MoE-v1-3_0B-2_16-sft",
        "Spico/LLaMA-MoE-v1-2_8-DynamicSFT",
        "Spico/LLaMA-MoE-v1-2_8-UniformSFT"
      ]
    },
    {
      "architecture_id": "Qwen2_5OmniThinkerForConditionalGeneration",
      "total_models": 8,
      "sample_models": [
        "LCO-Embedding/LCO-Embedding-Omni-7B",
        "LCO-Embedding/LCO-Embedding-Omni-3B",
        "Haoz0206/Omni-R1",
        "ngqtrung/Qwen2.5-Omni-Thinker-7B",
        "BarryFutureman/Omni-Whisper-3B",
        "TheMindExpansionNetwork/Pixel-OMNI",
        "ASultan/Qwen-Omni-7B-MELD-finetuned",
        "OpenMOSS-Team/RoboOmni-LIBERO-Spatial"
      ]
    },
    {
      "architecture_id": "YuanForCausalLM",
      "total_models": 8,
      "sample_models": [
        "IEITYuan/Yuan2-M32-hf",
        "IEITYuan/Yuan2-102B-hf",
        "IEITYuan/Yuan2-2B-Janus-hf",
        "IEITYuan/Yuan2-2B-Februa-hf",
        "IEITYuan/Yuan2-2B-Februa-hf-GGUF",
        "IEITYuan/Yuan2-M32",
        "IEITYuan/Yuan2-M32-hf-int4",
        "IEITYuan/Yuan2-M32-hf-int8"
      ]
    },
    {
      "architecture_id": "modeling_sparsetral.MistralForCausalLM",
      "total_models": 8,
      "sample_models": [
        "serpdotai/sparsetral-16x7B-v2",
        "serpdotai/sparsetral-16x7B-v1",
        "LoneStriker/sparsetral-16x7B-v2-8.0bpw-h8-exl2",
        "uukuguy/speechless-sparsetral-mistral-16x7b-MoE",
        "LoneStriker/sparsetral-16x7B-v2-5.0bpw-h6-exl2",
        "serpdotai/sparsetral-16x7B-v2-SPIN_iter0",
        "LoneStriker/sparsetral-16x7B-v2-4.0bpw-h6-exl2",
        "LoneStriker/sparsetral-16x7B-v2-3.0bpw-h6-exl2"
      ]
    },
    {
      "architecture_id": "Ernie4_5_ForCausalLM",
      "total_models": 8,
      "sample_models": [
        "baidu/ERNIE-4.5-0.3B-Paddle",
        "mlx-community/ERNIE-4.5-0.3B-PT-4bit",
        "lmstudio-community/ERNIE-4.5-0.3B-MLX-8bit",
        "mlx-community/ERNIE-4.5-0.3B-PT-bf16",
        "mlx-community/ERNIE-4.5-0.3B-PT-4bit-ft",
        "ahBeyond/ERNIE-4.5-0.3B-Base-BF16-gguf",
        "FriendliAI/ERNIE-4.5-0.3B-PT",
        "dengcao/ERNIE-4.5-0.3B-Base-Paddle"
      ]
    },
    {
      "architecture_id": "VideoBlipForConditionalGeneration",
      "total_models": 8,
      "sample_models": [
        "kpyu/eilev-blip2-opt-2.7b",
        "kpyu/video-blip-flan-t5-xl-ego4d",
        "turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0",
        "turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1",
        "turing-motors/heron-chat-blip-ja-llm-jp-13b-v0",
        "turing-motors/blip2-st-exp003",
        "turing-motors/blip2-st-abci-exp003",
        "kpyu/eilev-blip2-flan-t5-xl"
      ]
    },
    {
      "architecture_id": "LlavaLlamaAttForCausalLM",
      "total_models": 8,
      "sample_models": [
        "YanweiLi/llama-vid-7b-full-224-video-fps-1",
        "YanweiLi/llama-vid-7b-full-224-long-video",
        "YanweiLi/llama-vid-7b-full-224",
        "YanweiLi/llama-vid-7b-full-336",
        "YanweiLi/llama-vid-13b-full-224-video-fps-1",
        "YanweiLi/llama-vid-13b-full-336",
        "4bit/llama-vid-7b-full-224-long-video-MovieLLM",
        "4bit/llama-vid-7b-full-224-long-video-5GB"
      ]
    },
    {
      "architecture_id": "MegaForCausalLM",
      "total_models": 8,
      "sample_models": [
        "pszemraj/mega-ar-525m-v0.07-ultraTBfw",
        "pszemraj/mega-ar-small-4096-wikitext-103-raw-v1",
        "pszemraj/mega-ar-small-4096-sw_minipile",
        "pszemraj/mega-ar-large-2048-simplewiki",
        "BEE-spoke-data/mega-ar-350m-L3t-v0.08-ultraTBfw",
        "pszemraj/mega-ar-350m-v0.13",
        "hf-tiny-model-private/tiny-random-MegaForCausalLM",
        "RichardErkhov/BEE-spoke-data_-_mega-ar-126m-4k-8bits"
      ]
    },
    {
      "architecture_id": "BVVForCausalLM",
      "total_models": 8,
      "sample_models": [
        "Bochkov/emergent-semantics-model-uni-glyph-335m",
        "Bochkov/emergent-semantics-model-unfrozen-335m",
        "Bochkov/emergent-semantics-model-16-bit-269m",
        "Bochkov/emergent-semantics-model-256-bit-285m",
        "Bochkov/emergent-semantics-model-64-bit-272m",
        "Bochkov/emergent-semantics-model-16-float-269m",
        "Bochkov/emergent-semantics-model-1024-bit-335m",
        "Bochkov/emergent-semantics-model-1024-float-335m"
      ]
    },
    {
      "architecture_id": "ElectraForCausalLM",
      "total_models": 8,
      "sample_models": [
        "Gan1108/electraForCausalLM",
        "smeoni/nbme-electra-large-generator",
        "Star3073/my_awesome_ke_clm-model4",
        "Star3073/my_awesome_ke_clm-model3",
        "Star3073/my_awesome_ke_clm-model5",
        "Star3073/my_awesome_ke_clm-model",
        "Star3073/my_awesome_ke_clm-model2",
        "COCO0414/New-Word-Detect"
      ]
    },
    {
      "architecture_id": "Prot2TextModel",
      "total_models": 8,
      "sample_models": [
        "habdine/Prot2Text-Large-v1-1",
        "habdine/Prot2Text-Small-v1-1",
        "habdine/Esm2Text-Base-v1-0",
        "habdine/Prot2Text-Medium-v1-1",
        "habdine/Prot2Text-Base-v1-0",
        "habdine/Prot2Text-Large-v1-0",
        "habdine/Prot2Text-Small-v1-0",
        "habdine/Prot2Text-Medium-v1-0"
      ]
    },
    {
      "architecture_id": "BlenderbotForCausalLM",
      "total_models": 8,
      "sample_models": [
        "ColleenMacklin/blenderbot-400M-distill-couples_therapist_chat1",
        "hf-tiny-model-private/tiny-random-BlenderbotForCausalLM",
        "cyan-tangrui/blenderbot-400M-distill-Gensyn-Swarm-elusive_leaping_gibbon",
        "Danieldor/Baldor-Assist",
        "Bbrown44/aas-ds-v1",
        "Bbrown44/aas_nlp_v1",
        "Bbrown44/aas-ds-v2",
        "sir-evil/my-chat-model"
      ]
    },
    {
      "architecture_id": "CamembertForCausalLM",
      "total_models": 8,
      "sample_models": [
        "AntoineD/camembert_causal_language_modeling_tools",
        "nico-che/camembert-base-finetuned-labdataset",
        "maelfabien/marcel_customer_service_large",
        "maelfabien/marcel_customer_service_medium",
        "maelfabien/marcel_customer_service_xlarge",
        "zenonal/hugo_camembert",
        "lponsard/my_awesome_eli5_clm-model3",
        "lponsard/my_awesome_eli5_clm-model"
      ]
    },
    {
      "architecture_id": "MoLM",
      "total_models": 8,
      "sample_models": [
        "robinfaro/molm-fineweb-edu-prova5",
        "robinfaro/molm-router-fineweb_edu-100BT",
        "robinfaro/molm_log_prob_router",
        "robinfaro/molm-log_prob-router_advanced",
        "talphaidze/molm-fineweb-edu-scientific1",
        "robinfaro/molm_log_prob_router_trained",
        "robinfaro/molm-fineweb-edu_100BT",
        "robinfaro/molm_inverted_100BT"
      ]
    },
    {
      "architecture_id": "RECAST7b_LlamaForCausalLM",
      "total_models": 8,
      "sample_models": [
        "appledora/recast2-all-G16W32H16R16",
        "appledora/recast2-all-G16W64H32",
        "appledora/recast2-G8W16H4",
        "appledora/recast2-G16W64H32",
        "appledora/recast2-G16W16H8",
        "appledora/recast2-G16W128H64",
        "appledora/recastSDP2-G16W128H64",
        "appledora/recastSDP2-G16W32H16"
      ]
    },
    {
      "architecture_id": "VideoChatGPTLlamaForCausalLM",
      "total_models": 8,
      "sample_models": [
        "heldJan/llama-2-7b-froozen_CLIP_test",
        "heldJan/llama-2-7b-froozen_mvit",
        "heldJan/model_after40epochs",
        "heldJan/llama-2-7b-froozen_clip",
        "heldJan/llama-2-7b-froozen_CLIP_test_Train_only_projection",
        "heldJan/llama-2-7b-froozen_mvit_test",
        "heldJan/llama-2-7b_TEST",
        "heldJan/constant_50_epochs"
      ]
    },
    {
      "architecture_id": "RWKV",
      "total_models": 8,
      "sample_models": [
        "avey-ai/rwkv7-dpa-0.5B-95BT",
        "avey-ai/rwkv7-dpa-0.5B-90BT",
        "avey-ai/rwkv7-dpa-0.1B-95BT",
        "avey-ai/rwkv7-dpa-0.1B-90BT",
        "avey-ai/rwkv7-dpa-0.5B-100BT",
        "avey-ai/rwkv7-dpa-1.5B-100BT",
        "avey-ai/rwkv7-dpa-0.1B-100BT",
        "avey-ai/rwkv7-dpa-1.5B-90BT"
      ]
    },
    {
      "architecture_id": "RingAttentionGPT2LMHeadModel",
      "total_models": 8,
      "sample_models": [
        "Progz/CyclicGPT2-1.1.6",
        "Progz/CyclicGPT2-1.2.10",
        "Progz/CyclicGPT2-1.1.5",
        "Progz/CyclicGPT2-1.2.2",
        "Progz/CyclicGPT2-1.2.4",
        "Progz/CyclicGPT2-1.2.11",
        "Progz/CyclicGPT2-1.2.5.1",
        "Progz/CyclicGPT2-1.1.3"
      ]
    },
    {
      "architecture_id": "StickbreakingForCausalLM",
      "total_models": 8,
      "sample_models": [
        "Lanni-ni/stickbreaking_4_6_384_",
        "Lanni-ni/stickbreaking_4_6_384_pile",
        "Lanni-ni/stickbreaking_2L_4H_256D_babylm",
        "Lanni-ni/stickbreaking_babylm_100m_4layer",
        "Lanni-ni/stickbreaking_pile_2layer",
        "Lanni-ni/stickbreaking_pile_4layer",
        "Lanni-ni/stickbreaking_babylm_10m_2_4_256",
        "Lanni-ni/stickbreaking_babylm_10m_4layer"
      ]
    },
    {
      "architecture_id": "HfMoondream",
      "total_models": 7,
      "sample_models": [
        "vikhyatk/moondream2",
        "moondream/moondream3-preview",
        "alecccdd/moondream3-preview-4bit",
        "PierrunoYT/moondream3-preview",
        "vikhyatk/moondream-next",
        "EQX55/MD3",
        "nkasmanoff/moondream_finetune_nk_hf"
      ]
    },
    {
      "architecture_id": "HyenaDNAForCausalLM",
      "total_models": 7,
      "sample_models": [
        "LongSafari/hyenadna-large-1m-seqlen-hf",
        "LongSafari/hyenadna-small-32k-seqlen-hf",
        "LongSafari/hyenadna-tiny-1k-seqlen-hf",
        "LongSafari/hyenadna-medium-160k-seqlen-hf",
        "LongSafari/hyenadna-tiny-16k-seqlen-d128-hf",
        "LongSafari/hyenadna-tiny-1k-seqlen-d256-hf",
        "loieeyet/hyenaDNA_160K"
      ]
    },
    {
      "architecture_id": "Zamba2ForCausalLM",
      "total_models": 7,
      "sample_models": [
        "Zyphra/Zamba2-1.2B-instruct",
        "Zyphra/Zamba2-7B-Instruct",
        "Zyphra/Zamba2-2.7B",
        "ssmits/Zamba2-1.2B-instruct-Dutch",
        "Zyphra/Zamba2-7B-Instruct-v2",
        "Zyphra/Zamba2-2.7B-Instruct-v2",
        "Zyphra/Zamba2-1.2B-Instruct-v2"
      ]
    },
    {
      "architecture_id": "Plamo2ForCausalLM",
      "total_models": 7,
      "sample_models": [
        "pfnet/plamo-2-1b",
        "pfnet/plamo-2-translate",
        "pfnet/plamo-2.1-8b-cpt",
        "pfnet/plamo-2-translate-base",
        "pfnet/plamo-2-8b",
        "pfnet/plamo-2-translate-eval",
        "mlx-community/plamo-2-translate-bf16"
      ]
    },
    {
      "architecture_id": "Eagle3Speculator",
      "total_models": 7,
      "sample_models": [
        "RedHatAI/Qwen3-8B-speculator.eagle3",
        "RedHatAI/gpt-oss-20b-speculator.eagle3",
        "RedHatAI/Llama-3.1-8B-Instruct-speculator.eagle3",
        "RedHatAI/Llama-3.3-70B-Instruct-speculator.eagle3",
        "RedHatAI/Qwen3-32B-speculator.eagle3",
        "RedHatAI/Qwen3-14B-speculator.eagle3",
        "nm-testing/Llama-3.1-8B-Instruct-speculator.eagle3-converted"
      ]
    },
    {
      "architecture_id": "MiMoV2FlashForCausalLM",
      "total_models": 7,
      "sample_models": [
        "XiaomiMiMo/MiMo-V2-Flash",
        "cyankiwi/MiMo-V2-Flash-AWQ-4bit",
        "mlx-community/MiMo-V2-Flash-4bit",
        "XiaomiMiMo/MiMo-V2-Flash-Base",
        "inferencerlabs/MiMo-V2-Flash-MLX-6.5bit",
        "mlx-community/MiMo-V2-Flash-mlx-8bit-gs32",
        "unsloth/MiMo-V2-Flash"
      ]
    },
    {
      "architecture_id": "OuroForCausalLM",
      "total_models": 7,
      "sample_models": [
        "ByteDance/Ouro-1.4B",
        "ByteDance/Ouro-1.4B-Thinking",
        "ByteDance/Ouro-2.6B",
        "ByteDance/Ouro-2.6B-Thinking",
        "mlx-community/Ouro-2.6B-4bit",
        "mlx-community/Ouro-2.6B-Thinking-4bit",
        "mlx-community/Ouro-1.4B-Thinking-4bit"
      ]
    },
    {
      "architecture_id": "Dots1ForCausalLM",
      "total_models": 7,
      "sample_models": [
        "rednote-hilab/dots.llm1.inst",
        "rednote-hilab/dots.llm1.base",
        "rednote-hilab/dots.llm1.inst-FP8-dynamic",
        "mlx-community/dots.llm1.inst-mixed-4-6bit",
        "m-i/dots.llm1.inst-mixed-4-5bit",
        "m-i/dots.llm1.inst-mixed-5-6bit",
        "backups/dots.llm1.base"
      ]
    },
    {
      "architecture_id": "Emu3ForCausalLM",
      "total_models": 7,
      "sample_models": [
        "BAAI/Emu3-Gen",
        "BAAI/Emu3.5",
        "BAAI/Emu3-Stage1",
        "BAAI/Emu3.5-Image",
        "lodrick-the-lafted/Emu3-Gen-12B",
        "kazemnejad/Emu3-Base-SFT-got-Apr13_post_stochastic_256_lr1e-5-checkpoint-10800",
        "kazemnejad/reasoning_verbose-May1_got__256_1e-4_weightdecay0-stochastic-1.0-checkpoint-9600"
      ]
    },
    {
      "architecture_id": "GLAForCausalLM",
      "total_models": 7,
      "sample_models": [
        "fla-hub/gla-340M-15B",
        "fla-hub/gla-1.3B-100B",
        "fla-hub/gla-2.7B-100B",
        "fla-hub/gla-7B-mistral-20B",
        "zaydzuhri/gla-16M-test",
        "bailin28/gla-340m-15B",
        "PatrickHaller/gla-350M-10B"
      ]
    },
    {
      "architecture_id": "FlexOlmoForCausalLM",
      "total_models": 7,
      "sample_models": [
        "shanearora/Flex-reddit-2x7B-1T",
        "allenai/FlexOlmo-7x7B-1T",
        "allenai/Flex-math-2x7B-1T",
        "allenai/Flex-code-2x7B-1T",
        "allenai/Flex-news-2x7B-1T",
        "allenai/Flex-pes2o-2x7B-1T",
        "allenai/FlexOlmo-7x7B-1T-RT"
      ]
    },
    {
      "architecture_id": "BunnyLlamaForCausalLM",
      "total_models": 7,
      "sample_models": [
        "BAAI/Bunny-Llama-3-8B-V",
        "BAAI/Bunny-v1_1-Llama-3-8B-V",
        "scb10x/llama-3-typhoon-v1.5-8b-vision-preview",
        "VideoGameBunny/VideoGameBunny-V1",
        "mlx-community/Bunny-Llama-3-8B-V-4bit",
        "aisak-ai/aisak-tvi",
        "AIPeanutman/Bunny-MMR-8B"
      ]
    },
    {
      "architecture_id": "LlavaMPTForCausalLM",
      "total_models": 7,
      "sample_models": [
        "liuhaotian/LLaVA-Lightning-MPT-7B-preview",
        "advaitadasein/LLaVA-Lightning-MPT-7B-preview",
        "kaelee/llava-lightning-mpt-7b-chat-pretrain",
        "kaelee/llava-lightning-mpt-7b-instruct-pretrain",
        "kaelee/llava-lightning-mpt-7b-instruct-finetuning",
        "Hemachandiran/llava_Openvino_INT4",
        "shivani05/llava7B_openvino_int4"
      ]
    },
    {
      "architecture_id": "TrainableM2MForConditionalGeneration",
      "total_models": 7,
      "sample_models": [
        "Sunbird/translate-nllb-1.3b-salt",
        "Sunbird/translate-nllb-3.3b-salt",
        "jq/nllb-1.3B-asr-correction",
        "jq/nllb-1.3B-asr-summarisation",
        "EzekielMW/Eksl_dataset",
        "jq/nllb-3.3B-salt",
        "kitaka/nllb-3.3b-salt-lr2e-4-audersity"
      ]
    },
    {
      "architecture_id": "TFGPT2LMHeadModel",
      "total_models": 7,
      "sample_models": [
        "mymusise/gpt2-medium-chinese",
        "Ochiroo/tiny_mn_gpt",
        "mymusise/EasternFantasyNoval",
        "mymusise/EasternFantasyNoval-small",
        "mymusise/AIXX",
        "mymusise/CPM-GPT2",
        "Ericahooooo/testmodel"
      ]
    },
    {
      "architecture_id": "BartEncodecForConditionalGeneration",
      "total_models": 7,
      "sample_models": [
        "tsuyuan/speech-chatgpt-base-nar-se_gptspeech_amazon_google_tencent",
        "kuanhuggingface/speech-chatgpt-t5-base-encoder-bart-base-ar",
        "tsuyuan/speech-chatgpt-base-nar-se-sk-encodec",
        "tsuyuan/speech-chatgpt-base-nar-se_full",
        "kuanhuggingface/speech-chatgpt-base-nar-se-speech-tokenizer-promptTTS",
        "zion84006/encodec-bart-asr",
        "kuanhuggingface/speech-chatgpt-base-encodec2instruction-mix"
      ]
    },
    {
      "architecture_id": "BabyLlamaForCausalLM",
      "total_models": 7,
      "sample_models": [
        "AnnasBlackHat/babyLlama",
        "saurabhg2083/babyLlama",
        "prince-canuma/babyLlama",
        "AnnasBlackHat/BabyLlama-1",
        "pawelzm/babyLlama",
        "wronqa/babyLlama",
        "shamith/babyLlama-TinyStories"
      ]
    },
    {
      "architecture_id": "GPTPanguForCausalLM",
      "total_models": 7,
      "sample_models": [
        "imone/pangu_2_6B",
        "sunzeyeah/pangu-350M-sft",
        "Hanlard/Pangu_alpha",
        "sunzeyeah/pangu-350M-reward",
        "sunzeyeah/pangu-2_6B-sft",
        "superqing/pangu-evolution",
        "sunzeyeah/pangu-13B"
      ]
    },
    {
      "architecture_id": "Qwen3Model",
      "total_models": 7,
      "sample_models": [
        "abheekga/Qwen3-8B-bnb-4bit",
        "envsetup-rl-dl4c/Qwen3-8B-am",
        "RoadToNowhere/Qwen3-0.6B-ao-float8wo",
        "cs2764/Huihui-Qwen3-4B-Instruct-2507-abliterated-bnb-4bit",
        "andysalerno/Qwen3-8B-ao-autoquant",
        "cs2764/Huihui-Qwen3-4B-Instruct-2507-abliterated-ao-int8wo",
        "DreamerRimmer/Huihui-Qwen3-4B-Instruct-2507-abliterated-ao-int4wo"
      ]
    },
    {
      "architecture_id": "PolyverseForConditionalGeneration",
      "total_models": 7,
      "sample_models": [
        "utischoolnlp/Polyverse-587M-256-16-stage1",
        "utischoolnlp/Polyverse-700M-256-16-stage2",
        "utischoolnlp/Polyverse-700M-256-16-stage1",
        "utischoolnlp/Polyverse-1.8B-384-14-random",
        "utischoolnlp/Polyverse-1.3B-256-16-stage1",
        "utischoolnlp/Polyverse-1.3B-256-16-random",
        "utischoolnlp/Polyverse-700M-256-16-random"
      ]
    },
    {
      "architecture_id": "LongformerForSequenceClassification",
      "total_models": 7,
      "sample_models": [
        "open-paws/emotional_impact_prediction_longform",
        "open-paws/animal_advocate_preference_prediction_longform",
        "open-paws/text_performance_prediction_longform",
        "open-paws/level_of_rationality_prediction_longform",
        "open-paws/perceived_insightfulness_prediction_longform",
        "open-paws/potential_influence_prediction_longform",
        "open-paws/perceived_trustworthiness_prediction_longform"
      ]
    },
    {
      "architecture_id": "SDARMoeForCausalLM",
      "total_models": 7,
      "sample_models": [
        "JetLM/SDAR-30B-A3B-Chat",
        "JetLM/SDAR-30B-A3B-Sci",
        "JetLM/SDAR-30B-A3B-Chat-b16",
        "JetLM/SDAR-30B-A3B-Sci-Base",
        "JetLM/SDAR-30B-A3B-Chat-b64",
        "JetLM/SDAR-30B-A3B-Chat-b32",
        "JetLM/SDAR-30B-A3B-Chat-b8"
      ]
    },
    {
      "architecture_id": "LlavaNextForConditionalGeneration",
      "total_models": 7,
      "sample_models": [
        "swap-uniba/LLaVA-NDiNO_long",
        "swap-uniba/LLaVA-NDiNO_pt_short_it",
        "swap-uniba/LLaVA-NDiNO_pt_long",
        "CowCorpus/llama3-llava-next-8b-cowcorpus",
        "swap-uniba/LLaVA-NDiNO_short_it",
        "swap-uniba/LLaVA-NDiNO_short_long",
        "swap-uniba/LLaVA-NDiNO_pt_short_long"
      ]
    },
    {
      "architecture_id": "VRT_Qwen2VLForConditionalGeneration",
      "total_models": 7,
      "sample_models": [
        "rp-yu/Qwen2-VL-7b-VPT-CLIP",
        "rp-yu/Qwen2-VL-2b-VPT-Det-Alignment",
        "rp-yu/Qwen2-VL-2b-VPT-Seg",
        "rp-yu/Qwen2-VL-2b-VPT-CLIP",
        "rp-yu/Qwen2-VL-2b-VPT-Det-NoPrompt",
        "rp-yu/Qwen2-VL-2b-VPT-Seg-Alignment",
        "rp-yu/Qwen2-VL-2b-VPT-Det"
      ]
    },
    {
      "architecture_id": "ACIPModel",
      "total_models": 7,
      "sample_models": [
        "MerantixMomentum/acip_llama2_7b",
        "MerantixMomentum/acip_llama1_7b",
        "MerantixMomentum/acip_qwen25_7b",
        "MerantixMomentum/acip_qwen25_14b",
        "MerantixMomentum/acip_llama1_13b",
        "MerantixMomentum/acip_llama31_8b",
        "MerantixMomentum/acip_llama2_13b"
      ]
    },
    {
      "architecture_id": "OlmoModelForCausalLM",
      "total_models": 7,
      "sample_models": [
        "saurabh-shah/olmo-1B-safetensors",
        "allenai/paloma-1b-baseline-mc4",
        "allenai/paloma-1b-baseline-pile",
        "allenai/paloma-1b-baseline-c4",
        "allenai/paloma-1b-baseline-redpajama",
        "allenai/paloma-1b-baseline-dolma",
        "allenai/paloma-1b-baseline-falcon-refinedweb"
      ]
    },
    {
      "architecture_id": "MyLlamaForCausalLM",
      "total_models": 7,
      "sample_models": [
        "hzy00/RoPE-151M",
        "Youliang/llama3-8b-instruct-derta-100step",
        "hzy00/BiPE_RoPE-151M",
        "hzy00/ALiBi-151M",
        "hzy00/BiPE_ALiBi-151M",
        "sh-haruta/llama3.1_prune_post_test",
        "hzy00/BiPE_RoPE-1.6B"
      ]
    },
    {
      "architecture_id": "Qwen2BMForCausalLM",
      "total_models": 7,
      "sample_models": [
        "hdong0/deepseek-Qwen2.5-Math-1.5B-baseline-init",
        "hdong0/Qwen2.5-Math-1.5B-baseline-Open-R1-GRPO_deepscaler_mu_8_constant_lr",
        "hdong0/deepseek-Qwen-1.5B-baseline-thin-Open-R1-GRPO_deepscaler_mu_8_constant_lr_warmed",
        "hdong0/deepseek-Qwen2.5-7B-baseline-thin-init",
        "hdong0/deepseek-Qwen2.5-7B-baseline-thin-Open-R1-GRPO_deepscaler_acc_mu_8_constant_lr",
        "hdong0/deepseek-Qwen2.5-1.5B-baseline-thin-Open-R1-GRPO_deepscaler_mu_8_constant_lr",
        "hdong0/Qwen2.5-Math-1.5B-baseline-init"
      ]
    },
    {
      "architecture_id": "Videollama3Qwen2ForCausalLM",
      "total_models": 6,
      "sample_models": [
        "DAMO-NLP-SG/VideoLLaMA3-7B",
        "DAMO-NLP-SG/VideoLLaMA3-2B",
        "DAMO-NLP-SG/VideoRefer-VideoLLaMA3-7B",
        "DAMO-NLP-SG/VideoRefer-VideoLLaMA3-2B",
        "Fiaa/videollama3-s-t-a-g-e-5",
        "cbipok/VideoLLaMA3-2B-fork"
      ]
    },
    {
      "architecture_id": "LibraLlamaForCausalLM",
      "total_models": 6,
      "sample_models": [
        "X-iZhang/libra-v1.0-7b",
        "X-iZhang/libra-v1.0-3b",
        "X-iZhang/libra-llava-rad",
        "X-iZhang/Med-CXRGen-F",
        "X-iZhang/Med-CXRGen-I",
        "X-iZhang/libra-maira-2"
      ]
    },
    {
      "architecture_id": "CLIPT5ForConditionalGeneration",
      "total_models": 6,
      "sample_models": [
        "zhiqiulin/clip-flant5-xxl",
        "zhiqiulin/clip-flant5-xl",
        "Nano1337/clip-flant5-xxl-10p",
        "zhiqiulin/clip-flant5-xxl-no-split-text",
        "zhiqiulin/clip-flant5-xl-gpt4v-100k",
        "zhiqiulin/clip-flant5-xxl-with-stage1-captions"
      ]
    },
    {
      "architecture_id": "Share4VLlamaForCausalLM",
      "total_models": 6,
      "sample_models": [
        "Lin-Chen/ShareGPT4V-7B",
        "Lin-Chen/ShareGPT4V-13B",
        "caishihao/GeoGPT4V-ShareGPT4V-13B-v1",
        "Zery/MV-LLaVA-7B",
        "Lin-Chen/ShareGPT4V-13B_Pretrained_vit-large336-l12_vicuna-13b-v1.5",
        "Lin-Chen/ShareGPT4V-7B_Pretrained_vit-large336-l12_vicuna-7b-v1.5"
      ]
    },
    {
      "architecture_id": "BD3LM",
      "total_models": 6,
      "sample_models": [
        "kuleshov-group/bd3lm-owt-block_size4",
        "kuleshov-group/bd3lm-owt-block_size16",
        "kuleshov-group/bd3lm-owt-block_size8",
        "kuleshov-group/mdlm-owt-noeos",
        "kuleshov-group/sedd-noeos-owt",
        "kuleshov-group/ar-noeos-owt"
      ]
    },
    {
      "architecture_id": "MoSMambaForCausalLM",
      "total_models": 6,
      "sample_models": [
        "jonathanjordan21/mos-mamba-18x130m-trainer-dgx-pile",
        "jonathanjordan21/mos-mamba-6x130m-trainer-sft",
        "jonathanjordan21/mos-mamba-6x130m-train",
        "jonathanjordan21/mos-mamba-18x130m-trainer-dgx-lora-sft-merged",
        "jonathanjordan21/mixba",
        "jonathanjordan21/mos-mamba-18x130m-trainer-dgx"
      ]
    },
    {
      "architecture_id": "SpatialLMQwenForCausalLM",
      "total_models": 6,
      "sample_models": [
        "manycore-research/SpatialLM1.1-Qwen-0.5B",
        "manycore-research/SpatialLM-Qwen-0.5B",
        "ysmao/SpatialLM1.1-Qwen-0.5B-ARKitScenes-SFT",
        "ysmao/SpatialLM1.1-Qwen-0.5B-Structured3D-SFT",
        "ysmao/SpatialLM1.1-Qwen-0.5B-ScanNet-SFT",
        "kp-forks/SpatialLM-Qwen-0.5B"
      ]
    },
    {
      "architecture_id": "FP8Qwen2ForCausalLM",
      "total_models": 6,
      "sample_models": [
        "xihc-ucb/Qwen2.5-3B-train-Quasar-1002",
        "xihc-ucb/Qwen2.5-7B-train-Quasar-1002",
        "xihc-ucb/Qwen2.5-3B-train-Quasar-1214",
        "xihc-ucb/Qwen2.5-7B-train-Quasar-1214",
        "xihc-ucb/Qwen2.5-7B-Instruct-train-Quasar-1214",
        "xihc-ucb/Qwen2.5-7B-Instruct-train-Quasar-1002"
      ]
    },
    {
      "architecture_id": "MotifForCausalLM",
      "total_models": 6,
      "sample_models": [
        "Motif-Technologies/Motif-2-12.7B-Reasoning",
        "Motif-Technologies/Motif-2.6B",
        "Motif-Technologies/Motif-2-12.7B-Base",
        "Motif-Technologies/Motif-2.6b-v1.1-LC",
        "OpenLLM-Korea/Motif-2.6b-v1.1-LC",
        "OpenLLM-Korea/Motif-2.6B"
      ]
    },
    {
      "architecture_id": "MobileLLMP1ForCausalLM",
      "total_models": 6,
      "sample_models": [
        "facebook/MobileLLM-Pro",
        "facebook/MobileLLM-Pro-base-int4-cpu",
        "camenduru/MobileLLM-Pro",
        "Rohit-Katkar2003/mobilellm-pro-fine-tunned",
        "facebook/MobileLLM-Pro-base",
        "facebook/MobileLLM-Pro-base-int4-accelerator"
      ]
    },
    {
      "architecture_id": "BunnyPhiForCausalLM",
      "total_models": 6,
      "sample_models": [
        "BAAI/Bunny-v1_0-3B",
        "opencsg/opencsg-bunny-v0.1-3B",
        "phronetic-ai/owlet-phi-2",
        "RichardErkhov/BAAI_-_Bunny-v1_0-3B-4bits",
        "AIPeanutman/Bunny-MMR-3B",
        "RichardErkhov/BAAI_-_Bunny-v1_0-3B-8bits"
      ]
    },
    {
      "architecture_id": "NanoChatForCausalLM",
      "total_models": 6,
      "sample_models": [
        "onnx-community/nanochat-d32-ONNX",
        "pankajmathur/nanochat-d34-sft-hf",
        "HarleyCooper/nanochat561",
        "pankajmathur/nanochat-d34-rl-hf",
        "dnakov/nanochat-d20-mlx",
        "dnakov/nanochat-d20"
      ]
    },
    {
      "architecture_id": "HelpingAIForCausalLM",
      "total_models": 6,
      "sample_models": [
        "OEvortex/HelpingAI-3B-v2.2",
        "OEvortex/HelpingAI-3B-v2.1",
        "OEvortex/HelpingAI-180B-base",
        "Klevin/DECYPHERS-AI-3B-v2.2",
        "HelpingAI/hai3.1-checkpoint-0001",
        "Klevin/PRIME-3B-v2.2"
      ]
    },
    {
      "architecture_id": "BlueLMForCausalLM",
      "total_models": 6,
      "sample_models": [
        "vivo-ai/BlueLM-7B-Base",
        "vivo-ai/BlueLM-7B-Base-32K",
        "vivo-ai/BlueLM-7B-Chat",
        "vivo-ai/BlueLM-7B-Chat-4bits",
        "vivo-ai/BlueLM-7B-Chat-32K-AWQ",
        "vivo-ai/BlueLM-7B-Chat-32K-GPTQ"
      ]
    },
    {
      "architecture_id": "CogAgentForCausalLM",
      "total_models": 6,
      "sample_models": [
        "zai-org/cogagent-chat-hf",
        "zai-org/cogagent-vqa-hf",
        "yeelou/design2code-hf",
        "yeelou/design2code-hf-bit4",
        "yeelou/design2code-hf-bit8",
        "Thouph/cogagent-captioner-hf"
      ]
    },
    {
      "architecture_id": "InstellaForCausalLM",
      "total_models": 6,
      "sample_models": [
        "amd/Instella-3B",
        "amd/Instella-3B-Stage1",
        "hereticness/Heretic-Instella-3B-Long-Instruct",
        "amd/Instella-3B-Long-Instruct",
        "amd/Instella-3B-Math",
        "amd/Instella-3B-Math-SFT"
      ]
    },
    {
      "architecture_id": "KimiVLForConditionalGeneration",
      "total_models": 6,
      "sample_models": [
        "NexVeridian/Kimi-VL-A3B-Thinking-2506-4bit",
        "NexVeridian/Kimi-VL-A3B-Thinking-2506-6bit",
        "imagick/Kimi-VL-A3B-Thinking-2506-mlx",
        "NexVeridian/Kimi-VL-A3B-Thinking-2506-3bit",
        "NexVeridian/Kimi-VL-A3B-Thinking-2506-8bit",
        "NexVeridian/Kimi-VL-A3B-Thinking-2506-5bit"
      ]
    },
    {
      "architecture_id": "MistralModel",
      "total_models": 6,
      "sample_models": [
        "Severian/Nexus-IKM-Mistral-7B-v5-instruction",
        "max-2022/test_mistral2",
        "QueryloopAI/AlphaMonarch-dora",
        "theBodhiTree/Zephyr-Hermes-7B",
        "QueryloopAI/AlphaMonarch-laser",
        "Vision-CAIR/BFPO-redteaming-Zephyr-7b-beta"
      ]
    },
    {
      "architecture_id": "RwkvHybridForCausalLM",
      "total_models": 6,
      "sample_models": [
        "RWKV-Red-Team/ARWKV-R1-1B5",
        "RWKV-Red-Team/ARWKV-7B-Preview-0.1",
        "RWKV-Red-Team/ARWKV-R1-7B",
        "yueyulin/arwkv-qwen-r1-1b5",
        "RWKV-Red-Team/ARWKV-7B-Preview-0.1-NoG-32B",
        "RWKV-Red-Team/ARWKV-7B-Preview-0.1-NoG"
      ]
    },
    {
      "architecture_id": "TransnormerForCausalLM",
      "total_models": 6,
      "sample_models": [
        "TheBloke/TransNormerLLM-7B-GPTQ",
        "OpenNLPLab/TransNormerLLM2-1B-300B",
        "OpenNLPLab/TransNormerLLM-1B",
        "OpenNLPLab/TransNormerLLM-7B",
        "OpenNLPLab/TransNormerLLM2-7B-300B",
        "OpenNLPLab/TransNormerLLM2-3B-300B"
      ]
    },
    {
      "architecture_id": "Speech2TextTransformerForConditionalGeneration",
      "total_models": 6,
      "sample_models": [
        "valhalla/s2t_mustc_multilinguial_medium",
        "valhalla/s2t_librispeech_medium",
        "valhalla/s2t_covost2_en_de_small",
        "valhalla/s2t_librispeech_large",
        "valhalla/s2t_librispeech_small",
        "valhalla/s2t_mustc_en_fr_small"
      ]
    },
    {
      "architecture_id": "LongLlamaForCausalLM",
      "total_models": 6,
      "sample_models": [
        "syzymon/long_llama_code_7b",
        "syzymon/long_llama_3b",
        "syzymon/long_llama_3b_instruct",
        "syzymon/long_llama_code_7b_instruct",
        "syzymon/long_llama_3b_v1_1",
        "peterbeamish/long_llama_7b_env"
      ]
    },
    {
      "architecture_id": "GSAForCausalLM",
      "total_models": 6,
      "sample_models": [
        "fla-hub/gsa-1.3B-100B",
        "fla-hub/gsa-2.7B-100B",
        "zaydzuhri/gsa-8192-16M-test",
        "fla-hub/gsa-7B-mistral-100B",
        "zaydzuhri/gsa-16M-test",
        "fla-hub/gsa-7B-mistral-20B"
      ]
    },
    {
      "architecture_id": "PaDTForConditionalGeneration",
      "total_models": 6,
      "sample_models": [
        "PaDT-MLLM/PaDT_Pro_7B",
        "PaDT-MLLM/PaDT_REC_3B",
        "PaDT-MLLM/PaDT_RIC_7B",
        "PaDT-MLLM/PaDT_OVD_7B",
        "PaDT-MLLM/PaDT_RIC_3B",
        "PaDT-MLLM/PaDT_REC_7B"
      ]
    },
    {
      "architecture_id": "AnemoneForCausalLM",
      "total_models": 6,
      "sample_models": [
        "Ostixe360/MoMv5-bf16",
        "Ostixe360/MoMv3-bf16",
        "Ostixe360/Moah-MoE-1.58b-1B",
        "Ostixe360/MoM-1.58bits-1B",
        "Ostixe360/MoMv4-bf16",
        "Ostixe360/MoMv4-1.58bits"
      ]
    },
    {
      "architecture_id": "PegasusForCausalLM",
      "total_models": 6,
      "sample_models": [
        "tobijen/my_awesome_eli5_clm-model",
        "tobijen/pegasus_left_heading",
        "WilliamStar/my_awesome_eli5_clm-model",
        "hf-tiny-model-private/tiny-random-PegasusForCausalLM",
        "WilliamStar/eli5_clm-model",
        "saadr1231/legal-pegasus-summarization"
      ]
    },
    {
      "architecture_id": "LlavaMambaForCausalLM",
      "total_models": 6,
      "sample_models": [
        "lakelee/video-ma2mba-0.7b-clip",
        "lakelee/video-ma2mba-0.7b",
        "lakelee/video-ma2mba-1.8b-clip",
        "lakelee/video-ma2mba-3.1b",
        "lakelee/video-ma2mba-3.1b-clip",
        "lakelee/video-ma2mba-1.8b"
      ]
    },
    {
      "architecture_id": "MiniMindLM",
      "total_models": 6,
      "sample_models": [
        "cmz1024/minimind-zero",
        "ShengHongHaung/miniUSE-tw-v1-0.1b-Pretrain",
        "ShengHongHaung/miniUSE-tw-v1-0.1B-1024-Instruct",
        "sam2ai/minimind_odia_llm",
        "ShengHongHaung/miniUSE-tw-v1-0.1B-512-Instruct",
        "chenmingio/minimind-copy"
      ]
    },
    {
      "architecture_id": "LoRDCoderForCausalLM",
      "total_models": 6,
      "sample_models": [
        "nolanoAI/lordcoder-v0-14-5B",
        "nolanoAI/lordcoder-v0-12-3B",
        "nolanoAI/lordcoder-v0-12-6B",
        "nolanoAI/lordcoder-v0-13-8B",
        "nolanoAI/lordcoder-v0-13-2B",
        "nolanoAI/lordcoder-v0-14-9B"
      ]
    },
    {
      "architecture_id": "MonetForCausalLM",
      "total_models": 6,
      "sample_models": [
        "MonetLLM/monet-vd-4.1B-100BT-hf",
        "MonetLLM/monet-hd-1.4B-100BT-hf",
        "MonetLLM/codemonet-vd-1.4B-100BT-hf",
        "MonetLLM/monet-vd-1.4B-100BT-hf",
        "MonetLLM/monet-hd-4.1B-100BT-hf",
        "MonetLLM/monet-hd-850M-100BT-hf"
      ]
    },
    {
      "architecture_id": "Transformer",
      "total_models": 6,
      "sample_models": [
        "zju-zhouhao/minimind-v1",
        "tclh123/minimind-v1-small",
        "ewdlop/shakespeare-transformer",
        "studyinglover/IntelliKernel-0.03b-pretrained",
        "coffeecat304/minimind-v1",
        "studyinglover/IntelliKernel-0.03b-sft"
      ]
    },
    {
      "architecture_id": "ASVDLlamaForCausalLM",
      "total_models": 6,
      "sample_models": [
        "hahnyuan/Llama-2-13b-hf-asvd90",
        "hahnyuan/Llama-2-7b-hf-asvd85",
        "hahnyuan/Llama-2-13b-hf-asvd85",
        "hahnyuan/Llama-2-13b-hf-asvd95",
        "hahnyuan/Llama-2-7b-hf-asvd90",
        "hahnyuan/Llama-2-7b-hf-asvd95"
      ]
    },
    {
      "architecture_id": "DetikzifyCambrianForConditionalGeneration",
      "total_models": 6,
      "sample_models": [
        "tok2000/detikzify-cambrian-concat-1B-clip_siglip_dino_convnext-XXL_trained",
        "tok2000/detikzify-cambrian-concat-1B-clip_siglip_dino_trained",
        "tok2000/detikzify-cambrian-concat-1B-clip_siglip_dino",
        "tok2000/detikzify-cambrian-concat-1B-siglip_dino_trained",
        "tok2000/detikzify-cambrian-concat-1B-siglip_dino",
        "tok2000/detikzify-cambrian-concat-1B-clip_siglip_dino_convnext-XXL"
      ]
    },
    {
      "architecture_id": "SorobanModel",
      "total_models": 6,
      "sample_models": [
        "FastAccounting/soroban_1B_instruct_30_ckp",
        "FastAccounting/soroban_1B_instruct_120_ckp",
        "FastAccounting/soroban_1B_instruct_90_ckp",
        "FastAccounting/soroban_3.8B_instruct_fp32",
        "FastAccounting/soroban_1B_instruct_105_ckp",
        "FastAccounting/soroban_3.8B_instruct_bf16_2ep"
      ]
    },
    {
      "architecture_id": "Idefics3ForConditionalGeneration",
      "total_models": 5,
      "sample_models": [
        "ibm-granite/granite-docling-258M",
        "pbebbo/granite-docling-258m-fixed",
        "Tj/SmolVLM_Proxy",
        "ScottMacdonellDC/granite-docling-258M",
        "DeepMount00/Smol-OCR-preview"
      ]
    },
    {
      "architecture_id": "Ovis2_5",
      "total_models": 5,
      "sample_models": [
        "AIDC-AI/Ovis2.5-2B",
        "AIDC-AI/Ovis2.5-9B",
        "wsbagnsv1/Ovis2.5-9B-sinq-4bit-experimental",
        "wsbagnsv1/Ovis2.5-2B-sinq-4bit-experimental",
        "ViFortune-AI/VOVis2.5-2B-pt"
      ]
    },
    {
      "architecture_id": "HunyuanImage3ForCausalMM",
      "total_models": 5,
      "sample_models": [
        "tencent/HunyuanImage-3.0",
        "Disty0/HunyuanImage3-SDNQ-uint4-svd-r32",
        "Runware/hunyuan-image",
        "wikeeyang/Hunyuan-Image-30-Qint4",
        "chaitnya26/HunyuanImage-3.0-fork"
      ]
    },
    {
      "architecture_id": "LightOnOCRForConditionalGeneration",
      "total_models": 5,
      "sample_models": [
        "lightonai/LightOnOCR-1B-1025",
        "mlx-community/LightOnOCR-1B-1025-6bit",
        "mlx-community/LightOnOCR-1B-1025-8bit",
        "davanstrien/lightonocr-books-test",
        "mlx-community/LightOnOCR-1B-1025-5bit"
      ]
    },
    {
      "architecture_id": "MiniMaxM1ForCausalLM",
      "total_models": 5,
      "sample_models": [
        "MiniMaxAI/MiniMax-M1-40k",
        "yujiepan/minimax-m1-tiny-random",
        "justinjja/MiniMax-M1-80k-W4A16-INT4",
        "tiny-random/minimax-m1",
        "FriendliAI/MiniMax-M1-80k"
      ]
    },
    {
      "architecture_id": "SolarForCausalLM",
      "total_models": 5,
      "sample_models": [
        "upstage/solar-pro-preview-instruct",
        "solnoman/solarpro",
        "royleibov/solar-pro-preview-instruct-ZipNN-Compressed",
        "OpenLLM-Korea/solar-pro-preview-instruct",
        "upstage/solar-pro-preview-pretrained"
      ]
    },
    {
      "architecture_id": "InternLMXComposer2ForCausalLM",
      "total_models": 5,
      "sample_models": [
        "internlm/internlm-xcomposer2-7b",
        "internlm/internlm-xcomposer2-7b-4bit",
        "HexPlex0xFF/internlm-xcomposer2-7b",
        "stanrom/internlm-xcomposer2-7b-4bit",
        "stanrom/internlm-xcomposer2-7b"
      ]
    },
    {
      "architecture_id": "IsaacForConditionalGeneration",
      "total_models": 5,
      "sample_models": [
        "PerceptronAI/Isaac-0.1",
        "OscarGD6/Isaac-0.1",
        "merve/Isaac-0.1",
        "solnoman/PerceptronAI",
        "GoodiesHere/Isaac-0.1-Is-Uncensored"
      ]
    },
    {
      "architecture_id": "WeDLMForCausalLM",
      "total_models": 5,
      "sample_models": [
        "tencent/WeDLM-8B-Instruct",
        "tencent/WeDLM-7B-Instruct",
        "tencent/WeDLM-7B-Base",
        "zimengxiong/WeDLM-8B-Instruct-MLX-4bit",
        "zimengxiong/WeDLM-8B-Instruct-MLX"
      ]
    },
    {
      "architecture_id": "MobileLlamaForCausalLM",
      "total_models": 5,
      "sample_models": [
        "mtgv/MobileVLM_V2-1.7B",
        "mtgv/MobileVLM-1.7B",
        "mtgv/MobileVLM-3B",
        "mtgv/MobileVLM_V2-7B",
        "mtgv/MobileVLM_V2-3B"
      ]
    },
    {
      "architecture_id": "PointLLMLlamaForCausalLM",
      "total_models": 5,
      "sample_models": [
        "RunsenXu/PointLLM_7B_v1.2",
        "RunsenXu/PointLLM_7B_v1.1_init",
        "RunsenXu/PointLLM_13B_v1.2",
        "RunsenXu/PointLLM_13B_v1.1",
        "RunsenXu/PointLLM_7B_v1.1"
      ]
    },
    {
      "architecture_id": "HyperCLOVAXForCausalLM",
      "total_models": 5,
      "sample_models": [
        "FriendliAI/HyperCLOVAX-SEED-Think-14B",
        "K-Compression/HyperCLOVAX-SEED-Think-14B-GPTQ",
        "sigridjineth/HyperCLOVAX-SEED-Think-DeepConf-14B",
        "OpenLLM-Korea/HyperCLOVAX-SEED-Think-14B",
        "dev7halo/HyperCLOVAX-SEED-Think-14B-gptq-kli-rag"
      ]
    },
    {
      "architecture_id": "Int8OPTForCausalLM",
      "total_models": 5,
      "sample_models": [
        "mit-han-lab/opt-125m-smoothquant",
        "mit-han-lab/opt-1.3b-smoothquant",
        "mit-han-lab/opt-13b-smoothquant",
        "mit-han-lab/opt-2.7b-smoothquant",
        "mit-han-lab/opt-6.7b-smoothquant"
      ]
    },
    {
      "architecture_id": "HGRN2ForCausalLM",
      "total_models": 5,
      "sample_models": [
        "fla-hub/hgrn2-1.3B-100B",
        "fla-hub/hgrn2-2.7B-100B",
        "zhixuan-lin/hgrn2-760m-longcrawl64-48b",
        "PatrickHaller/hgrn2_pile_100m_distill_babylm",
        "PatrickHaller/hgrn2_de_wiki"
      ]
    },
    {
      "architecture_id": "MiniMindForCausalLM",
      "total_models": 5,
      "sample_models": [
        "fariasultana/MiniMind",
        "ShengweiPeng/MiniMind2-Small-Traditional-Chinese",
        "ZeLi111/freeTalk-chinese-uncensored-Instruct",
        "ZeLi111/freeTalk-chinese-uncensored-base",
        "yiwenX/MiniMind-MoE-640-120M"
      ]
    },
    {
      "architecture_id": "TelechatForCausalLM",
      "total_models": 5,
      "sample_models": [
        "Tele-AI/telechat-7B",
        "Tele-AI/telechat-7B-int8",
        "Tele-AI/TeleChat-12B-int8",
        "Tele-AI/TeleChat-1B",
        "Tele-AI/TeleChat-12B-int4"
      ]
    },
    {
      "architecture_id": "RWKV6Qwen2ForCausalLM",
      "total_models": 5,
      "sample_models": [
        "featherless-ai/QRWKV-72B",
        "recursal/QRWKV6-32B-Instruct-Preview-v0.1",
        "featherless-ai/QRWKV-QwQ-32B",
        "recursal/QRWKV6-7B-Base",
        "recursal/QRWKV6-7B-Instruct"
      ]
    },
    {
      "architecture_id": "MBartForConditionalGeneration",
      "total_models": 5,
      "sample_models": [
        "DeepPavlov/mbart-large-50-ru-persona-chat",
        "MahmutCanBoran/turkish-0-6-child-stories",
        "damienliccia/RuTaskFlow-mBART-T26-200K",
        "feedlight42/mbart25-text2picto",
        "Obeida/smart_reply_v2"
      ]
    },
    {
      "architecture_id": "SmalLmForCausalLM",
      "total_models": 5,
      "sample_models": [
        "Azrail/smallm_70",
        "Azrail/smallm_350",
        "Azrail/smallm_70_instruct",
        "Azrail/smallm_140_rope",
        "Azrail/smallm_70_rope"
      ]
    },
    {
      "architecture_id": "BunnyPhi3ForCausalLM",
      "total_models": 5,
      "sample_models": [
        "BAAI/Bunny-v1_0-4B",
        "praysimanjuntak/llava-phi3-3.8b-lora",
        "VideoGameBunny/VideoGameBunny-V1-4B",
        "AIPeanutman/Bunny-MMR-4B",
        "asgaardlab/VideoGameBunny-v1_0-4B"
      ]
    },
    {
      "architecture_id": "INFLMForCausalLM",
      "total_models": 5,
      "sample_models": [
        "infly/INF-34B-Chat",
        "infly/INF-34B-Base",
        "infly/INF-34B-Chat-GPTQ-4bit",
        "infly/INF-34B-Chat-GPTQ-8bit",
        "infly/INF-34B-Chat-AWQ"
      ]
    },
    {
      "architecture_id": "Rwkv5ForCausalLM",
      "total_models": 5,
      "sample_models": [
        "EleutherAI/Hermes-RWKV-v5-3B-HF",
        "RWKV/rwkv-5-world-1b5",
        "SmerkyG/rwkv-5-world-1b5",
        "TimeMobius/Mobius-RWKV-Chat-12B-128k-v4-HF",
        "SmerkyG/rwkv-5-world-3b"
      ]
    },
    {
      "architecture_id": "LilleForCausalLM",
      "total_models": 5,
      "sample_models": [
        "Nikity/lille-130m-base",
        "mlx-community/lille-130m-instruct-bf16",
        "mlx-community/lille-130m-instruct-fp16",
        "mlx-community/lille-130m-instruct-6bit",
        "mlx-community/lille-130m-instruct-8bit"
      ]
    },
    {
      "architecture_id": "MobilintLlamaForCausalLM",
      "total_models": 5,
      "sample_models": [
        "mobilint/Llama-3.2-3B-Instruct",
        "mobilint/HyperCLOVAX-SEED-Text-Instruct-0.5B",
        "mobilint/HyperCLOVAX-SEED-Text-Instruct-1.5B",
        "mobilint/Llama-3.2-1B-Instruct",
        "mobilint/Llama-3.1-8B-Instruct"
      ]
    },
    {
      "architecture_id": "DeltalmForConditionalGeneration",
      "total_models": 5,
      "sample_models": [
        "IDEA-CCNL/Randeng-Deltalm-362M-En-Zh",
        "IDEA-CCNL/Randeng-Deltalm-362M-Zh-En",
        "nguyenvulebinh/deltalm-base",
        "Ahmed-Selem/bug",
        "shahadalll/T10"
      ]
    },
    {
      "architecture_id": "GitLlamaForCausalLM",
      "total_models": 5,
      "sample_models": [
        "Inoichan/GIT-Llama-2-7B",
        "turing-motors/heron-chat-git-Llama-2-7b-v0",
        "turing-motors/heron-chat-git-ELYZA-fast-7b-v0",
        "turing-motors/heron-preliminary-git-Llama-2-70b-v0",
        "bushali/GITLlama"
      ]
    },
    {
      "architecture_id": "DetikzifyForCausalLM",
      "total_models": 5,
      "sample_models": [
        "nllg/detikzify-ds-1.3b",
        "nllg/detikzify-ds-7b",
        "nllg/detikzify-cl-7b",
        "nllg/detikzify-tl-1.1b",
        "Mostafa3zazi/exp_1"
      ]
    },
    {
      "architecture_id": "ReplitLM",
      "total_models": 5,
      "sample_models": [
        "lentan/replit",
        "sadiqj/camlcoder",
        "lukasmoeller/replchat",
        "amazingvince/replitchat-alpha",
        "amazingvince/replit-chat"
      ]
    },
    {
      "architecture_id": "FlamingoForConditionalGeneration",
      "total_models": 5,
      "sample_models": [
        "luodian/OTTER-MPT7B-Init",
        "luodian/OTTER-LLaMA7B-Init",
        "luodian/Flamingo-Llama2-Chat7B-CC3M",
        "sugiv/Spoonbill-Llama2OtterFlamingoAreFriends-7B-Chat",
        "sugiv/Spoonbill-GarudaOtterFlamingoAreFriends-7B-Chat"
      ]
    },
    {
      "architecture_id": "LlavaPhi3ForCausalLM",
      "total_models": 5,
      "sample_models": [
        "FreedomIntelligence/ALLaVA-Phi3-mini-128k",
        "shi-labs/OLA-VLM-CLIP-ViT-Phi3-4k-mini",
        "shi-labs/OLA-VLM-CLIP-ConvNeXT-Phi3-4k-mini",
        "mucai/vip-llava-phi-3-mini-3.8B",
        "binxia/llmga-Phi-3-mini-128k-full-finetune"
      ]
    },
    {
      "architecture_id": "GPTForCausalLM",
      "total_models": 5,
      "sample_models": [
        "Wonder-Griffin/Shorsey-T2000",
        "Doreamonzzz/xmixers_gpt_small_lpe_50b",
        "Doreamonzzz/xmixers_gpt_120m_50b",
        "keita-origin/Bowchan-1-open",
        "laitkor/mistralai-finetuned-v3"
      ]
    },
    {
      "architecture_id": "BaichuanM1ForCausalLM",
      "total_models": 5,
      "sample_models": [
        "mlx-community/Baichuan-M1-14B-Instruct-4bit-ft",
        "pcuenq/Baichuan-M1-14B-Instruct-tokenizer",
        "mlx-community/Baichuan-M1-14B-Instruct-8bit",
        "mlx-community/Baichuan-M1-14B-Instruct",
        "mlx-community/Baichuan-M1-14B-Instruct-4bit"
      ]
    },
    {
      "architecture_id": "BigBirdForCausalLM",
      "total_models": 5,
      "sample_models": [
        "breadlicker45/musenet-untrained",
        "hf-tiny-model-private/tiny-random-BigBirdForCausalLM",
        "tristayqc/my_awesome_eli5_clm-model",
        "radic2682/bigBird-base-fine-tuning-squad-B64",
        "radic2682/bigBird-base-fine-tuning-squad-B16R1"
      ]
    },
    {
      "architecture_id": "BVVAbsForCausalLM",
      "total_models": 5,
      "sample_models": [
        "Bochkov/abs-bvv-1",
        "Bochkov/abs-bvv-4",
        "Bochkov/abs-bvv-2",
        "Bochkov/abs-bvv-5",
        "Bochkov/abs-bvv-3"
      ]
    },
    {
      "architecture_id": "LlamaGlideDecoderLayer",
      "total_models": 5,
      "sample_models": [
        "sail/longspec-vicuna-13b-v1.5-16k",
        "sail/longspec-longchat-7b-v1.5-32k",
        "sail/longspec-Llama-3-8B-Instruct-262k",
        "sail/longspec-vicuna-7b-v1.5-16k",
        "sail/longspec-longchat-13b-16k"
      ]
    },
    {
      "architecture_id": "MyT5ForConditionalGeneration",
      "total_models": 5,
      "sample_models": [
        "Inria-CEDAR/GrailQAT5JointGT",
        "Inria-CEDAR/DartT5JointGT",
        "Inria-CEDAR/WebNLG20T5JointGT",
        "zabojeb/MvP",
        "Inria-CEDAR/SimpleQT5JointGT"
      ]
    },
    {
      "architecture_id": "CustomBartForConditionalGeneration",
      "total_models": 5,
      "sample_models": [
        "p208p2002/qmst-qgg-qa",
        "p208p2002/qmst-qgg",
        "gayanin/temp",
        "gayanin/custom1",
        "gayanin/custom2"
      ]
    },
    {
      "architecture_id": "FunctionaryForCausalLM",
      "total_models": 5,
      "sample_models": [
        "andyai01/model-20k6-0708-v31",
        "selectorseb/s2-oracle-function-calling.v2",
        "andyai01/model_19k5_1508_v3.2_new",
        "selectorseb/s2-oracle-function-calling.v1",
        "saudsaleem/meetkai-8b-1"
      ]
    },
    {
      "architecture_id": "Phi3Model",
      "total_models": 5,
      "sample_models": [
        "tasal9/ZamAI-Phi-3-Mini-Pashto",
        "bnb-community/phi-4-bnb-4bit",
        "marcsun13/phi-4-bnb-4bit",
        "medmekk/phi-4-bnb-4bit",
        "Frane92O/Phi-4-mini-instruct-bnb-4bit"
      ]
    },
    {
      "architecture_id": "EduLLMForCausalLM",
      "total_models": 5,
      "sample_models": [
        "amnae/base_edu_llm_damex",
        "amnae/base_edu_llm_xmoe",
        "amnae/base_edu_llm_mixtral",
        "amnae/base_edu_llm_xmoe_trained",
        "amnae/base_edu_llm_mixtral_trained"
      ]
    },
    {
      "architecture_id": "UserLlamaForCausalLM",
      "total_models": 5,
      "sample_models": [
        "humainlab/llama3.1-8b-instruct-prlhf-1200",
        "humainlab/llama3.1-8b-instruct-dpo-allusers",
        "humainlab/llama3.1-8b-instruct-dpo-1200",
        "humainlab/llama3.1-8b-instruct-prlhf-bos-origatten",
        "humainlab/llama3.1-8b-instruct-dpo-bos-origatten"
      ]
    },
    {
      "architecture_id": "IdeficsForVisionText2Text",
      "total_models": 4,
      "sample_models": [
        "HuggingFaceM4/idefics-9b",
        "HuggingFaceM4/idefics-80b-instruct",
        "HuggingFaceM4/idefics-9b-instruct",
        "areegtarek/idefics-9b-instruct-all"
      ]
    },
    {
      "architecture_id": "ChameleonXLLMXForConditionalGeneration",
      "total_models": 4,
      "sample_models": [
        "Alpha-VLLM/Lumina-mGPT-7B-768",
        "Alpha-VLLM/Lumina-mGPT-7B-768-Omni",
        "Alpha-VLLM/Lumina-mGPT-7B-512-MultiImage",
        "xing0916/ARRA-Adapt-MIMIC-7B"
      ]
    },
    {
      "architecture_id": "AprielForCausalLM",
      "total_models": 4,
      "sample_models": [
        "ServiceNow-AI/Apriel-5B-Instruct",
        "ServiceNow-AI/Apriel-5B-Base",
        "lunahr/apriel-5b-instruct-abliterated",
        "mrfakename/Apriel-5B-Instruct"
      ]
    },
    {
      "architecture_id": "StableLMAlphaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "stabilityai/stablelm-base-alpha-7b-v2",
        "stabilityai/stablelm-base-alpha-3b-v2",
        "llmware/dragon-stablelm-7b-v0",
        "TinyPixel/stablelm-base-alpha-3b-v2"
      ]
    },
    {
      "architecture_id": "DeciCoderForCausalLM",
      "total_models": 4,
      "sample_models": [
        "Deci/DeciCoder-1b",
        "smangrul/DeciCoder1B-personal-copilot-merged",
        "Soheil-FM/deci-finetuned-1b-test",
        "chgk13/decicoder-1b-openvino-int8"
      ]
    },
    {
      "architecture_id": "RITAModelForCausalLM",
      "total_models": 4,
      "sample_models": [
        "lightonai/RITA_xl",
        "lightonai/RITA_s",
        "lightonai/RITA_m",
        "lightonai/RITA_l"
      ]
    },
    {
      "architecture_id": "StepAudio2ForCausalLM",
      "total_models": 4,
      "sample_models": [
        "stepfun-ai/Step-Audio-2-mini",
        "stepfun-ai/Step-Audio-R1",
        "TransWithAI/Step-Audio-R1-NVFP4A16",
        "chaitnya26/Step-Audio-2-mini-fork"
      ]
    },
    {
      "architecture_id": "IQuestCoderForCausalLM",
      "total_models": 4,
      "sample_models": [
        "IQuestLab/IQuest-Coder-V1-40B-Instruct",
        "mlx-community/IQuest-Coder-V1-40B-Instruct-4bit",
        "mlx-community/IQuest-Coder-V1-40B-Instruct-6bit",
        "IQuestLab/IQuest-Coder-V1-40B-Base"
      ]
    },
    {
      "architecture_id": "Videollama2Qwen2ForCausalLM",
      "total_models": 4,
      "sample_models": [
        "DAMO-NLP-SG/VideoLLaMA2.1-7B-AV",
        "DAMO-NLP-SG/VideoLLaMA2-72B",
        "lym0302/VideoLLaMA2.1-7B-AV-CoT",
        "lym0302/VideoLLaMA2.1-7B-AV-QA"
      ]
    },
    {
      "architecture_id": "OtterForConditionalGeneration",
      "total_models": 4,
      "sample_models": [
        "luodian/OTTER-MPT1B-RPJama-Init",
        "luodian/OTTER-Video-LLaMA7B-DenseCaption",
        "luodian/OTTER-Image-MPT7B",
        "tabtoyou/Ko-Otter-9B-LACR-v0"
      ]
    },
    {
      "architecture_id": "SeerAttnLlamaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "SeerAttention/SeerAttention-Llama-3.1-8B-AttnGates",
        "SeerAttention/SeerAttention-DeepSeek-R1-Distill-Llama-70B-AttnGates",
        "SeerAttention/SeerAttention-Llama-3.1-70B-AttnGates",
        "SeerAttention/SeerAttention-Llama-3.1-8B"
      ]
    },
    {
      "architecture_id": "BolmoForCausalLM",
      "total_models": 4,
      "sample_models": [
        "allenai/Bolmo-1B",
        "AlekseyCalvin/Byte_LYRICAL_Translation_ru2en_2_Bolmo7b_SFT_wOlmoCore",
        "AlekseyCalvin/Lyrical_Bolmo_7b_SFT_Merged",
        "AlekseyCalvin/Lyrical_Bolmo_7b_ORPO"
      ]
    },
    {
      "architecture_id": "LLaVAOneVision1_5_ForConditionalGeneration",
      "total_models": 4,
      "sample_models": [
        "Jinghao-Guo/llavaov1.5-4B-instruct-converted",
        "Jinghao-Guo/llavaov1.5-4B-instruct-converted-qwen",
        "Jinghao-Guo/LLaVA-OneVision-1.5-4B-Base-converted",
        "Jinghao-Guo/llavaov1.5-1B-instruct-converted-qwen"
      ]
    },
    {
      "architecture_id": "CogVLMVideoForCausalLM",
      "total_models": 4,
      "sample_models": [
        "zai-org/cogvlm2-llama3-caption",
        "zai-org/cogvlm2-video-llama3-chat",
        "zai-org/cogvlm2-video-llama3-base",
        "MonsterMMORPG/CogVLMFixed"
      ]
    },
    {
      "architecture_id": "Videollama2MistralForCausalLM",
      "total_models": 4,
      "sample_models": [
        "DAMO-NLP-SG/VideoLLaMA2-7B",
        "DAMO-NLP-SG/VideoLLaMA2-7B-16F",
        "Aliayub1995/VideoLLaMA2-7B",
        "ccclemenfff/AVL"
      ]
    },
    {
      "architecture_id": "LlavaCrystalForCausalLM",
      "total_models": 4,
      "sample_models": [
        "LLM360/CrystalChat-7B-Web2Code",
        "qazimbhat1/Crystal-based-MLLM-7B",
        "qazimbhat1/my-model-repo3",
        "qazimbhat1/crystal-chat-general"
      ]
    },
    {
      "architecture_id": "Llamavision",
      "total_models": 4,
      "sample_models": [
        "qresearch/llama-3-vision-alpha-hf",
        "qresearch/llama-3.1-8B-vision-378",
        "kadirnar/Llama-3.1-8B-Vision",
        "kadirnar/Llama-3.2-1B-Vision"
      ]
    },
    {
      "architecture_id": "TanukiForCausalLM",
      "total_models": 4,
      "sample_models": [
        "weblab-GENIAC/Tanuki-8x8B-dpo-v1.0",
        "team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ",
        "team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-4bit",
        "team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-GPTQ-8bit"
      ]
    },
    {
      "architecture_id": "MGMTTSForCausalLM",
      "total_models": 4,
      "sample_models": [
        "wcy1122/MGM-Omni-TTS-2B-0927",
        "wcy1122/MGM-Omni-TTS-0.6B",
        "wcy1122/MGM-Omni-TTS-2B",
        "wcy1122/MGM-Omni-TTS-4B"
      ]
    },
    {
      "architecture_id": "CambrianLlamaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "nyu-visionx/cambrian-8b",
        "nyu-visionx/cambrian-13b",
        "nyu-visionx/cambrian-34b",
        "tsbpp/llava-vicuna-7b-diffusion-sd2_1-p16-res512-737k-bs512"
      ]
    },
    {
      "architecture_id": "NovaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "lt-asset/nova-1.3b-bcr",
        "harshit36/NOVA-Verse",
        "harshit36/Nova-Casual-LLM",
        "dineth554/nova28k"
      ]
    },
    {
      "architecture_id": "ExtendedMptForCausalLM",
      "total_models": 4,
      "sample_models": [
        "normalcomputing/extended-mind-mpt-7b-chat",
        "normalcomputing/extended-mind-mpt-7b",
        "normalcomputing/extended-mind-mpt-30b-chat",
        "normalcomputing/extended-mind-mpt-30b"
      ]
    },
    {
      "architecture_id": "ErnieForCausalLM",
      "total_models": 4,
      "sample_models": [
        "mohitsha/tiny-ernie-random-remote-code",
        "wybxc/next-yiri",
        "wybxc/new-yiri",
        "wybxc/base-yiri"
      ]
    },
    {
      "architecture_id": "PicoDecoderHF",
      "total_models": 4,
      "sample_models": [
        "pico-lm/pico-decoder-small",
        "pico-lm/pico-decoder-medium",
        "pico-lm/pico-decoder-large",
        "llm-slice/pico-decoder-medium"
      ]
    },
    {
      "architecture_id": "ZayaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "Zyphra/ZAYA1-base",
        "yujiepan/zaya1-tiny-random",
        "Zyphra/ZAYA1-reasoning-base",
        "tiny-random/zaya1"
      ]
    },
    {
      "architecture_id": "PipelinedBartForConditionalGeneration",
      "total_models": 4,
      "sample_models": [
        "Payoto/bart-base-finetuned-en-to-ro",
        "graphcore-rahult/bart-base-finetuned-en-to-ro",
        "jimypbr/bart-base-finetuned-xsum",
        "nmb-paperspace-hf/bart-base-finetuned-en-to-ro"
      ]
    },
    {
      "architecture_id": "ShikraLlamaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "shikras/shikra-7b-delta-v1",
        "shikras/shikra-7b-delta-v1-0708",
        "Anonymous-G/Genixer-shikra-7b",
        "Anonymous-G/shikra-Genixer-350K-7b"
      ]
    },
    {
      "architecture_id": "FP8LlamaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "xihc-ucb/Llama-3.1-8B-train-Quasar-1002",
        "xihc-ucb/Llama-3.1-8B-Instruct-train-Quasar-1002",
        "xihc-ucb/Meta-Llama-3-8B-Instruct-train-Quasar-1002",
        "xihc-ucb/Meta-Llama-3-8B-train-Quasar-1002"
      ]
    },
    {
      "architecture_id": "LlavaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "shauray/Llava-v1.5-7B-hf",
        "vonjack/Hermes-2-Pro-BakLLaVA-Mistral-7B",
        "DeepXR/Helion-V2.0-Thinking",
        "cretone/alphex-13B-prototype"
      ]
    },
    {
      "architecture_id": "KANGPT2LMHeadModel",
      "total_models": 4,
      "sample_models": [
        "paolvz/gpt2kanpart12",
        "paolvz/gpt2kanpart8",
        "paolvz/gpt2kanfull12",
        "paolvz/nanokat"
      ]
    },
    {
      "architecture_id": "Qwen2_5OmniModel",
      "total_models": 4,
      "sample_models": [
        "giangndm/qwen2.5-omni-3b-mlx-8bit",
        "giangndm/qwen2.5-omni-7b-mlx-4bit",
        "giangndm/qwen2.5-omni-3b-mlx-4bit",
        "giangndm/qwen2.5-omni-7b-mlx-8bit"
      ]
    },
    {
      "architecture_id": "MiphaPhiForCausalLM",
      "total_models": 4,
      "sample_models": [
        "zhumj34/Mipha-3B",
        "Yuanze/Olympus",
        "PhelixZhen/Algea-VE",
        "EQUES/eques-vlm-v1"
      ]
    },
    {
      "architecture_id": "ImpForCausalLM",
      "total_models": 4,
      "sample_models": [
        "MILVLG/Imp-v1.5-3B-Phi2",
        "ManishThota/CustomModel",
        "MILVLG/Imp-v1.5-3B-196",
        "iambestfeed/imp-v1-3b-mrope"
      ]
    },
    {
      "architecture_id": "MobilintExaoneForCausalLM",
      "total_models": 4,
      "sample_models": [
        "mobilint/EXAONE-3.5-7.8B-Instruct",
        "mobilint/EXAONE-3.5-2.4B-Instruct",
        "mobilint/EXAONE-Deep-2.4B",
        "mobilint/EXAONE-Deep-7.8B"
      ]
    },
    {
      "architecture_id": "LiteWhisperForConditionalGeneration",
      "total_models": 4,
      "sample_models": [
        "onnx-community/lite-whisper-large-v3-turbo-ONNX",
        "onnx-community/lite-whisper-large-v3-fast-ONNX",
        "onnx-community/lite-whisper-large-v3-acc-ONNX",
        "onnx-community/lite-whisper-large-v3-turbo-acc-ONNX"
      ]
    },
    {
      "architecture_id": "GraphsGPTForCausalLM",
      "total_models": 4,
      "sample_models": [
        "DaizeDong/GraphsGPT-1W",
        "DaizeDong/GraphsGPT-4W",
        "DaizeDong/GraphsGPT-8W",
        "DaizeDong/GraphsGPT-2W"
      ]
    },
    {
      "architecture_id": "MistralStarForCausalLM",
      "total_models": 4,
      "sample_models": [
        "LeroyDyer/_Spydaz_Web_AI_MistralStar_V2",
        "LeroyDyer/_Spydaz_Web_AI_MistralStar_001_Project",
        "LeroyDyer/QuietStar_Project",
        "LeroyDyer/SpydazWeb_AGI_MistralStar_001_Project"
      ]
    },
    {
      "architecture_id": "LCKVLlamaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "whynlp/tinyllama-lckv-w2-ft-100b",
        "whynlp/tinyllama-lckv-w2-100b",
        "whynlp/tinyllama-lckv-w10-ft-250b",
        "whynlp/tinyllama-lckv-w10-100b"
      ]
    },
    {
      "architecture_id": "ExtendedLlamaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "normalcomputing/extended-mind-llama-2-7b",
        "normalcomputing/extended-mind-llama-2-7b-chat",
        "normalcomputing/extended-mind-llama-2-70b",
        "normalcomputing/extended-mind-llama-2-70b-chat"
      ]
    },
    {
      "architecture_id": "CPTForConditionalGeneration",
      "total_models": 4,
      "sample_models": [
        "RUCAIBox/Erya",
        "RUCAIBox/Erya4FT",
        "ShacklesLay/CPT4task2",
        "jaong/CPT_for_Feedback_Generation"
      ]
    },
    {
      "architecture_id": "MixtralMoleForCausalLM",
      "total_models": 4,
      "sample_models": [
        "ondevicellm/tinyllama_mole_sft_router05_lr1e-4_ep3",
        "ondevicellm/tinyllama_mole_dpo_ep3",
        "ondevicellm/tinyllama_mole_sftv2_ultrachat_ep3",
        "ondevicellm/tinyllama_mole_sft_router05_ep3"
      ]
    },
    {
      "architecture_id": "MobilintQwen2ForCausalLM",
      "total_models": 4,
      "sample_models": [
        "mobilint/Qwen2.5-0.5B-Instruct",
        "mobilint/Qwen2.5-3B-Instruct",
        "mobilint/Qwen2.5-1.5B-Instruct",
        "mobilint/Qwen2.5-7B-Instruct"
      ]
    },
    {
      "architecture_id": "MegrezMoeForCausalLM",
      "total_models": 4,
      "sample_models": [
        "Infinigence/Megrez2-3x7B-A3B",
        "sii-research/InnoMegrez2-Preview",
        "Infinigence/Megrez2-3x7B-A3B-Preview",
        "sii-research/InnoMegrez2"
      ]
    },
    {
      "architecture_id": "Qwen2ChunkingForCausalLM",
      "total_models": 4,
      "sample_models": [
        "DongfuJiang/Qwen2.5-0.5B-Instruct",
        "DongfuJiang/qwen2_chunking_mlp_freeze_uniform_with_shared_start_and_end_2_6_pt",
        "DongfuJiang/qwen2_chunking_mlp_freeze_uniform_with_shared_start_pt",
        "DongfuJiang/qwen2_chunking_mlp_freeze_uniform_with_shared_start_and_end_2_12_pt"
      ]
    },
    {
      "architecture_id": "VoRAForCausalLM",
      "total_models": 4,
      "sample_models": [
        "Hon-Wong/VoRA-7B-Instruct",
        "Hon-Wong/VoRA-7B-Base",
        "yalharbi/vora_chartvqa",
        "LuwamMajor/vora-biomistral-vqa"
      ]
    },
    {
      "architecture_id": "VILAForCasualLM",
      "total_models": 4,
      "sample_models": [
        "Efficient-Large-Model/Llama-3-VILA15-8B-hf-preview",
        "Efficient-Large-Model/VILA15-13b-hf-preview",
        "Efficient-Large-Model/VILA15-40b-hf-preview",
        "Efficient-Large-Model/VILA15-3b-hf-preview"
      ]
    },
    {
      "architecture_id": "GPT2LLMHeadModel",
      "total_models": 4,
      "sample_models": [
        "crumbly/gpt2-linear-xl-sharded-bf16",
        "crumbly/gpt2-linear-small",
        "crumbly/gpt2-linear-large",
        "crumbly/gpt2-linear-medium"
      ]
    },
    {
      "architecture_id": "BertModel",
      "total_models": 4,
      "sample_models": [
        "OceanOmics/eDNABERT-S_16S",
        "OceanOmics/eDNABERT-S_12S",
        "yogami9/need-semantic-search",
        "vivienfanghua/my-bert-model"
      ]
    },
    {
      "architecture_id": "MoAMetricLM",
      "total_models": 4,
      "sample_models": [
        "reaperdoesntknow/MoA-155M",
        "reaperdoesntknow/MoA-100M",
        "reaperdoesntknow/MoA-150M",
        "reaperdoesntknow/MoA-400M"
      ]
    },
    {
      "architecture_id": "NanoGPTModel",
      "total_models": 4,
      "sample_models": [
        "prompterminal/nanogpt-shakespeare-compressed",
        "0zk1/nanochat-d16-rocmrx9070-instruct",
        "0zk1/nanochat-d16-rocmrx9070-mid",
        "0zk1/nanochat-d16-rocmrx9070-base"
      ]
    },
    {
      "architecture_id": "Qwen3SharedMoeForCausalLM",
      "total_models": 4,
      "sample_models": [
        "Doctor-Shotgun/Qwen3-Coder-30B-A3B-Instruct-ScatterMoE",
        "Doctor-Shotgun/Qwen3-235B-A22B-Instruct-2507-ScatterMoE",
        "Doctor-Shotgun/Qwen3-30B-A3B-Thinking-2507-ScatterMoE",
        "Doctor-Shotgun/Qwen3-30B-A3B-Instruct-2507-ScatterMoE"
      ]
    },
    {
      "architecture_id": "AttnQwenForCausalLM",
      "total_models": 4,
      "sample_models": [
        "mtzig/qwen3_decoder_small",
        "mtzig/qwen3_link3_small",
        "mtzig/qwen3_linv3_no-sssd-linkr8_small",
        "mtzig/qwen3_TEST_VM"
      ]
    },
    {
      "architecture_id": "BlenderbotSmallForCausalLM",
      "total_models": 4,
      "sample_models": [
        "Orectique/Sarcizian",
        "christianbaluti/bot",
        "kellyjiayixu/my_awesome_eli5_clm-model_blenderbot_small",
        "zxdexpo/text_model_blenderbot"
      ]
    },
    {
      "architecture_id": "ValleyLlamaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "luoruipu1/valley-13b-v1-delta",
        "luoruipu1/Valley2-7b",
        "luoruipu1/valley-13b-pretrain",
        "luoruipu1/Valley2-7b-pretrain"
      ]
    },
    {
      "architecture_id": "Qwen2ForProcessRewardModel",
      "total_models": 4,
      "sample_models": [
        "kingsleykim/orm_1.5b",
        "kingsleykim/Qwen2.5-Math-PRM-1.5B",
        "kingsleykim/Qwen2.5-Math-7B-Prm-Three",
        "kingsleykim/min_loss_1.5B"
      ]
    },
    {
      "architecture_id": "Phi2MoeForCausalLM",
      "total_models": 4,
      "sample_models": [
        "erniesg/boya",
        "mzbac/phi-2-2x3-hf",
        "mzbac/phi-2-2x3",
        "mzbac/phi-2-2x4-hf"
      ]
    },
    {
      "architecture_id": "BertForSequenceClassification",
      "total_models": 4,
      "sample_models": [
        "miguelmejias0512/mi-super-modelo",
        "MattStammers/Covid19_Text_Model",
        "b-aser/jku-g3-llm-v2",
        "yogami9/need-content-moderation"
      ]
    },
    {
      "architecture_id": "NGMEForCausalLM",
      "total_models": 4,
      "sample_models": [
        "PatrickHaller/ngme-llama-264M",
        "PatrickHaller/ngme-tiny-stories",
        "PatrickHaller/ngme-babylm-100M",
        "PatrickHaller/ngme-babylm-100M-v2"
      ]
    },
    {
      "architecture_id": "MambaInQwenForCausalLM",
      "total_models": 4,
      "sample_models": [
        "ucmp137538/miqhybrid_mixed_simpo_reasoning",
        "ucmp137538/miqhybridmixed_simpo_grppol_ckp_1k",
        "ucmp137538/miqhybridmixed_simpo_grppol_concat_ckp_1k",
        "ucmp137538/miqhybrid_iter3"
      ]
    },
    {
      "architecture_id": "RecLlamaForCausalLM",
      "total_models": 4,
      "sample_models": [
        "Arthur-LAGACHERIE/SmolRec-50k-Instruct",
        "Arthur-LAGACHERIE/RecLlama-3.2-1B-Instruct-15k",
        "Arthur-LAGACHERIE/SmolRec-50k",
        "Arthur-LAGACHERIE/RecLlama-3.2-1B-Instruct"
      ]
    },
    {
      "architecture_id": "XQwen3ForCausalLM",
      "total_models": 4,
      "sample_models": [
        "khalidrizki/xRAG-pretrained-v2",
        "khalidrizki/xRAG",
        "khalidrizki/xRAG-v3",
        "khalidrizki/xRAG-fix"
      ]
    },
    {
      "architecture_id": "EmbformerForCausalLM",
      "total_models": 4,
      "sample_models": [
        "HighCWu/Embformer-MiniMind-Base-0.1B",
        "HighCWu/Embformer-MiniMind-Seqlen512-0.1B",
        "HighCWu/Embformer-MiniMind-0.1B",
        "HighCWu/Embformer-MiniMind-RLHF-0.1B"
      ]
    },
    {
      "architecture_id": "GPTRefactForCausalLM",
      "total_models": 3,
      "sample_models": [
        "refactai/Refact-1_6B-fim",
        "refactai/Refact-1_6-base",
        "pravsels/Refact-1_6B-fim-finetuned-manimation"
      ]
    },
    {
      "architecture_id": "TarsierForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "omni-research/Tarsier-7b",
        "omni-research/Tarsier-34b",
        "princepride/tarsier-7b-finetune"
      ]
    },
    {
      "architecture_id": "Maira2ForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "microsoft/maira-2",
        "gnedivad/maira-2",
        "kp-forks/maira-2"
      ]
    },
    {
      "architecture_id": "Fgclip2Model",
      "total_models": 3,
      "sample_models": [
        "qihoo360/fg-clip2-base",
        "qihoo360/fg-clip2-so400m",
        "qihoo360/fg-clip2-large"
      ]
    },
    {
      "architecture_id": "MiniMaxForCausalLM",
      "total_models": 3,
      "sample_models": [
        "MiniMaxAI/MiniMax-Text-01-hf",
        "MiniMaxAI/MiniMax-M1-40k-hf",
        "MiniMaxAI/MiniMax-M1-80k-hf"
      ]
    },
    {
      "architecture_id": "Plamo3ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "pfnet/plamo-3-nict-2b-base",
        "pfnet/plamo-3-nict-8b-base",
        "WayBob/Way-sft-plamo-3-8b-chat"
      ]
    },
    {
      "architecture_id": "ArcticForCausalLM",
      "total_models": 3,
      "sample_models": [
        "Snowflake/snowflake-arctic-instruct",
        "Snowflake/snowflake-arctic-base",
        "katuni4ka/tiny-random-snowflake"
      ]
    },
    {
      "architecture_id": "GPTNeoXJapaneseForCausalLM",
      "total_models": 3,
      "sample_models": [
        "abeja/gpt-neox-japanese-2.7b",
        "hf-tiny-model-private/tiny-random-GPTNeoXJapaneseForCausalLM",
        "ebisuke/liz-nojaloli-nxja-ja"
      ]
    },
    {
      "architecture_id": "PersimmonForCausalLM",
      "total_models": 3,
      "sample_models": [
        "adept/persimmon-8b-chat",
        "adept/persimmon-8b-base",
        "pszemraj/perSLIMmon-8b-base"
      ]
    },
    {
      "architecture_id": "HulumedQwen3ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "ZJU-AI4H/Hulu-Med-4B",
        "ZJU-AI4H/Hulu-Med-14B",
        "Flare77/YXSMLLM"
      ]
    },
    {
      "architecture_id": "HulumedQwen2ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "ZJU-AI4H/Hulu-Med-7B",
        "ZJU-AI4H/Hulu-Med-32B",
        "Flare77/HuLuLLM"
      ]
    },
    {
      "architecture_id": "VaultGemmaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "google/vaultgemma-1b",
        "OpenKing/vualtgemma-1b-non-gated",
        "onnx-community/vaultgemma-1b-ONNX"
      ]
    },
    {
      "architecture_id": "NemotronFlashForCausalLM",
      "total_models": 3,
      "sample_models": [
        "nvidia/Nemotron-Flash-3B",
        "nvidia/Nemotron-Flash-3B-Instruct",
        "nvidia/Nemotron-Flash-1B"
      ]
    },
    {
      "architecture_id": "InductionVLForCausalLM",
      "total_models": 3,
      "sample_models": [
        "jonathanli/induction-vl-pretrained",
        "jonathanli/induction-vl-pretrained-tmp-tiny",
        "jonathanli/induction-vl"
      ]
    },
    {
      "architecture_id": "modeling_camelidae.LlamaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "hywu/Camelidae-8x34B",
        "hywu/Camelidae-8x7B",
        "hywu/Camelidae-8x13B"
      ]
    },
    {
      "architecture_id": "LayerWiseMiniCPMForCausalLM",
      "total_models": 3,
      "sample_models": [
        "BAAI/bge-reranker-v2-minicpm-layerwise",
        "bespin-global/bge-reranker-v2-minicpm-layerwise-fine-tuning",
        "boboliu/bge-reranker-v2-minicpm-layerwise-gptq"
      ]
    },
    {
      "architecture_id": "GRIN-MoE",
      "total_models": 3,
      "sample_models": [
        "microsoft/GRIN-MoE",
        "alexbuz/GRIN-MoE-2",
        "alexbuz/GRIN-MoE"
      ]
    },
    {
      "architecture_id": "MagmaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "microsoft/Magma-8B",
        "xuanzhaopeng/Magma-8B",
        "alvarobartt/Magma-8B"
      ]
    },
    {
      "architecture_id": "BottleneckT5LMWithPerturb",
      "total_models": 3,
      "sample_models": [
        "thesephist/contra-bottleneck-t5-large-wikipedia",
        "thesephist/contra-bottleneck-t5-small-wikipedia",
        "thesephist/contra-bottleneck-t5-xl-wikipedia"
      ]
    },
    {
      "architecture_id": "NeuroBLASTForCausalLM",
      "total_models": 3,
      "sample_models": [
        "mkurman/NeuroBLAST-V3-SYNTH-EC-150000",
        "mkurman/NeuroBLAST-V3-SYNTH-EC-150000-JAX",
        "mkurman/NeuroBLAST-V3-0.6B-SYNTH-EC-144B-TOK"
      ]
    },
    {
      "architecture_id": "Step1ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "stepfun-ai/Step-Audio-EditX",
        "snakech/cot_5k-GGUF",
        "StepLaw/StepLaw-N_1.0B-D_56.0B-LR9.766e-04-BS65536"
      ]
    },
    {
      "architecture_id": "xLSTMForCausalLM",
      "total_models": 3,
      "sample_models": [
        "stefan-it/xlstm-german-wikipedia",
        "ethicalabs/xLSTM-7b-Instruct",
        "ethicalabs/xLSTM-7b-Polymath"
      ]
    },
    {
      "architecture_id": "DeltaNetForCausalLM",
      "total_models": 3,
      "sample_models": [
        "fla-hub/delta_net-1.3B-100B",
        "fla-hub/delta_net-1.3B-8K-100B",
        "fla-hub/delta_net-2.7B-100B"
      ]
    },
    {
      "architecture_id": "PegasusForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "Eemansleepdeprived/Humaneyes",
        "varocarras/Humaneyes",
        "data-plumber/pegasus-cnn-dailymail-tf"
      ]
    },
    {
      "architecture_id": "AraGPT2LMHeadModel",
      "total_models": 3,
      "sample_models": [
        "aubmindlab/aragpt2-large",
        "aubmindlab/aragpt2-mega",
        "ehab215/DrAI-chatbot"
      ]
    },
    {
      "architecture_id": "FuxiTranyuForCausalLM",
      "total_models": 3,
      "sample_models": [
        "TJUNLP/FuxiTranyu-8B",
        "TJUNLP/FuxiTranyu-8B-SFT",
        "TJUNLP/FuxiTranyu-8B-DPO"
      ]
    },
    {
      "architecture_id": "CodeShellForCausalLM",
      "total_models": 3,
      "sample_models": [
        "WisdomShell/CodeShell-7B",
        "WisdomShell/CodeShell-7B-Chat",
        "ZHENGRAN/zephyr-7b-sft-full"
      ]
    },
    {
      "architecture_id": "CrystalCoderLMHeadModel",
      "total_models": 3,
      "sample_models": [
        "LLM360/Crystal",
        "LLM360/CrystalChat",
        "qazimbhat1/crystal-chat-general-pretrain"
      ]
    },
    {
      "architecture_id": "PipelinedT5ForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "Payoto/t5-small-finetuned-xsum",
        "graphcore-rahult/t5-small-finetuned-xsum",
        "ncouro/flan-t5-xl-ipu"
      ]
    },
    {
      "architecture_id": "ParamBharatGenForCausalLM",
      "total_models": 3,
      "sample_models": [
        "bharatgenai/AyurParam",
        "bharatgenai/Param-1-2.9B-Instruct",
        "bharatgenai/Param-1"
      ]
    },
    {
      "architecture_id": "CpmBeeForCausalLM",
      "total_models": 3,
      "sample_models": [
        "openbmb/cpm-bee-1b",
        "openbmb/cpm-bee-2b",
        "openbmb/cpm-bee-5b"
      ]
    },
    {
      "architecture_id": "GPT3DevLMHeadModel",
      "total_models": 3,
      "sample_models": [
        "k050506koch/GPT3-dev-125m-0612",
        "k050506koch/GPT3-dev-125m-1202",
        "k050506koch/GPT3-dev"
      ]
    },
    {
      "architecture_id": "PhariaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "Aleph-Alpha/Pharia-1-LLM-7B-control-aligned-hf",
        "bil-y/Pharia-1-LLM-7B-control-bnb-4bit",
        "bil-y/Pharia-1-LLM-7B-control-hf"
      ]
    },
    {
      "architecture_id": "ZsGPT2LMHeadModel",
      "total_models": 3,
      "sample_models": [
        "claritylab/zero-shot-vanilla-gpt2",
        "claritylab/zero-shot-explicit-gpt2",
        "claritylab/zero-shot-implicit-gpt2"
      ]
    },
    {
      "architecture_id": "PhoneLMForCausalLM",
      "total_models": 3,
      "sample_models": [
        "mllmTeam/PhoneLM-0.5B",
        "mllmTeam/PhoneLM-0.5B-Instruct",
        "mllmTeam/PhoneLM-1.5B-Call"
      ]
    },
    {
      "architecture_id": "HelloWorldModel",
      "total_models": 3,
      "sample_models": [
        "berwart/Inteligent_ai",
        "chiedo/hello-world",
        "vinothkannans/hello-world"
      ]
    },
    {
      "architecture_id": "LlavaJambaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "FreedomIntelligence/LongLLaVA-53B-A13B",
        "FreedomIntelligence/LongLLaVA-9B",
        "FreedomIntelligence/LongLLaVAMed-9B"
      ]
    },
    {
      "architecture_id": "LlavaQWenForCausalLM",
      "total_models": 3,
      "sample_models": [
        "LanguageBind/MoE-LLaVA-Qwen-Stage2",
        "LINs-lab/DynMoE-Qwen-1.8B",
        "power0341/llava-v1_5-mlp2x-336px-qwen1_8b"
      ]
    },
    {
      "architecture_id": "YoutuForCausalLM",
      "total_models": 3,
      "sample_models": [
        "tencent/Youtu-LLM-2B-Base",
        "mlx-community/Youtu-LLM-2B-4bit",
        "mlx-community/Youtu-LLM-2B"
      ]
    },
    {
      "architecture_id": "KlearMoeForCausalLM",
      "total_models": 3,
      "sample_models": [
        "Kwai-Klear/Klear-46B-A2.5B-Base",
        "Kwai-Klear/Klear-46B-A2.5B-Instruct",
        "mlx-community/Klear-46B-A2.5B-Instruct-3bit"
      ]
    },
    {
      "architecture_id": "LongcatFlashOmniForCausalLM",
      "total_models": 3,
      "sample_models": [
        "meituan-longcat/LongCat-Flash-Omni",
        "meituan-longcat/LongCat-Flash-Omni-FP8",
        "aiqtech/LongCat-Flash-Omni"
      ]
    },
    {
      "architecture_id": "Beit3LlavaLlamaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "openbmb/RLHF-V",
        "openbmb/RLHF-V-SFT",
        "Yirany/Muffin-13B"
      ]
    },
    {
      "architecture_id": "TTTForCausalLM",
      "total_models": 3,
      "sample_models": [
        "alan918727/ttt-125M-TinyStories",
        "amy-hyunji-lee/ttt-linear-125m-books-2k",
        "amy-hyunji-lee/ttt-linear-1.3b-books-32k"
      ]
    },
    {
      "architecture_id": "i3HybridModel",
      "total_models": 3,
      "sample_models": [
        "i3-lab/i3-Ethan-Base",
        "i3-lab/i3-200m-v2",
        "i3-lab/i3-500m"
      ]
    },
    {
      "architecture_id": "ApolloForCausalLM",
      "total_models": 3,
      "sample_models": [
        "GoodiesHere/Apollo-LMMs-Apollo-3B-t32",
        "Sri-Vigneshwar-DJ/Apollo-LMMs-Apollo-7B-t32",
        "Sri-Vigneshwar-DJ/Apollo-LMMs-Apollo-3B-t32"
      ]
    },
    {
      "architecture_id": "LlavaQwen3ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "markendo/llava-extract-qwen3-1.7B",
        "markendo/llava-extract-qwen3-0.6B",
        "markendo/llava-extract-from-scratch-qwen3-1.7B"
      ]
    },
    {
      "architecture_id": "ErniePixelForCausalLM",
      "total_models": 3,
      "sample_models": [
        "ernie-research/PixelGPT",
        "ernie-research/DualGPT",
        "ernie-research/MonoGPT"
      ]
    },
    {
      "architecture_id": "VLMForCausalLM",
      "total_models": 3,
      "sample_models": [
        "unum-cloud/uform-gen-chat",
        "mirodavide/vlm-vqa",
        "4bit/uform-gen"
      ]
    },
    {
      "architecture_id": "ChatUniViLlamaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "Chat-UniVi/Chat-UniVi-13B",
        "Chat-UniVi/Chat-UniVi-7B-v1.5",
        "Chat-UniVi/Chat-UniVi-ScienceQA"
      ]
    },
    {
      "architecture_id": "TraVisionForCausalLM",
      "total_models": 3,
      "sample_models": [
        "ucsahin/TraVisionLM-base",
        "ucsahin/TraVisionLM-DPO",
        "ucsahin/TraVisionLM-Object-Detection-ft"
      ]
    },
    {
      "architecture_id": "Cohere2Model",
      "total_models": 3,
      "sample_models": [
        "Lumia101/c4ai-command-r7b-12-2024-bnb-4bit",
        "sh0ck0r/Agatha-111B-v1-FP8-Dynamic",
        "sh0ck0r/Agatha-111B-v1.1-FP8-Dynamic"
      ]
    },
    {
      "architecture_id": "ModuleFormerForCausalLM",
      "total_models": 3,
      "sample_models": [
        "ibm-research/MoLM-350M-4B",
        "ibm-research/MoLM-700M-4B",
        "ibm-research/MoLM-700M-8B"
      ]
    },
    {
      "architecture_id": "QH360_VL_LlamaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "ecfirst/360VL_PHI",
        "qihoo360/360VL-8B",
        "qihoo360/360VL-70B"
      ]
    },
    {
      "architecture_id": "MplugOwlForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "MAGAer13/mplug-owl-llama-7b-ft",
        "Mizukiluke/ureader-v1",
        "cnut1648/mplug-7b-ft-0814"
      ]
    },
    {
      "architecture_id": "ShivikM4ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "ziadrone/Shivik-M4-Reasoning-Packed",
        "ziadrone/Shivik-M4-Reasoning-SFT",
        "ziadrone/Shivik-M4.1"
      ]
    },
    {
      "architecture_id": "InternvlFlashForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "lyx98/InternVL3_5_Flash-2B-HF",
        "lyx98/InternVL3_5_Flash-1B-HF",
        "lyx98/InternVL3_5_Flash-4B-HF"
      ]
    },
    {
      "architecture_id": "MuToRGemmaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "nasos10/MuToR-gemma-2B-GSM8K-dmax_4_a_03",
        "nasos10/MuToR-gemma-2B-1M_GSM-dmax_4_a_03",
        "nasos10/MuToR-gemma-2B-1M_MATH-dmax_3_a_02"
      ]
    },
    {
      "architecture_id": "GPTModel",
      "total_models": 3,
      "sample_models": [
        "bitlabsdb/gpt2-124m-transformer_model",
        "lemms/openllm-small-extended-6k",
        "lemms/openllm-small-extended-7k"
      ]
    },
    {
      "architecture_id": "MobilintLlamaBatchForCausalLM",
      "total_models": 3,
      "sample_models": [
        "mobilint/Llama-3.2-3B-Instruct-Batch16",
        "mobilint/Llama-3.1-8B-Instruct-Batch16",
        "mobilint/Llama-3.1-8B-Instruct-Batch32"
      ]
    },
    {
      "architecture_id": "modeling_llama_butler.LlamaButlerForCausalLM",
      "total_models": 3,
      "sample_models": [
        "akhauriyash/Llama-3.2-1B-Butler",
        "akhauriyash/Llama-3.1-8B-Butler",
        "akhauriyash/Llama-2-7b-hf-Butler"
      ]
    },
    {
      "architecture_id": "TPUGemma3ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "benjamin/gemma-3-1b-it-flax",
        "benjamin/gemma-3-1b-pt-flax",
        "benjamin/gemma-3-12b-it-flax"
      ]
    },
    {
      "architecture_id": "LongformerEncoderDecoderForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "nguyenkhoa/LongBartRecipe1M",
        "zedfum/arman-longformer-8k",
        "nguyenkhoa/bart_longformer"
      ]
    },
    {
      "architecture_id": "LlamaForSequenceClassification",
      "total_models": 3,
      "sample_models": [
        "leptonai/Llama-3.2-1B-Instruct-Regression-Test",
        "summerstars/Solara-deepMATH",
        "abdoelsayed/dear-8b-reranker-listwise-v1"
      ]
    },
    {
      "architecture_id": "MGMLlamaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "YanweiLi/MGM-8B",
        "HuanjinYao/DenseConnector-with-mgm-7B",
        "YanweiLi/MGM-8B-HD"
      ]
    },
    {
      "architecture_id": "AlignGPTForCausalLM",
      "total_models": 3,
      "sample_models": [
        "nlpzhaof/aligngpt-7b",
        "nlpzhaof/aligngpt-13b",
        "ernestoBocini/ITClass_LLavA_ft"
      ]
    },
    {
      "architecture_id": "OpenThaiWilaiForCausalLM",
      "total_models": 3,
      "sample_models": [
        "OpensourceThai/Wilai",
        "ZombitX64/Wilai-1.5",
        "JonusNattapong/OpenThaiWilai"
      ]
    },
    {
      "architecture_id": "SiQ_VLForCausalLM",
      "total_models": 3,
      "sample_models": [
        "classtag/siq-vl_siglip2-large-patch16-512_qwen2.5-1.5b-instruct_stage1",
        "classtag/siq-vl_siglip2-large-patch16-512_qwen3-1.7b_stage1",
        "classtag/siq-vl_siglip2-so400m-patch14-224_qwen3-0.6b-base_stage1"
      ]
    },
    {
      "architecture_id": "CustomPegasusForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "nloc2578/QAG_Pegasus_2ep",
        "nloc2578/QAG_Pegasus_2ep_eval",
        "nloc2578/QAG_Pegasus"
      ]
    },
    {
      "architecture_id": "ReAttForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "neulab/reatt-large-nq",
        "neulab/reatt-large-nq-fiqa",
        "neulab/reatt-large-nq-bioasq"
      ]
    },
    {
      "architecture_id": "LSTLlamaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "YouAreSpecialToMe/QST-70B-checkpoint",
        "tennant/LSTLlama-2-13b-v0.0.1-pdl-m16-a4-ep2",
        "tennant/LSTLlama-2-13b-v0.0.1-pdl_flan-m16-a4-ep2"
      ]
    },
    {
      "architecture_id": "PlusModelForCausalLM",
      "total_models": 3,
      "sample_models": [
        "crumb/ParaLlama-p-medium",
        "crumb/ParaLlama-p-small",
        "crumb/ParaLlama-p-micro"
      ]
    },
    {
      "architecture_id": "MonoFormerForCausalLM",
      "total_models": 3,
      "sample_models": [
        "Raniahossam33/monoformer-audio",
        "Raniahossam33/monoformer_ultrachat_72k",
        "Raniahossam33/monoformer_ultrachat_72k_ema"
      ]
    },
    {
      "architecture_id": "Phi4FlashForCausalLM",
      "total_models": 3,
      "sample_models": [
        "solnoman/testt",
        "ronx-labs/affine-phi",
        "tiny-random/phi-4-flash"
      ]
    },
    {
      "architecture_id": "TranceptionLMHeadModel",
      "total_models": 3,
      "sample_models": [
        "PascalNotin/Tranception_Small",
        "PascalNotin/Tranception_Medium",
        "PascalNotin/Tranception_Large"
      ]
    },
    {
      "architecture_id": "WrappedLlamav2ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "wolfgangshen/vicuna-7b-16k-musicai_v0",
        "wolfgangshen/llama2-7b-musicai_v0",
        "wolfgangshen/llama2-7b-musicai_v1"
      ]
    },
    {
      "architecture_id": "MolLLaMA",
      "total_models": 3,
      "sample_models": [
        "DongkiKim/Mol-Llama-3.1-8B-Instruct",
        "DongkiKim/Mol-Llama-3.1-8B-Instruct-Full-Weights",
        "DongkiKim/Mol-Llama-2-7b-chat"
      ]
    },
    {
      "architecture_id": "LumeesForCausalLM",
      "total_models": 3,
      "sample_models": [
        "lumees/lumees-362m-base",
        "lumees/lumees-177m-base",
        "lumees/Lumees-3.8B-Reasoning"
      ]
    },
    {
      "architecture_id": "Qwen3ForGuardModel",
      "total_models": 3,
      "sample_models": [
        "abnormalmapstudio/Qwen3Guard-Stream-0.6B-mxfp4-mlx",
        "abnormalmapstudio/Qwen3Guard-Stream-8B-mxfp4-mlx",
        "abnormalmapstudio/Qwen3Guard-Stream-4B-mxfp4-mlx"
      ]
    },
    {
      "architecture_id": "LMHeadWithValueModel",
      "total_models": 3,
      "sample_models": [
        "anshr/distilgpt2_trained_policy_model_02",
        "anshr/distilgpt2_trained_policy_model_01",
        "anshr/distilgpt2_trained_policy_model_final"
      ]
    },
    {
      "architecture_id": "PipelinedGPT2LMHeadModel",
      "total_models": 3,
      "sample_models": [
        "nmb-paperspace-hf/gpt2-wikitext2",
        "graphcore-rahult/gpt2-wikitext2",
        "SonLam/gpt2-wikitext2"
      ]
    },
    {
      "architecture_id": "MyModelForCausalLM",
      "total_models": 3,
      "sample_models": [
        "ydshieh/Gemma_4_Future_Tokens",
        "joelb/Mixtral-8x7B-1l",
        "ydshieh/LLama_4_Future_Tokens"
      ]
    },
    {
      "architecture_id": "MidmLMHeadModel",
      "total_models": 3,
      "sample_models": [
        "Nagase-Kotono/midm-bitext-S-7B-koAlpaca-qlora-1000step-test",
        "jangmin/midm-7b-safetensors-only",
        "jangmin/merged-midm-7B-food-order-understanding-30K"
      ]
    },
    {
      "architecture_id": "SliMMForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "menglc/SliMM-DeepStackE-Qwen2VL-2B",
        "menglc/SliMM-Qwen2-0.5B",
        "menglc/SliMM-DeepStackM-Qwen2-0.5B"
      ]
    },
    {
      "architecture_id": "Llama2BiasModel",
      "total_models": 3,
      "sample_models": [
        "Grogros/Grogros-llama2-7b-hf-unremovable-std0.6-key0-ft-OpenMathInstruct",
        "Grogros/Grogros-llama2-7b-hf-unremovable-std0.6-key0-ft-learnability_adv",
        "Grogros/llama2-7b-hf-unremovable-std0.6-key0"
      ]
    },
    {
      "architecture_id": "MoVELlavaLlamaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "jiaojuncao/MoVE-KD-13b-v1.1",
        "jiaojuncao/MoVE-KD-7b-v1.0",
        "jiaojuncao/MoVE-KD-7b-v1.1"
      ]
    },
    {
      "architecture_id": "SapnousT1ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "Sapnous-AI/Sapnous-VR-6B",
        "Sapnous-AI/Sapnous-VR-47B",
        "Sapnous-AI/Sapnous-VR-12B"
      ]
    },
    {
      "architecture_id": "Qwen2VLAudioForConditionalGeneration",
      "total_models": 3,
      "sample_models": [
        "OscarGD6/audio-command-VQA-dev",
        "OscarGD6/qwen2-vl-audio-prompt-asr-qformer-512-queries",
        "OscarGD6/qwen2-vl-audio-compress-coco"
      ]
    },
    {
      "architecture_id": "T5EncoderModel",
      "total_models": 3,
      "sample_models": [
        "exdysa/DeepFloyd-t5-v1_1-xxl-SAFETENSORS",
        "mingyi456/t5-v1_1-xxl-DF11",
        "mingyi456/t5-v1_1-xxl-fp16-DF11"
      ]
    },
    {
      "architecture_id": "MoLAForCausalLM",
      "total_models": 3,
      "sample_models": [
        "MoLA-LLM/MoLA-v0.5-9x4b",
        "MoLA-LLM/MoLA-v0.7-8x4b",
        "MoLA-LLM/MoLA-v0.6-9x4b"
      ]
    },
    {
      "architecture_id": "Gemma3TextModel",
      "total_models": 3,
      "sample_models": [
        "trollek/ImagePromptHelper-gemma3-270M",
        "TamWaiban/gemma-3-270m-autoquant",
        "TamWaiban/gemma-3-270m-int4"
      ]
    },
    {
      "architecture_id": "MambaModelForCausalLM",
      "total_models": 3,
      "sample_models": [
        "mjschock/mamba-790m",
        "mjschock/mamba-1.4b",
        "mjschock/mamba-130m-ppo"
      ]
    },
    {
      "architecture_id": "BigBirdPegasusForCausalLM",
      "total_models": 3,
      "sample_models": [
        "RichardErkhov/pszemraj_-_bigbird-pegasus-large-K-booksum-4bits",
        "hf-tiny-model-private/tiny-random-BigBirdPegasusForCausalLM",
        "RichardErkhov/pszemraj_-_bigbird-pegasus-large-K-booksum-8bits"
      ]
    },
    {
      "architecture_id": "PhimoeForCausalLM",
      "total_models": 3,
      "sample_models": [
        "rkumar1999/Phi-mini-MoE-Prover-openr1-distill-SFT",
        "rkumar1999/Phi-mini-MoE-Prover-Math-openr1-distill-SFT",
        "rkumar1999/Phi-mini-MoE-Mix-Prover-openr1-distill-SFT"
      ]
    },
    {
      "architecture_id": "YsnrfdForCausalLM",
      "total_models": 3,
      "sample_models": [
        "ysn-rfd/ysnrfd-base-V2",
        "ysn-rfd/ysnrfd-base",
        "ysn-rfd/ysnrfd-base-V3"
      ]
    },
    {
      "architecture_id": "QTSplusQwen2_5_VLTextForCausalLM",
      "total_models": 3,
      "sample_models": [
        "AlpachinoNLP/QTSplus-3B-FT",
        "AlpachinoNLP/QTSplus-7B",
        "AlpachinoNLP/QTSplus-3B"
      ]
    },
    {
      "architecture_id": "DragonflyForCausalLM",
      "total_models": 3,
      "sample_models": [
        "SillyTilly/LLama-3-Dragonfly-Med",
        "togethercomputer/Llama-3-8B-Dragonfly-Med-v1",
        "togethercomputer/Llama-3-8B-Dragonfly-v1"
      ]
    },
    {
      "architecture_id": "SMModelForCausalLM",
      "total_models": 3,
      "sample_models": [
        "cmykk/gemma-2-2b-fips-pb50",
        "cmykk/gemma-2-2b-fips-pb60",
        "cmykk/gemma-2-2b-fips-pb75"
      ]
    },
    {
      "architecture_id": "KeystoneFuseForCausalLM",
      "total_models": 3,
      "sample_models": [
        "Hyper-AI-Computer/KeystoneFuse-Init",
        "Hyper-AI-Computer/KeystoneFuse-Base-Baseline-001-Test-DP-FSDP",
        "Hyper-AI-Computer/KeystoneFuse-Base-Baseline-001-Pretrain"
      ]
    },
    {
      "architecture_id": "MeshModel",
      "total_models": 3,
      "sample_models": [
        "mesh-labs/v0.1-2x2-stage002",
        "mesh-labs/v0.1-2x2-stage003",
        "mesh-labs/v0.1-2x2-stage001"
      ]
    },
    {
      "architecture_id": "LlavaQwen1_5ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "tuanio/ft-lora-llavaqwen1.5-1.8b-complex_reasoning",
        "tuanio/llavaqwen1.5-1.8b-ft-mergedlora",
        "tuanio/ft-llavaqwen1.5-1.8b-complex_reasoning-merged"
      ]
    },
    {
      "architecture_id": "GPT2MIMOLMHeadModel",
      "total_models": 3,
      "sample_models": [
        "ammarnasr/mimo-ver-0.0.1",
        "ammarnasr/gpt2mimo-config",
        "ammarnasr/gpt2mimo-lmhead"
      ]
    },
    {
      "architecture_id": "OffsetLlamaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "nasos10/MuToR-llama3-8B-1M_GSM-dmax_5_a_01",
        "nasos10/MuToR-llama3-8B-GSM8K-dmax_4_a_03",
        "nasos10/MuToR-llama3-8B-1M_MATH-dmax_4_a_01"
      ]
    },
    {
      "architecture_id": "DenseformerForCausalLM",
      "total_models": 3,
      "sample_models": [
        "gudleifrr/denseformer_150M_d1_p1_202",
        "gudleifrr/denseformer_746",
        "gudleifrr/denseformer_150M_d2_p2_202"
      ]
    },
    {
      "architecture_id": "FSDPQwen3ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "hungphongtrn/Qwen3-1.7B-proactive-1610",
        "kevinshin/test-run-fsdp-v2-full-state-dict",
        "thehosy/hsthe-1.4b-base-ckpt"
      ]
    },
    {
      "architecture_id": "LlavaMiniLlamaForCausalLM",
      "total_models": 3,
      "sample_models": [
        "sumo43/llava-mini-llama-3.1-8b-r1-merge-0.5_1.0",
        "sumo43/llava-mini-llama-3.1-8b-r1-merge-0.5_0.5",
        "sumo43/llava-mini-llama-3.1-8b-r1-merge-1.0_0.5"
      ]
    },
    {
      "architecture_id": "Gemma2Model",
      "total_models": 3,
      "sample_models": [
        "lapfed255/gemma-2-9b-it-bnb-4bit",
        "abheekga/gemma-2-2b-it-bnb-4bit",
        "jp1924/gemma-2-9b-national-corpus"
      ]
    },
    {
      "architecture_id": "CWICForCausalLM",
      "total_models": 3,
      "sample_models": [
        "crystal-ai/CWICLlama-3.2-1B-A620M-Instruct",
        "crystal-ai/CWICLlama-3.2-1B-A310M-Instruct",
        "crystal-ai/CWICLlama-3.2-1B-A206M-Instruct"
      ]
    },
    {
      "architecture_id": "NGen4ForCausalLM",
      "total_models": 3,
      "sample_models": [
        "TNSA/NGen4-Base-Exp-a-05-08-2025-pre",
        "TNSA/NGen4-Base-Exp-b-05-08-2025-pre",
        "TNSA/NGen4-Base-Exp-c-05-08-2025-pre"
      ]
    },
    {
      "architecture_id": "CableModel",
      "total_models": 3,
      "sample_models": [
        "axiomlaborg/cable-wiki-tiny-512",
        "axiomlaborg/cable-wiki-tiny-1024",
        "axiomlaborg/cable-edufineweb-md-512"
      ]
    },
    {
      "architecture_id": "HunYuanVLForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "tencent/HunyuanOCR",
        "metanthropic/MahenOCR-1B"
      ]
    },
    {
      "architecture_id": "MonkeyLMHeadModel",
      "total_models": 2,
      "sample_models": [
        "echo840/Monkey-Chat",
        "echo840/Monkey"
      ]
    },
    {
      "architecture_id": "HCXVisionForCausalLM",
      "total_models": 2,
      "sample_models": [
        "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B",
        "OpenLLM-Korea/HyperCLOVAX-SEED-Vision-Instruct-3B"
      ]
    },
    {
      "architecture_id": "Step3VLForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "stepfun-ai/step3",
        "tiny-random/step3-vllm"
      ]
    },
    {
      "architecture_id": "H2OVLChatModel",
      "total_models": 2,
      "sample_models": [
        "h2oai/h2ovl-mississippi-800m",
        "h2oai/h2ovl-mississippi-2b"
      ]
    },
    {
      "architecture_id": "JetNemotronForCausalLM",
      "total_models": 2,
      "sample_models": [
        "jet-ai/Jet-Nemotron-2B",
        "jet-ai/Jet-Nemotron-4B"
      ]
    },
    {
      "architecture_id": "InternS1ForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "internlm/Intern-S1",
        "internlm/Intern-S1-mini"
      ]
    },
    {
      "architecture_id": "HCXVisionV2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "naver-hyperclovax/HyperCLOVAX-SEED-Think-32B",
        "naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B"
      ]
    },
    {
      "architecture_id": "Grok1ModelForCausalLM",
      "total_models": 2,
      "sample_models": [
        "hpcai-tech/grok-1",
        "yujiepan/grok-1-tiny-random"
      ]
    },
    {
      "architecture_id": "InternLMXComposerForCausalLM",
      "total_models": 2,
      "sample_models": [
        "internlm/internlm-xcomposer-7b",
        "internlm/internlm-xcomposer-vl-7b"
      ]
    },
    {
      "architecture_id": "StarVectorForCausalLM",
      "total_models": 2,
      "sample_models": [
        "starvector/starvector-8b-im2svg",
        "mrfakename/starvector-starvector-8b-im2svg"
      ]
    },
    {
      "architecture_id": "VMistralForVisionText2Text",
      "total_models": 2,
      "sample_models": [
        "HuggingFaceM4/VLM_WebSight_finetuned",
        "Jaykintecblic/Html"
      ]
    },
    {
      "architecture_id": "IQuestLoopCoderForCausalLM",
      "total_models": 2,
      "sample_models": [
        "IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct",
        "mlx-community/IQuest-Coder-V1-40B-Loop-Instruct-4bit"
      ]
    },
    {
      "architecture_id": "Sarashina2VisionForCausalLM",
      "total_models": 2,
      "sample_models": [
        "sbintuitions/sarashina2.2-vision-3b",
        "sbintuitions/sarashina2-vision-14b"
      ]
    },
    {
      "architecture_id": "DeepseekVLV2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Isotr0py/deepseek-vl2-tiny",
        "yzhang0112/tw-longtail-recall-deepseek-vl2-tiny-lora"
      ]
    },
    {
      "architecture_id": "MaincoderForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Maincode/Maincoder-1B",
        "Maincode/Maincoder-1B-ONNX"
      ]
    },
    {
      "architecture_id": "MoshiForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "kmhf/hf-moshiko",
        "kmhf/hf-moshika"
      ]
    },
    {
      "architecture_id": "AX4VLForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "skt/A.X-4.0-VL-Light",
        "OpenLLM-Korea/A.X-4.0-VL-Light"
      ]
    },
    {
      "architecture_id": "GeoChatLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "MBZUAI/geochat-7B",
        "ll-13/SkySenseGPT-7B-CLIP-ViT"
      ]
    },
    {
      "architecture_id": "CheXagentForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "StanfordAIMI/CheXagent-8b",
        "Neptune1722/CheXagent-nf4-quantized"
      ]
    },
    {
      "architecture_id": "XMistralForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Hannibal046/xrag-7b",
        "Hannibal046/xrag-v1.1-7b"
      ]
    },
    {
      "architecture_id": "BunnyQwenForCausalLM",
      "total_models": 2,
      "sample_models": [
        "mlx-community/dolphin-vision-72b-4bit",
        "qnguyen3/nanoLLaVA-1.5"
      ]
    },
    {
      "architecture_id": "VARGPTQwen2VLForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "VARGPT-family/VARGPT-v1.1",
        "VARGPT-family/VARGPT-v1.1-edit"
      ]
    },
    {
      "architecture_id": "ZambaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Zyphra/Zamba-7B-v1",
        "Zyphra/Zamba-7B-v1-phase1"
      ]
    },
    {
      "architecture_id": "TeleFLMForCausalLM",
      "total_models": 2,
      "sample_models": [
        "CofeAI/Tele-FLM-1T",
        "nqzfaizal77ai/fei-lian-reinit-567m-zero"
      ]
    },
    {
      "architecture_id": "Eagle3DraftModel",
      "total_models": 2,
      "sample_models": [
        "RedHatAI/Qwen3-235B-A22B-Instruct-2507-speculator.eagle3",
        "RedHatAI/Qwen3-30B-A3B-Instruct-2507-speculator.eagle3"
      ]
    },
    {
      "architecture_id": "Kanana2VecModel",
      "total_models": 2,
      "sample_models": [
        "kakaocorp/kanana-nano-2.1b-embedding",
        "OpenLLM-Korea/kanana-nano-2.1b-embedding"
      ]
    },
    {
      "architecture_id": "TinyllmForCausalLM",
      "total_models": 2,
      "sample_models": [
        "wdndev/tiny_llm_sft_92m",
        "wdndev/tiny_llm_sft_76m_llama"
      ]
    },
    {
      "architecture_id": "AquilaDenseForCausalLM",
      "total_models": 2,
      "sample_models": [
        "BAAI/AquilaDense-7B",
        "BAAI/AquilaDense-16B"
      ]
    },
    {
      "architecture_id": "RWKV6ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "fla-hub/rwkv6-1.6B-finch",
        "fla-hub/rwkv6-7B-finch"
      ]
    },
    {
      "architecture_id": "DiCoWForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "BUT-FIT/DiCoW_v1",
        "BUT-FIT/DiCoW_v3"
      ]
    },
    {
      "architecture_id": "LlamaForCausalLMEagle",
      "total_models": 2,
      "sample_models": [
        "thunlp/LLaMA3-Instruct-8B-FR-Spec",
        "thunlp/LLaMA3.2-Instruct-1B-FR-Spec"
      ]
    },
    {
      "architecture_id": "SMTModelForCausalLM",
      "total_models": 2,
      "sample_models": [
        "PRAIG/smt-grandstaff",
        "antoniorv6/mozarteum_synthetic"
      ]
    },
    {
      "architecture_id": "CeruleGemmaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Tensoic/Cerule-v0.1",
        "Tensoic/Cerule-backup"
      ]
    },
    {
      "architecture_id": "Qwen3OmniMoeForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "abnormalmapstudio/Qwen3-Omni-30B-A3B-Instruct-mxfp4-mlx",
        "MagistrTheOne/Radon-35B-Ultra-X-RU"
      ]
    },
    {
      "architecture_id": "MGMOmniForCausalLM",
      "total_models": 2,
      "sample_models": [
        "wcy1122/MGM-Omni-7B",
        "wcy1122/MGM-Omni-32B"
      ]
    },
    {
      "architecture_id": "GitJapaneseStableLMAlphaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "turing-motors/heron-chat-git-ja-stablelm-base-7b-v0",
        "turing-motors/heron-chat-git-ja-stablelm-base-7b-v1"
      ]
    },
    {
      "architecture_id": "ShrinkModelForCausalLM",
      "total_models": 2,
      "sample_models": [
        "crumb/shrink-v1",
        "crumb/shrink-init"
      ]
    },
    {
      "architecture_id": "PanguEmbeddedForCausalLM",
      "total_models": 2,
      "sample_models": [
        "FreedomIntelligence/openPangu-Embedded-7B",
        "FreedomIntelligence/openPangu-Embedded-1B-V1.1"
      ]
    },
    {
      "architecture_id": "DecoderOnlyT5Model",
      "total_models": 2,
      "sample_models": [
        "google/madlad400-8b-lm",
        "jbochi/madlad400-8b-lm"
      ]
    },
    {
      "architecture_id": "DebertaV2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "ltg/deberta-xxlarge-fixed",
        "tailocbmt123/deberta-xxlarge-fixed"
      ]
    },
    {
      "architecture_id": "SpatialLMLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "manycore-research/SpatialLM1.1-Llama-1B",
        "manycore-research/SpatialLM-Llama-1B"
      ]
    },
    {
      "architecture_id": "Olmo2ForSequenceClassification",
      "total_models": 2,
      "sample_models": [
        "allenai/OLMo-2-1124-7B-RM",
        "allenai/OLMo-2-1124-13B-RM"
      ]
    },
    {
      "architecture_id": "StreamVLNForCausalLM",
      "total_models": 2,
      "sample_models": [
        "mengwei0427/StreamVLN_Video_qwen_1_5_r2r_rxr_envdrop_scalevln",
        "mengwei0427/StreamVLN_Video_qwen_1_5_r2r_rxr_envdrop_scalevln_v1_3"
      ]
    },
    {
      "architecture_id": "SDLMQwen2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "OpenGVLab/SDLM-3B-D8",
        "OpenGVLab/SDLM-3B-D4"
      ]
    },
    {
      "architecture_id": "NilexForCausalLM",
      "total_models": 2,
      "sample_models": [
        "MBZUAI-Paris/Nile-Chat-3x4B-A6B",
        "MBZUAI-Paris/Nile-Chat-2x4B-A6B"
      ]
    },
    {
      "architecture_id": "ChemQ3MTPForCausalLM",
      "total_models": 2,
      "sample_models": [
        "gbyuvd/ChemMiniQ3-SAbRLo",
        "gbyuvd/ChemQ3MTP-base"
      ]
    },
    {
      "architecture_id": "KoreanLLMForCausalLM",
      "total_models": 2,
      "sample_models": [
        "shopkeeper/korean-wiki-120125",
        "shopkeeper/korean-wiki-1207_070319"
      ]
    },
    {
      "architecture_id": "DeCodon",
      "total_models": 2,
      "sample_models": [
        "goodarzilab/decodon-200M",
        "goodarzilab/decodon-200M-euk"
      ]
    },
    {
      "architecture_id": "CoDALanguageModel",
      "total_models": 2,
      "sample_models": [
        "Salesforce/CoDA-v0-Instruct",
        "Salesforce/CoDA-v0-Base"
      ]
    },
    {
      "architecture_id": "KayraForCausalLM",
      "total_models": 2,
      "sample_models": [
        "sixfingerdev/kayra-1-exp",
        "sixfingerdev/kayra-1"
      ]
    },
    {
      "architecture_id": "DiffusionVL_Qwen2_5_VL_ForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "hustvl/DiffusionVL-Qwen2.5VL-7B",
        "hustvl/DiffusionVL-Qwen2.5VL-3B"
      ]
    },
    {
      "architecture_id": "VisionEncoderDecoderModel",
      "total_models": 2,
      "sample_models": [
        "QuickHawk/trocr-indic",
        "ndileep/vit-gpt2-caption-model"
      ]
    },
    {
      "architecture_id": "GistLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "jayelm/llama-7b-gist-1",
        "yoojus/gist_maxlength256_numgist1"
      ]
    },
    {
      "architecture_id": "Glm4vForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "EasyDeL/GLM-4.6V-Flash",
        "QuantTrio/GLM-4.1V-9B-Thinking-GPTQ-Int4-Int8Mix"
      ]
    },
    {
      "architecture_id": "OBILanguageModel",
      "total_models": 2,
      "sample_models": [
        "aframson/RDPD-mini",
        "aframson/RDPDLM"
      ]
    },
    {
      "architecture_id": "OmniLMMForCausalLM",
      "total_models": 2,
      "sample_models": [
        "openbmb/OmniLMM-12B",
        "openbmb/RLAIF-V-12B"
      ]
    },
    {
      "architecture_id": "EmuForCausalLM",
      "total_models": 2,
      "sample_models": [
        "BAAI/Emu2-Chat",
        "BAAI/Emu2"
      ]
    },
    {
      "architecture_id": "MobilintBertForMaskedLM",
      "total_models": 2,
      "sample_models": [
        "mobilint/bert-kor-base",
        "mobilint/bert-base-uncased"
      ]
    },
    {
      "architecture_id": "TinyLlavaPhiForCausalLM",
      "total_models": 2,
      "sample_models": [
        "bczhou/TinyLLaVA-3.1B",
        "bczhou/TinyLLaVA-3.1B-Pretrain"
      ]
    },
    {
      "architecture_id": "KirimForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Kirim-ai/Kirim-1-Math",
        "Kirim-ai/Kirim-Safeguard-R1-10B"
      ]
    },
    {
      "architecture_id": "TinyLlavaLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "bczhou/TinyLLaVA-1.5B",
        "RichardLuo/Shotluck-Holmes-1.5"
      ]
    },
    {
      "architecture_id": "CostWiseMistralForCausalLM",
      "total_models": 2,
      "sample_models": [
        "BAAI/Matroyshka-ReRanker-passage",
        "cfli/Matroyshka-ReRanker-passage"
      ]
    },
    {
      "architecture_id": "SPILlavaMPTForCausalLM",
      "total_models": 2,
      "sample_models": [
        "shilongz/GPT4RoI-7B-delta-V0",
        "shilongz/debug"
      ]
    },
    {
      "architecture_id": "TransformerModel",
      "total_models": 2,
      "sample_models": [
        "zhangux/TinyChat-0.1B-pretrain",
        "zhangux/TinyChat-0.1B-sft"
      ]
    },
    {
      "architecture_id": "MinGRULMForCausalLM",
      "total_models": 2,
      "sample_models": [
        "suayptalha/minGRU-LM",
        "suayptalha/minGRULM-base"
      ]
    },
    {
      "architecture_id": "Qwen2ForCausalLMPostBlockSteeringFixed",
      "total_models": 2,
      "sample_models": [
        "atrost/test_steerable_hf_model_v4",
        "atrost/test_steerable_hf_model_v3"
      ]
    },
    {
      "architecture_id": "SrcProberForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "PurCL/src_prober_codellama-13b-last1unfreeze",
        "PurCL/src_prober_codellama-34b-last1unfreeze"
      ]
    },
    {
      "architecture_id": "EVELlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "BAAI/EVE-7B-v1.0",
        "BAAI/EVE-7B-Pretrain-v1.0"
      ]
    },
    {
      "architecture_id": "RobertaForMaskedLM",
      "total_models": 2,
      "sample_models": [
        "huynguyen251/finetuned-phobert-law",
        "nelson2424/distilroberta-base-finetuned-cot"
      ]
    },
    {
      "architecture_id": "LOLALMHeadModel",
      "total_models": 2,
      "sample_models": [
        "dice-research/lola_v1",
        "dice-research/lola_v1_alpaca_instructions"
      ]
    },
    {
      "architecture_id": "LlavaQwenSlowFastForCausalLM",
      "total_models": 2,
      "sample_models": [
        "shi-labs/slowfast-video-mllm-qwen2-7b-convnext-576-frame64-s1t4",
        "shi-labs/slowfast-video-mllm-qwen2-7b-convnext-576-frame96-s1t6"
      ]
    },
    {
      "architecture_id": "JudgeXL",
      "total_models": 2,
      "sample_models": [
        "Wonder-Griffin/XL-Judge-LLM",
        "Wonder-Griffin/judge-xl-model"
      ]
    },
    {
      "architecture_id": "SimpleStories4MModel",
      "total_models": 2,
      "sample_models": [
        "broskicodes/simple-stories-4M",
        "aimlessdrivel/test"
      ]
    },
    {
      "architecture_id": "GiddForDiffusionLM",
      "total_models": 2,
      "sample_models": [
        "dvruette/gidd-mask-3b",
        "dvruette/gidd-unif-3b"
      ]
    },
    {
      "architecture_id": "DDLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "xuan-luo/FlexiDepth-Llama-3-8B-Instruct",
        "xuan-luo/DiffSkip-Llama-3-8B-Instruct"
      ]
    },
    {
      "architecture_id": "omFlaxBartForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "apol/dalle-mini",
        "flax-community/dalle-mini"
      ]
    },
    {
      "architecture_id": "TaiVisionForCausalLM",
      "total_models": 2,
      "sample_models": [
        "benchang1110/TaiVisionLM-base-v1",
        "benchang1110/TaiVisionLM-base-v2"
      ]
    },
    {
      "architecture_id": "Qwen2AudioForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "yujiepan/qwen2-audio-tiny-random",
        "KasuleTrevor/Qwen2-Luganda-ASR"
      ]
    },
    {
      "architecture_id": "ArabicGPTModel",
      "total_models": 2,
      "sample_models": [
        "alphatechlogics/FaseehGPT",
        "codewithdark/ArabicLM"
      ]
    },
    {
      "architecture_id": "LlavaStablelmForCausalLM",
      "total_models": 2,
      "sample_models": [
        "LanguageBind/MoE-LLaVA-StableLM-Stage2",
        "LanguageBind/MoE-LLaVA-StableLM-Stage2-384"
      ]
    },
    {
      "architecture_id": "VoxtralForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "yujiepan/voxtral-tiny-random",
        "tiny-random/voxtral"
      ]
    },
    {
      "architecture_id": "CosmicFish",
      "total_models": 2,
      "sample_models": [
        "MistyozAI/CosmicFish-120M",
        "MistyozAI/CosmicFish-90M"
      ]
    },
    {
      "architecture_id": "LightbrainHybridForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Levelfive/Light-100m-untrained",
        "Levelfive/Light-100M-untrainedv0.01"
      ]
    },
    {
      "architecture_id": "CodifyForCausalLM",
      "total_models": 2,
      "sample_models": [
        "refactai/codify_medium_multi",
        "refactai/codify_3b_multi"
      ]
    },
    {
      "architecture_id": "MiniDeepSeekV3ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "WKQ9411/Mini-DeepSeekV3-160M-A100M-Base",
        "WKQ9411/Mini-DeepSeekV3-160M-A100M-SFT"
      ]
    },
    {
      "architecture_id": "MiniLlama3ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "WKQ9411/Mini-Llama3-100M-Base",
        "WKQ9411/Mini-Llama3-100M-SFT"
      ]
    },
    {
      "architecture_id": "PeftModelForCausalLM",
      "total_models": 2,
      "sample_models": [
        "mahimairaja/tweet-summarization-llama-2-finetuned",
        "oka19007/elyza_Llama-3-ELYZA-JP-8B-instruct-s"
      ]
    },
    {
      "architecture_id": "Step3vForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "yujiepan/step3-tiny-random",
        "tiny-random/step3"
      ]
    },
    {
      "architecture_id": "TinyWayForCausalLM",
      "total_models": 2,
      "sample_models": [
        "NNEngine/TinyWay-1.1.0",
        "NNEngine/TinyWay-1.0.0"
      ]
    },
    {
      "architecture_id": "Qwen3MoeFusedForCausalLM",
      "total_models": 2,
      "sample_models": [
        "woctordho/Qwen3-30B-A3B-fused",
        "woctordho/Qwen3-30B-A3B-fused-bnb-4bit"
      ]
    },
    {
      "architecture_id": "SerayukiForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Vynie/Serayuki-1B-v1.1-pre2-step-3k-1B",
        "SeraphyneLab/Serayuki-1B"
      ]
    },
    {
      "architecture_id": "CustomGPT2LMHeadModel",
      "total_models": 2,
      "sample_models": [
        "RyanJT/gpt2_gutenberg",
        "dimzhead/gpt2-test"
      ]
    },
    {
      "architecture_id": "CustomQwen2Model",
      "total_models": 2,
      "sample_models": [
        "AXCXEPT/EZO-AutoCoTRAG-Qwen2.5-32B-Instruct",
        "AXCXEPT/EZO-AutoCoTRAG-Qwen2.5-72B-Instruct_q4"
      ]
    },
    {
      "architecture_id": "YForCausalLM2",
      "total_models": 2,
      "sample_models": [
        "SnifferCaptain/YModel2-s-2",
        "SnifferCaptain/YModel2-s0"
      ]
    },
    {
      "architecture_id": "MMGPTQwenForCausalLM",
      "total_models": 2,
      "sample_models": [
        "HaoranWei/Vary-toy",
        "Chengzi0201/model"
      ]
    },
    {
      "architecture_id": "LlavaQwenZipForCausalLM",
      "total_models": 2,
      "sample_models": [
        "BBBBCHAN/LLaVA-Scissor-baseline-7B",
        "BBBBCHAN/LLaVA-Scissor-baseline-0.5B"
      ]
    },
    {
      "architecture_id": "RWModel",
      "total_models": 2,
      "sample_models": [
        "jinaai/falcon-40b-code-alpaca",
        "jinaai/falcon-7b-code-alpaca"
      ]
    },
    {
      "architecture_id": "OspreyLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "sunshine-lwt/Osprey-7b",
        "sunshine-lwt/Osprey-7b-stage2"
      ]
    },
    {
      "architecture_id": "i3Model",
      "total_models": 2,
      "sample_models": [
        "i3-lab/i3-22m",
        "i3-lab/i3-80m"
      ]
    },
    {
      "architecture_id": "SafeLlavaLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "etri-vilab/SafeLLaVA-7B",
        "etri-vilab/SafeLLaVA-13B"
      ]
    },
    {
      "architecture_id": "VCoderDSLlavaLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "shi-labs/vcoder_ds_llava-v1.5-13b",
        "shi-labs/vcoder_ds_llava-v1.5-7b"
      ]
    },
    {
      "architecture_id": "SESAMEForCausalLM",
      "total_models": 2,
      "sample_models": [
        "tsunghanwu/SESAME",
        "tsunghanwu/SESAME_minus"
      ]
    },
    {
      "architecture_id": "OryxQwenForCausalLM",
      "total_models": 2,
      "sample_models": [
        "THUdyh/Oryx-7B",
        "THUdyh/Oryx-1.5-32B"
      ]
    },
    {
      "architecture_id": "LlaaaLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "LinkSoul/LLaSM-Cllama2",
        "LinkSoul/LLaSM-Baichuan"
      ]
    },
    {
      "architecture_id": "HawkForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Rexopia/HawkLM-demo",
        "Rexopia/HawkLM-Chat-demo"
      ]
    },
    {
      "architecture_id": "MiniGeminiMixtralForCausalLM",
      "total_models": 2,
      "sample_models": [
        "YanweiLi/MGM-8x7B-HD",
        "YanweiLi/MGM-8x7B"
      ]
    },
    {
      "architecture_id": "MagicForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "ermu2001/pllava-13b",
        "ermu2001/pllava-34b"
      ]
    },
    {
      "architecture_id": "MolmoForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "Molbap/molmo-hf-7B-D",
        "Molbap/molmo-hf-72B"
      ]
    },
    {
      "architecture_id": "UrsaForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "URSA-MATH/URSA-8B",
        "URSA-MATH/URSA-8B-PS-GRPO"
      ]
    },
    {
      "architecture_id": "MUDDPythia",
      "total_models": 2,
      "sample_models": [
        "Caiyun-AI/MUDDPythia-1.4B",
        "Caiyun-AI/MUDDPythia-2.8B"
      ]
    },
    {
      "architecture_id": "FusionLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "starriver030515/FUSION-LLaMA3.1-8B",
        "starriver030515/FUSION-X-LLaMA3.1-8B"
      ]
    },
    {
      "architecture_id": "SpeechLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "ICTNLP/SLED-TTS-Streaming-Libriheavy",
        "ICTNLP/SLED-TTS-Libriheavy"
      ]
    },
    {
      "architecture_id": "EnergyTransformer",
      "total_models": 2,
      "sample_models": [
        "cccczshao/CALM-M",
        "cccczshao/CALM-L"
      ]
    },
    {
      "architecture_id": "T5GraphForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "gigant/graph_t5_230612",
        "gigant/graph_t5_230621"
      ]
    },
    {
      "architecture_id": "IdeficsForCausalLM",
      "total_models": 2,
      "sample_models": [
        "HuggingFaceM4/tiny-random-idefics-m4",
        "philschmid/tiny-random-idefics-m4"
      ]
    },
    {
      "architecture_id": "DistilWhisperForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "naver/multilingual-distilwhisper-28k",
        "naver/multilingual-distilwhisper-10k"
      ]
    },
    {
      "architecture_id": "AudioQwen2VLForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "lordChipotle/qwen2-vl-audio-7b",
        "lordChipotle/qwen2-vl-audio-finetuned-stage1"
      ]
    },
    {
      "architecture_id": "SwitchGPT2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "crumb/Ducky-MoMoe-prototype-e4-causal",
        "crumb/Ducky-MoMoe-prototype-e4-ul2"
      ]
    },
    {
      "architecture_id": "FegeoLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "NaughtyDog97/DFE-GPS-34B",
        "NaughtyDog97/DFE-GPS-9B"
      ]
    },
    {
      "architecture_id": "EuclidQwen2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "euclid-multimodal/Euclid-convnext-xxlarge-120524",
        "euclid-multimodal/Euclid-convnext-large-120524"
      ]
    },
    {
      "architecture_id": "SewyV2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Aarushhh/SEWY2-640M-untrained",
        "Aarushhh/smol-SEWY2-184M"
      ]
    },
    {
      "architecture_id": "SeerAttnQwen2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "SeerAttention/SeerAttention-Qwen2.5-32B-AttnGates",
        "SeerAttention/SeerAttention-Qwen2.5-14B-AttnGates"
      ]
    },
    {
      "architecture_id": "BacformerForCausalGM",
      "total_models": 2,
      "sample_models": [
        "macwiatrak/bacformer-causal-MAG",
        "macwiatrak/bacformer-causal-complete-genomes"
      ]
    },
    {
      "architecture_id": "BVVBestForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Bochkov/demo_bvv_ru",
        "Bochkov/demo_bvv_unfrozen_ru"
      ]
    },
    {
      "architecture_id": "MultimodalLFM2Model",
      "total_models": 2,
      "sample_models": [
        "GoofyLM/N2.3-Eye-1.3B-DEV",
        "GoofyLM/N2.2-Eye-1.3B"
      ]
    },
    {
      "architecture_id": "DistilBertForMaskedLM",
      "total_models": 2,
      "sample_models": [
        "gumran/distilbert-diffusion-TinyStories",
        "Sakil/bertfined_finetunedmodel_fakenews"
      ]
    },
    {
      "architecture_id": "OPT_PromptTuned_For_SentimentAnalysis",
      "total_models": 2,
      "sample_models": [
        "furquan/opt_2_7_b_prompt_tuned_sentiment_analysis",
        "furquan/opt-1-3b-prompt-tuned-sentiment-analysis"
      ]
    },
    {
      "architecture_id": "LSWTForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Avelina/lovelace-medium-alpha1",
        "Avelina/lovelace-medium-alpha1-sft"
      ]
    },
    {
      "architecture_id": "LingLongForCausalLM",
      "total_models": 2,
      "sample_models": [
        "AlumiK/LingLong-317M-Chat",
        "AlumiK/LingLong-317M"
      ]
    },
    {
      "architecture_id": "BlenderbotForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "12sciencejnv/TalkGPT",
        "Maida14/fine-tuned-blenderbot"
      ]
    },
    {
      "architecture_id": "Qwen2VLVAEForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "DongfuJiang/Qwen2-VL-VAE-7B-Instruct",
        "DongfuJiang/Qwen2-VL-VAE-7B-Instruct-mochi-vae"
      ]
    },
    {
      "architecture_id": "ThetaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "SVECTOR-CORPORATION/Theta-35",
        "SVECTOR-CORPORATION/Theta-35-Preview"
      ]
    },
    {
      "architecture_id": "ZeusForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Wonder-Griffin/ZeusMM-SFT-oasst1",
        "Wonder-Griffin/ZeusMM"
      ]
    },
    {
      "architecture_id": "Data2VecTextForCausalLM",
      "total_models": 2,
      "sample_models": [
        "hf-tiny-model-private/tiny-random-Data2VecTextForCausalLM",
        "dlwnsdnjs/my_awesome_eli5_clm-model"
      ]
    },
    {
      "architecture_id": "GistT5ForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "jayelm/flan-t5-xxl-neg_control-1",
        "jayelm/flan-t5-xxl-pos_control-1"
      ]
    },
    {
      "architecture_id": "HgrnForCausalLM",
      "total_models": 2,
      "sample_models": [
        "OpenNLPLab/HGRN-1B",
        "OpenNLPLab/HGRN-355M"
      ]
    },
    {
      "architecture_id": "CovSVDLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "iboing/CorDA_IPA_math_finetuned_math",
        "iboing/CorDA_KPA_nqopen_finetuned_math"
      ]
    },
    {
      "architecture_id": "ModernBertForMaskedLM",
      "total_models": 2,
      "sample_models": [
        "sjster/test_v2_medium",
        "diffutron/DiffutronLM-0.3B-Instruct"
      ]
    },
    {
      "architecture_id": "LlavaOnevision1_5ForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "Deep-VLM/LLaVA-OneVision-1.5-8B-Instruct-hf",
        "Deep-VLM/LLaVA-OneVision-1.5-4B-Instruct-hf"
      ]
    },
    {
      "architecture_id": "HrmForCausalLM",
      "total_models": 2,
      "sample_models": [
        "zbloss/HRM-sudoku-extreme",
        "zbloss/HRM-maze-30x30-hard"
      ]
    },
    {
      "architecture_id": "HelionForCausalLM",
      "total_models": 2,
      "sample_models": [
        "DeepXR/Helion-V1.5-XL",
        "DeepXR/Helion-V2"
      ]
    },
    {
      "architecture_id": "CodeLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "vikp/instruct_llama_7b",
        "vikp/nbs_instruct"
      ]
    },
    {
      "architecture_id": "FSGPTMoEForCausalLM",
      "total_models": 2,
      "sample_models": [
        "pingzhili/fairseq-moe-15b-bf16",
        "pingzhili/fairseq-moe-15b"
      ]
    },
    {
      "architecture_id": "LibraForCausalLM",
      "total_models": 2,
      "sample_models": [
        "YifanXu/libra-11b-chat",
        "YifanXu/libra-11b-base"
      ]
    },
    {
      "architecture_id": "PLMForCausalLM",
      "total_models": 2,
      "sample_models": [
        "PLM-Team/PLM-1.8B-Base",
        "PLM-Team/PLM-1.8B-Instruct"
      ]
    },
    {
      "architecture_id": "WisentQwen2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "wisent-ai/qwen2.5-math-7b-wisent-caa",
        "wisent-ai/qwen2.5-coder-7b-wisent-caa"
      ]
    },
    {
      "architecture_id": "AuroraGPT2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "ThatHungarian/Aurora-10M",
        "ThatHungarian/Aurora-30M"
      ]
    },
    {
      "architecture_id": "LiquidForCausalLM",
      "total_models": 2,
      "sample_models": [
        "reaperdoesntknow/DNA-50M",
        "reaperdoesntknow/DNA-175M"
      ]
    },
    {
      "architecture_id": "NanoGPTLMHeadModel",
      "total_models": 2,
      "sample_models": [
        "jacksuuuu/tinystories",
        "LevenKoko/mymodel-test"
      ]
    },
    {
      "architecture_id": "PoptorchPipelinedBartForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "jimypbr/bart-large-test",
        "Jinchen/bart-base-finetuned-en-to-ro"
      ]
    },
    {
      "architecture_id": "ProphetNetForCausalLM",
      "total_models": 2,
      "sample_models": [
        "hf-tiny-model-private/tiny-random-ProphetNetForCausalLM",
        "eleanorbeers/glue_clm-model"
      ]
    },
    {
      "architecture_id": "LlamaForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "qcw/llama2-panda-zh-13b",
        "chitanda/panda-7b-open-llama-preview-300pt"
      ]
    },
    {
      "architecture_id": "YiVLForCausalLM",
      "total_models": 2,
      "sample_models": [
        "BabyChou/Yi-VL-34B",
        "BabyChou/Yi-VL-6B"
      ]
    },
    {
      "architecture_id": "HuazangQWenForCausalLM",
      "total_models": 2,
      "sample_models": [
        "wei01/MY_MMLM",
        "AIXI-AIGC/OCR_MLLM_TOY"
      ]
    },
    {
      "architecture_id": "HFCausalModel",
      "total_models": 2,
      "sample_models": [
        "dinalt/walsh_instruct-1-7b",
        "dinalt/walsh-1-7b"
      ]
    },
    {
      "architecture_id": "vwLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "MYTH-Lab/VW-LMM-Vicuna-pif-7b",
        "MYTH-Lab/VW-LMM-Mistral-7b"
      ]
    },
    {
      "architecture_id": "LLaMAVIDLlavaForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "Nilesh360/llama-vid-7b-full-336",
        "Nilesh360/llama-vid-7b-full-224-video-fps-1"
      ]
    },
    {
      "architecture_id": "ActivationsGPTNeoForCausalLM",
      "total_models": 2,
      "sample_models": [
        "AISE-TUDelft/Custom-Activations-GPT-Adaptive-GELU",
        "AISE-TUDelft/Custom-Activations-GPT-KAN"
      ]
    },
    {
      "architecture_id": "GPT2Vision",
      "total_models": 2,
      "sample_models": [
        "damerajee/GPTVision-1-ft",
        "damerajee/GPT-Vision-1.5"
      ]
    },
    {
      "architecture_id": "MultimodalStarcoder2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "saujasv/multimodal-cad-starcoder-init",
        "saujasv/multimodal-cad-starcoder2-3b-cad"
      ]
    },
    {
      "architecture_id": "CostWiseGemmaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Digital-At-Work-Christopher/gemma-2-reranker",
        "boboliu/bge-reranker-v2.5-gemma2-lightweight-gptq"
      ]
    },
    {
      "architecture_id": "OlaLlavaLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "shi-labs/pretrain_dsg_OLA-VLM-CLIP-ViT-Llama3-8b",
        "shi-labs/pretrain_dsg_OLA-VLM-CLIP-ConvNeXT-Llama3-8b"
      ]
    },
    {
      "architecture_id": "ChexBonesForCausalLM",
      "total_models": 2,
      "sample_models": [
        "AIRI-Institute/chexfract-maira2",
        "AIRI-Institute/chexfract-chexagent"
      ]
    },
    {
      "architecture_id": "CLIPVisionMarianForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "flax-community/Image-captioning-Indonesia",
        "acul3/image-captioning-marian"
      ]
    },
    {
      "architecture_id": "MegatronForCausalLM",
      "total_models": 2,
      "sample_models": [
        "hyunwoongko/megatron-11B",
        "anton-l/megatron-11b"
      ]
    },
    {
      "architecture_id": "MegLMForCausalLM",
      "total_models": 2,
      "sample_models": [
        "malteos/2_6B_multilingual-bpe_hf_32768_10_rotary",
        "malteos/2_6B_multilingual-bpe_hf_32768_10_rotary_b"
      ]
    },
    {
      "architecture_id": "MyMossForCausalLM",
      "total_models": 2,
      "sample_models": [
        "ssbuild/moss-moon-003-sft-plugin-int4",
        "ssbuild/moss-moon-003-sft-int4"
      ]
    },
    {
      "architecture_id": "smallLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "cxdu/glide-vicuna13b",
        "cxdu/glide-33b"
      ]
    },
    {
      "architecture_id": "MyLlamaForTokenClassification",
      "total_models": 2,
      "sample_models": [
        "xmu-nlp/Llama-3-8b-gsm8k-value-A",
        "xmu-nlp/Llama-3-8b-gsm8k-value-B"
      ]
    },
    {
      "architecture_id": "TTTMLPForCausalLM",
      "total_models": 2,
      "sample_models": [
        "b-re-w/TTTMLP-1.5B-Base_37.4ppl",
        "b-re-w/TTTMLP-1.5B-Base_66.7ppl"
      ]
    },
    {
      "architecture_id": "PixelreferliteMetaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Alibaba-DAMO-Academy/PixelRefer-Lite-2B",
        "Alibaba-DAMO-Academy/PixelRefer-Lite-7B"
      ]
    },
    {
      "architecture_id": "GPT4DevForCausalLM",
      "total_models": 2,
      "sample_models": [
        "k050506koch/GPT4-dev-177M-1511",
        "k050506koch/GPT4-dev-177M-1511-Instruct"
      ]
    },
    {
      "architecture_id": "Braille256Model",
      "total_models": 2,
      "sample_models": [
        "ryanscottbarrett/braille256-v1",
        "ryanscottbarrett/braille256-v2"
      ]
    },
    {
      "architecture_id": "Pix2SeqForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "nielsr/pix2seq-base",
        "nielsr/pix2seq-simple"
      ]
    },
    {
      "architecture_id": "VCoderLlavaLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "shi-labs/vcoder_llava-v1.5-7b",
        "shi-labs/vcoder_llava-v1.5-13b"
      ]
    },
    {
      "architecture_id": "PaloForCausalLM",
      "total_models": 2,
      "sample_models": [
        "MBZUAI/MobilePALO-1.7B",
        "MBZUAI/PALO-7B"
      ]
    },
    {
      "architecture_id": "MMGPTLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Kangheng/Merlin-chat",
        "Kangheng/Merlin"
      ]
    },
    {
      "architecture_id": "OryxQwen2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "THUdyh/Oryx-7B-Image",
        "THUdyh/Oryx-1.5-Image"
      ]
    },
    {
      "architecture_id": "BertGenerationDecoder",
      "total_models": 2,
      "sample_models": [
        "ammonbro/bert_sp_updown",
        "Zlovoblachko/testik_L1_sent_generator"
      ]
    },
    {
      "architecture_id": "MyLLaMa",
      "total_models": 2,
      "sample_models": [
        "Mortie1/new-nlp-hw3-llama3",
        "Mortie1/new-nlp-hw3-llama2"
      ]
    },
    {
      "architecture_id": "OlaLlavaPhi3ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "shi-labs/pretrain_dsg_OLA-VLM-CLIP-ConvNeXT-Phi3-4k-mini",
        "shi-labs/pretrain_dsg_OLA-VLM-CLIP-ViT-Phi3-4k-mini"
      ]
    },
    {
      "architecture_id": "Sewy3ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Aarushhh/SEWY3-246M-untrained",
        "Aarushhh/SEWY3-246M-math-base"
      ]
    },
    {
      "architecture_id": "GLUSForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Swindl/GLUS-S",
        "Swindl/GLUS-A"
      ]
    },
    {
      "architecture_id": "TTTLinearForCausalLM",
      "total_models": 2,
      "sample_models": [
        "b-re-w/TTTLinear-1.5B-Base_67.6ppl",
        "b-re-w/TTTLinear-1.5B-Base_26.6ppl"
      ]
    },
    {
      "architecture_id": "LlamaMLPWithBiasForCausalLM",
      "total_models": 2,
      "sample_models": [
        "cfierro/Llama-2-7b-bias-chat-harmful-af-answer",
        "cfierro/Llama-2-7b-bias-harmful-af-refuse"
      ]
    },
    {
      "architecture_id": "Lfm2VlForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "laykaa/Med-TunAI-3B",
        "blackbird/lfm2-vl-furigana-ocr"
      ]
    },
    {
      "architecture_id": "OmniLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "harvey2333/omni_video_assistant_6_1",
        "harvey2333/omni_video_assistant_5_3"
      ]
    },
    {
      "architecture_id": "BoomerForCausalLM",
      "total_models": 2,
      "sample_models": [
        "budecosystem/boomer-bitnet-634m",
        "budecosystem/boomer-4b"
      ]
    },
    {
      "architecture_id": "EffT5ForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "jvelja/t5-samsum",
        "jvelja/t5-squad"
      ]
    },
    {
      "architecture_id": "DeepFFNLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "steph0713/deepffnllama-768_6_4-2",
        "steph0713/deepffnllama-768_12_4-1"
      ]
    },
    {
      "architecture_id": "Llama3ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "AlexHung29629/test_mllama_11B_v11",
        "AlexHung29629/test_mllama_11B_v13"
      ]
    },
    {
      "architecture_id": "TALlavaGemmaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "JianhongTu/ta-llava-pretrain-phase2",
        "JianhongTu/ta-llava"
      ]
    },
    {
      "architecture_id": "NGen3Model",
      "total_models": 2,
      "sample_models": [
        "TNSA/NGen2-15M",
        "TNSA/NGen4-Nano-Exp-a-05-09-2025-pre"
      ]
    },
    {
      "architecture_id": "MoEGPTForCausalLM",
      "total_models": 2,
      "sample_models": [
        "arnomatic/german-moe-gpt-v8-pretrained",
        "arnomatic/moe-combined-all3-v9-156k-finetuned"
      ]
    },
    {
      "architecture_id": "RotaryGPT2LMHeadModel",
      "total_models": 2,
      "sample_models": [
        "Zuckerbird/RoPE-gpt2-0.0",
        "Zuckerbird/RoPE-gpt2"
      ]
    },
    {
      "architecture_id": "MyBaichuanForCausalLM",
      "total_models": 2,
      "sample_models": [
        "ssbuild/baichuan2-7b-chat-int4",
        "ssbuild/baichuan2-13b-chat-int4"
      ]
    },
    {
      "architecture_id": "LlamaMoEUpscalingForCausalLM",
      "total_models": 2,
      "sample_models": [
        "terryyz/ds-8x6.7b-top-2-universal-evol-instruct-5e-5_bs_64_epoch_4",
        "terryyz/ds-8x6.7b-top-2-universal-evol-instruct-2e-5_bs_64_epoch_4_e_4"
      ]
    },
    {
      "architecture_id": "VLCLIPGPTNeoXForCausalLM",
      "total_models": 2,
      "sample_models": [
        "gpantaz/vl-pythia-1-best",
        "gpantaz/vl-pythia-1-final"
      ]
    },
    {
      "architecture_id": "TrOCRForCausalLM",
      "total_models": 2,
      "sample_models": [
        "ta4tsering/Lhasa_Kanjur_TrOCR_model",
        "ta4tsering/Lhasa_Kanjur_TrOCR_model-44000"
      ]
    },
    {
      "architecture_id": "LlamaMoDForCausalLM",
      "total_models": 2,
      "sample_models": [
        "kevin1010607/llama2-7b-mod-sft-full",
        "kevin1010607/llama2-7b-mod-sft-full-3"
      ]
    },
    {
      "architecture_id": "MixsenseLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Zero-Vision/Llama-3-MixSenseV1_1",
        "Zero-Vision/Llama-3-MixSense"
      ]
    },
    {
      "architecture_id": "CBHybridLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "cerebras/Llama-3-CBHybridL-8B",
        "cerebras/Llama-3-CBHybridM-8B"
      ]
    },
    {
      "architecture_id": "Qwen2_5_VL_PGNForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "jiwan-chung/qwen2_5vl_3b_pgn_gqa_cot",
        "jiwan-chung/qwen2_5vl_3b_pgn_refcoco"
      ]
    },
    {
      "architecture_id": "SpecT1ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "SVECTOR-CORPORATION/Spec-T1-RL-7B",
        "SVECTOR-CORPORATION/Spec-T1-Base-7B"
      ]
    },
    {
      "architecture_id": "Qwen3ReasoningForCausalLM",
      "total_models": 2,
      "sample_models": [
        "agurung/sft_qwen8B_25percent",
        "agurung/sft_qwen8B_25percent_lr_1e4"
      ]
    },
    {
      "architecture_id": "Gemma3nForCausalLM",
      "total_models": 2,
      "sample_models": [
        "igorktech/gemma-3n-e2b-it-language-pruned",
        "h4shy/gemma-3n-E2B-prototype-pytorch"
      ]
    },
    {
      "architecture_id": "InternVLForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "whalezzz/MM-RAIT-InternVL_3",
        "hyun77/InternVL3-2B-finetuned"
      ]
    },
    {
      "architecture_id": "LLTransformerForCausalLM",
      "total_models": 2,
      "sample_models": [
        "mtzig/lltransformer-linear-test1",
        "mtzig/lltransformer-fwe"
      ]
    },
    {
      "architecture_id": "HFByteETM",
      "total_models": 2,
      "sample_models": [
        "idah4/byteetm-korean-tiny",
        "idah4/byteetm-korean-small"
      ]
    },
    {
      "architecture_id": "DribbleForCausalLM",
      "total_models": 2,
      "sample_models": [
        "awilliamson/dribble",
        "awilliamson/dribble-decoder"
      ]
    },
    {
      "architecture_id": "RPTForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Shahar603/rpt-torch-1",
        "iohadrubin/rpt-1b"
      ]
    },
    {
      "architecture_id": "AlexaLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "sfulay/zephyr-7b-sft-full-amazon",
        "namazifar/Qwen-2.5-7B-Simple-RL"
      ]
    },
    {
      "architecture_id": "GeckoForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "farrosalferro24/Gecko-Mantis-8B-siglip-llama3",
        "farrosalferro24/Gecko-Mantis-8B-clip-llama3"
      ]
    },
    {
      "architecture_id": "DistributedLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "ash001/arxiv-llama-13b-0-1000",
        "ash001/arxiv-llama-13b-test-index"
      ]
    },
    {
      "architecture_id": "LLaMA",
      "total_models": 2,
      "sample_models": [
        "pavl0/LLaMA5M",
        "pavl0/LLaMA5M_config"
      ]
    },
    {
      "architecture_id": "EmovaQwen2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Emova-ollm/emova-qwen-2-5-72b",
        "Emova-ollm/emova-qwen-2-5-7b"
      ]
    },
    {
      "architecture_id": "VicaQwenForCausalLM",
      "total_models": 2,
      "sample_models": [
        "nkkbr/ViCA2-thinkng",
        "nkkbr/ViCA2-stage2-onevision-ft"
      ]
    },
    {
      "architecture_id": "Qwen2SteeringVectorForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Vadim21221/qwen2_1_5b-instruct-steering-vectorsteeringxlora",
        "Vadim21221/qwen2_1_5b-instruct-steering-vector"
      ]
    },
    {
      "architecture_id": "MoeTransformerForCausalLM",
      "total_models": 2,
      "sample_models": [
        "pranavkarra/moe-5l-active-arxiv-code-simplestories",
        "pranavkarra/moe-5l-total-arxiv-code-simplestories"
      ]
    },
    {
      "architecture_id": "Gemma3Model",
      "total_models": 2,
      "sample_models": [
        "dineshkvr/gemma3-270m-tinystories-gpt2-tokenizer",
        "chinmaydk99/gemma3-270m-tinystories"
      ]
    },
    {
      "architecture_id": "RobertaPreLayerNormForCausalLM",
      "total_models": 2,
      "sample_models": [
        "hf-tiny-model-private/tiny-random-RobertaPreLayerNormForCausalLM",
        "CornCube/my_awesome_eli5_clm-model"
      ]
    },
    {
      "architecture_id": "GaudiLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "nguyenthanhdo/vhac_v3.2",
        "MaterialsAI/robocr_poscar_2col_llama"
      ]
    },
    {
      "architecture_id": "GistGPTNeoForCausalLM",
      "total_models": 2,
      "sample_models": [
        "ayushchakravarthy/gist_gptneo_untrained",
        "ayushchakravarthy/gist-gptneo-untrained"
      ]
    },
    {
      "architecture_id": "RRForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "fanjiang98/CLASS-XOR-Retrieve",
        "fanjiang98/CLASS-XOR-Full"
      ]
    },
    {
      "architecture_id": "LlavaMixtralForCausalLM",
      "total_models": 2,
      "sample_models": [
        "shi-labs/CuMo-mixtral-8x7b",
        "ura-hcmut/MixSUraV"
      ]
    },
    {
      "architecture_id": "MPLUGDocOwlForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "danaaubakirova/mplugdocowl1.5",
        "danaaubakirova/mplugdocowl1.5-Chat-hf"
      ]
    },
    {
      "architecture_id": "DSHybridForCausalLM",
      "total_models": 2,
      "sample_models": [
        "codys12/dshybrid",
        "codys12/dshybridempty"
      ]
    },
    {
      "architecture_id": "OryxLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "THUdyh/Oryx-34B",
        "THUdyh/Oryx-34B-Image"
      ]
    },
    {
      "architecture_id": "CosmosForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "RaushanTurganbay/cosmos-4B-hf",
        "RaushanTurganbay/cosmos-5B-hf"
      ]
    },
    {
      "architecture_id": "Doge2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "SmallDoge/Doge2-175M-checkpoint",
        "SmallDoge/Transformer-MHA-1.7B"
      ]
    },
    {
      "architecture_id": "HFTransformerModel",
      "total_models": 2,
      "sample_models": [
        "singhsumony2j/SeedGPT-V2",
        "singhsumony2j/SeedGPT-V1"
      ]
    },
    {
      "architecture_id": "tnl1-385m-10b-token_no-act",
      "total_models": 2,
      "sample_models": [
        "OpenNLPLab/TransNormerLLM-385M_10B-Token_no-act",
        "OpenNLPLab/TransNormerLLM-385M_10B-Token_relu"
      ]
    },
    {
      "architecture_id": "RecombinationTransformerForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Xuezha/RecombinationTransformer-base",
        "Xuezha/RecombinationTransformer-small-base-pretrained"
      ]
    },
    {
      "architecture_id": "CyclicFormerForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Q-bert/CyclicFormer-wiki-en",
        "Q-bert/CyclicFormer-tiny-shakespeare"
      ]
    },
    {
      "architecture_id": "SeqaxLMHeadModel",
      "total_models": 2,
      "sample_models": [
        "rachittibrewal/seqax1b_2x_lr_2.5e-3-kto",
        "rachittibrewal/seqax1b_2x_lr_2.5e-3-SFT"
      ]
    },
    {
      "architecture_id": "Qwen2_5OmniThinkerForCausalLM",
      "total_models": 2,
      "sample_models": [
        "AlexHung29629/Qwen-2.5-Omni-ThinkerTextModel",
        "AlexHung29629/Qwen-2.5-Omni-ThinkerTextModel-s1"
      ]
    },
    {
      "architecture_id": "GPT2WithHMForCausalLM",
      "total_models": 2,
      "sample_models": [
        "CDHAI/gpt2-cgm-clm-hm-2022cgm-epoch2",
        "CDHAI/gpt2-cgm-clm-hm-epoch2"
      ]
    },
    {
      "architecture_id": "Qwen2blForCausalLM",
      "total_models": 2,
      "sample_models": [
        "kartmannXu/Qwen2.5-3B-bl-0.4",
        "kartmannXu/Qwen2.5-3B-Instruct-bl-0.4"
      ]
    },
    {
      "architecture_id": "DockGenModel",
      "total_models": 2,
      "sample_models": [
        "Franso/DockGen-Qwen3-4B-sft",
        "Franso/DockGen-Qwen3-0.6B"
      ]
    },
    {
      "architecture_id": "Qwen3OmniMoeThinkerForConditionalGeneration",
      "total_models": 2,
      "sample_models": [
        "ngqtrung/Qwen3-Omni-Thinker-30B-Thinking",
        "ngqtrung/Qwen3-Omni-Thinker-30B-Instruct"
      ]
    },
    {
      "architecture_id": "LlamaSeqEndMaskBMForCausalLM",
      "total_models": 2,
      "sample_models": [
        "hdong0/deepseek-Llama-8B-baseline-thin-init",
        "hdong0/deepseek-Llama-8B-baseline-Open-R1-GRPO_deepscaler_acc_mu_8_constant_lr"
      ]
    },
    {
      "architecture_id": "PaluLlamaForCausalLM",
      "total_models": 2,
      "sample_models": [
        "Syed-Hasan-8503/PaluLlama-3-8B-Instruct",
        "jimc86/test-llama-3.1-8b-lowrank-Q"
      ]
    },
    {
      "architecture_id": "ZZJRabbit2ForCausalLM",
      "total_models": 2,
      "sample_models": [
        "xiaoyewuz-Ruster/zzjrabbit2",
        "xiaoyewuz-Ruster/zzjrabbit2.1"
      ]
    },
    {
      "architecture_id": "Qwen2CapreseForCausalLM",
      "total_models": 2,
      "sample_models": [
        "hdong0/qwen2_dummy_lora",
        "hdong0/qwen2_dummy_cap"
      ]
    },
    {
      "architecture_id": "GPT2ForSequenceClassification",
      "total_models": 2,
      "sample_models": [
        "qgyd2021/reward_model_gpt2_stack_exchange",
        "Wade5/0.01v"
      ]
    },
    {
      "architecture_id": "EnhancedCustomLoRAModel",
      "total_models": 2,
      "sample_models": [
        "hson1003/50cethics_model",
        "hson1003/50cethics_provip"
      ]
    },
    {
      "architecture_id": "MiMoAudioForCausalLM",
      "total_models": 2,
      "sample_models": [
        "emoact/emoact-pt",
        "mrfakename/sft-430"
      ]
    },
    {
      "architecture_id": "POINTSV15ChatModel",
      "total_models": 1,
      "sample_models": [
        "tencent/POINTS-Reader"
      ]
    },
    {
      "architecture_id": "IlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "hmellor/Ilama-3.2-1B"
      ]
    },
    {
      "architecture_id": "SFLVisionEncoderModel",
      "total_models": 1,
      "sample_models": [
        "Cyril666/SFL-Encoder-Pretrained-Qwen3"
      ]
    },
    {
      "architecture_id": "LibraMistralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "X-iZhang/libra-llava-med-v1.5-mistral-7b"
      ]
    },
    {
      "architecture_id": "GritLM",
      "total_models": 1,
      "sample_models": [
        "parasail-ai/GritLM-7B-vllm"
      ]
    },
    {
      "architecture_id": "CPMAntForCausalLM",
      "total_models": 1,
      "sample_models": [
        "openbmb/cpm-ant-10b"
      ]
    },
    {
      "architecture_id": "ARCHunyuanVideoForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "TencentARC/ARC-Hunyuan-Video-7B"
      ]
    },
    {
      "architecture_id": "Qwen2_5_VL_VGGTForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Diankun/Spatial-MLLM-subset-sft"
      ]
    },
    {
      "architecture_id": "JinaVLMForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "jinaai/jina-vlm"
      ]
    },
    {
      "architecture_id": "Jais2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "inceptionai/Jais-2-8B-Chat"
      ]
    },
    {
      "architecture_id": "DharaForMaskedDiffusion",
      "total_models": 1,
      "sample_models": [
        "codelion/dhara-70m"
      ]
    },
    {
      "architecture_id": "TorchMultiOmicsModel",
      "total_models": 1,
      "sample_models": [
        "InstaDeepAI/ChatNT"
      ]
    },
    {
      "architecture_id": "FunAudioChatForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "FunAudioLLM/Fun-Audio-Chat-8B"
      ]
    },
    {
      "architecture_id": "Qwen2ForSequenceClassification",
      "total_models": 1,
      "sample_models": [
        "nvidia/AceMath-7B-RM"
      ]
    },
    {
      "architecture_id": "DotsVLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "rednote-hilab/dots.vlm1.inst"
      ]
    },
    {
      "architecture_id": "CircuitGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "openai/circuit-sparsity"
      ]
    },
    {
      "architecture_id": "MiniMaxText01ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MiniMaxAI/MiniMax-Text-01"
      ]
    },
    {
      "architecture_id": "FGCLIPModel",
      "total_models": 1,
      "sample_models": [
        "qihoo360/fg-clip-large"
      ]
    },
    {
      "architecture_id": "MolformerForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ibm-research/GP-MoLFormer-Uniq"
      ]
    },
    {
      "architecture_id": "VSMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "craigwu/seal_vsm_7b"
      ]
    },
    {
      "architecture_id": "MoYiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "astanahub/alemllm"
      ]
    },
    {
      "architecture_id": "Qwen2ForClassifier",
      "total_models": 1,
      "sample_models": [
        "VGS-AI/DeepSeek-VM-1.5B"
      ]
    },
    {
      "architecture_id": "GrinQwen2VLForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "HIT-TMG/Uni-MoE-2.0-Omni"
      ]
    },
    {
      "architecture_id": "Qwen3TSForCausalLM",
      "total_models": 1,
      "sample_models": [
        "bytedance-research/ChatTS-8B"
      ]
    },
    {
      "architecture_id": "DIVEdoc",
      "total_models": 1,
      "sample_models": [
        "JayRay5/DIVE-Doc-FRD"
      ]
    },
    {
      "architecture_id": "OvisU1",
      "total_models": 1,
      "sample_models": [
        "AIDC-AI/Ovis-U1-3B"
      ]
    },
    {
      "architecture_id": "TinyLlavaStablelmForCausalLM",
      "total_models": 1,
      "sample_models": [
        "bczhou/TinyLLaVA-2.0B"
      ]
    },
    {
      "architecture_id": "GPT2CustomLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "fxmarty/tiny-testing-gpt2-remote-code"
      ]
    },
    {
      "architecture_id": "Gemma2MoeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "suayptalha/Sungur-3x9B-Cosmos"
      ]
    },
    {
      "architecture_id": "modeling_grove_moe.GroveMoeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "inclusionAI/GroveMoE-Inst"
      ]
    },
    {
      "architecture_id": "MnemosyneForCausalLM",
      "total_models": 1,
      "sample_models": [
        "amewebstudio/mnemosyne-multimodal-v4"
      ]
    },
    {
      "architecture_id": "VibeVoiceRealTimeForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "bezzam/VibeVoice-0.5B"
      ]
    },
    {
      "architecture_id": "ElBartForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "yali98/trainer_ver"
      ]
    },
    {
      "architecture_id": "Esm2LlamaInstructForCausalLM",
      "total_models": 1,
      "sample_models": [
        "xiao-fei/Prot2Text-V2-11B-Instruct-hf"
      ]
    },
    {
      "architecture_id": "LamedPhi3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "GoodBaiBai88/M3D-LaMed-Phi-3-4B"
      ]
    },
    {
      "architecture_id": "C3QwenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "liufanfanlff/C3-Context-Cascade-Compression"
      ]
    },
    {
      "architecture_id": "CXRMateEDModel",
      "total_models": 1,
      "sample_models": [
        "aehrc/cxrmate-ed"
      ]
    },
    {
      "architecture_id": "OlmoeUpropForCausalLM",
      "total_models": 1,
      "sample_models": [
        "sqlinn/uq-OLMoE-1B-7B-0924"
      ]
    },
    {
      "architecture_id": "QwerkyLlamaMambaHybridForCausalLM",
      "total_models": 1,
      "sample_models": [
        "QwerkyAI/Qwerky-Optimized-Llama3.1-Mamba-0.2-8B-Instruct"
      ]
    },
    {
      "architecture_id": "LamedLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "GoodBaiBai88/M3D-LaMed-Llama-2-7B"
      ]
    },
    {
      "architecture_id": "ModernMarianMTModel",
      "total_models": 1,
      "sample_models": [
        "sbartlett97/gqa-opus-mt-de-en"
      ]
    },
    {
      "architecture_id": "TinyChartPhiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "mPLUG/TinyChart-3B-768"
      ]
    },
    {
      "architecture_id": "LLaDAMoEModel",
      "total_models": 1,
      "sample_models": [
        "inclusionAI/LLaDA-MoE-7B-A1B-Base"
      ]
    },
    {
      "architecture_id": "DNikudModel",
      "total_models": 1,
      "sample_models": [
        "NadavShaked/D_Nikud"
      ]
    },
    {
      "architecture_id": "OlaQwenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "THUdyh/Ola-7b"
      ]
    },
    {
      "architecture_id": "BunnyQwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "BAAI/Bunny-v1_0-2B-zh"
      ]
    },
    {
      "architecture_id": "DebertaV2PairRM",
      "total_models": 1,
      "sample_models": [
        "llm-blender/PairRM-hf"
      ]
    },
    {
      "architecture_id": "LingoWhaleForCausalLM",
      "total_models": 1,
      "sample_models": [
        "deeplang-ai/LingoWhale-8B"
      ]
    },
    {
      "architecture_id": "MoELLaVAQWenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "LanguageBind/MoE-LLaVA-Qwen-1.8B-4e"
      ]
    },
    {
      "architecture_id": "VaryTinyForCausalLM",
      "total_models": 1,
      "sample_models": [
        "jonathanli/vary-tiny"
      ]
    },
    {
      "architecture_id": "XmodelLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "XiaoduoAILab/Xmodel_VLM"
      ]
    },
    {
      "architecture_id": "FusionInDecoderForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Intel/fid_flan_t5_base_nq"
      ]
    },
    {
      "architecture_id": "MeteorMambaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "BK-Lee/Meteor-Mamba"
      ]
    },
    {
      "architecture_id": "Custom_MPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "insilicomedicine/precious3-gpt-multi-modal"
      ]
    },
    {
      "architecture_id": "GrokForCausalLM",
      "total_models": 1,
      "sample_models": [
        "keyfan/grok-1-hf"
      ]
    },
    {
      "architecture_id": "DUO",
      "total_models": 1,
      "sample_models": [
        "s-sahoo/duo"
      ]
    },
    {
      "architecture_id": "LOLEVEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Marks-lab/LOL-EVE"
      ]
    },
    {
      "architecture_id": "KangarooForCausalLM",
      "total_models": 1,
      "sample_models": [
        "KangarooGroup/kangaroo"
      ]
    },
    {
      "architecture_id": "XModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "XiaoduoAILab/Xmodel_LM"
      ]
    },
    {
      "architecture_id": "MAEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "UniMus/OpenJMLA"
      ]
    },
    {
      "architecture_id": "CambrianPhi3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "nyu-visionx/cambrian-phi3-3b"
      ]
    },
    {
      "architecture_id": "LlavaAVQwenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "tsinghua-ee/video-SALMONN-2"
      ]
    },
    {
      "architecture_id": "Qwen2VLMOEForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "chaoyinshe/EchoVLM"
      ]
    },
    {
      "architecture_id": "DusMistralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "adalbertojunior/DUSMistral"
      ]
    },
    {
      "architecture_id": "NablaVLForCausalLM",
      "total_models": 1,
      "sample_models": [
        "nablasinc/NABLA-VL"
      ]
    },
    {
      "architecture_id": "Typhoon2Audio2AudioForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "scb10x/llama3.1-typhoon2-audio-8b-instruct"
      ]
    },
    {
      "architecture_id": "KateAIForCausalLM",
      "total_models": 1,
      "sample_models": [
        "unamedai/KateAI"
      ]
    },
    {
      "architecture_id": "AquilaMoeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "BAAI/AquilaMoE-SFT"
      ]
    },
    {
      "architecture_id": "TextToTextModel",
      "total_models": 1,
      "sample_models": [
        "charent/ChatLM-mini-Chinese"
      ]
    },
    {
      "architecture_id": "MeteorForCausalLM",
      "total_models": 1,
      "sample_models": [
        "BK-Lee/Meteor-MLM"
      ]
    },
    {
      "architecture_id": "LlavaStableLMEpochForCausalLM",
      "total_models": 1,
      "sample_models": [
        "NousResearch/Obsidian-3B-V0.5"
      ]
    },
    {
      "architecture_id": "GPTBigCodeLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "bigcode/santacoderpack"
      ]
    },
    {
      "architecture_id": "RexSeekQwenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "IDEA-Research/RexSeek-3B"
      ]
    },
    {
      "architecture_id": "SongGenDualTrackForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "LiuZH-19/SongGen_interleaving_A_V"
      ]
    },
    {
      "architecture_id": "WhaleyeForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "nalazhar/tiny_whaleye"
      ]
    },
    {
      "architecture_id": "CoherenceMomentumModel",
      "total_models": 1,
      "sample_models": [
        "aisingapore/coherence-momentum"
      ]
    },
    {
      "architecture_id": "LlaMAForCausalLM",
      "total_models": 1,
      "sample_models": [
        "circulus/alpaca-7b"
      ]
    },
    {
      "architecture_id": "EvoMistralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "SakanaAI/EvoLLM-JP-v1-10B"
      ]
    },
    {
      "architecture_id": "XLMRobertaForTokenClassification",
      "total_models": 1,
      "sample_models": [
        "QomSSLab/Anonymizer-xlm-roberta"
      ]
    },
    {
      "architecture_id": "RuQwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "FractalGPT/RuQwen2.5-3B-Instruct-AWQ"
      ]
    },
    {
      "architecture_id": "LlamaIRForCausalLM",
      "total_models": 1,
      "sample_models": [
        "akemiH/JMLR"
      ]
    },
    {
      "architecture_id": "SmallThinkerForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Tiiny/SmallThinker-4BA0.6B-Instruct"
      ]
    },
    {
      "architecture_id": "Glm4vMoeForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "EasyDeL/GLM-4.6V"
      ]
    },
    {
      "architecture_id": "LlamoeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "damerajee/Llamoe-test"
      ]
    },
    {
      "architecture_id": "VaetkiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "NC-AI-consortium-VAETKI/VAETKI"
      ]
    },
    {
      "architecture_id": "LeerooDedicatedOOE",
      "total_models": 1,
      "sample_models": [
        "leeroo/LeerooDedicated-Math-7b"
      ]
    },
    {
      "architecture_id": "GPT2LMHeadModelForMultiTokenPrediction",
      "total_models": 1,
      "sample_models": [
        "Ray-000/2024s-japanese-gpt2-MTP4"
      ]
    },
    {
      "architecture_id": "FERRETLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "venkycs/apple-ferret-7b"
      ]
    },
    {
      "architecture_id": "QuasarForCausalLM",
      "total_models": 1,
      "sample_models": [
        "AstraMindAI/AstraQuasar-4B"
      ]
    },
    {
      "architecture_id": "MultiHeadGPT2",
      "total_models": 1,
      "sample_models": [
        "ibm-research/gpt2-medium-multiexit"
      ]
    },
    {
      "architecture_id": "GPT2FlashLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "lxuechen/tldr-gpt2-xl"
      ]
    },
    {
      "architecture_id": "LlavaVistralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Vi-VLM/Vistral-V-7B"
      ]
    },
    {
      "architecture_id": "SoloForCausalLM",
      "total_models": 1,
      "sample_models": [
        "YangyiYY/SOLO-7B"
      ]
    },
    {
      "architecture_id": "MiniPhi3",
      "total_models": 1,
      "sample_models": [
        "niwz/Mini-Chinese-Phi3"
      ]
    },
    {
      "architecture_id": "AdaLlavaLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "zhuoyanxu/ada-llava-L-v1.5-7b"
      ]
    },
    {
      "architecture_id": "MMistralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ByteDance-Seed/SAIL-7B"
      ]
    },
    {
      "architecture_id": "Neutrino",
      "total_models": 1,
      "sample_models": [
        "neuralcrew/neutrino-instruct"
      ]
    },
    {
      "architecture_id": "LlamaMedITForCausalLM",
      "total_models": 1,
      "sample_models": [
        "meditsolutions/Llama-3.2-SUN-1B-Instruct"
      ]
    },
    {
      "architecture_id": "KeyeVLMoeForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Kwai-Keye/Keye-VL-671B-A37B"
      ]
    },
    {
      "architecture_id": "MixtralModel",
      "total_models": 1,
      "sample_models": [
        "BiMediX/BiMediX-Ara"
      ]
    },
    {
      "architecture_id": "Qwen2ForRewardModel",
      "total_models": 1,
      "sample_models": [
        "infly/Universal-PRM-7B"
      ]
    },
    {
      "architecture_id": "OpenModel",
      "total_models": 1,
      "sample_models": [
        "thenexthub/OpenModel-1T-A50B-Instruct"
      ]
    },
    {
      "architecture_id": "OrionMOECausalLM",
      "total_models": 1,
      "sample_models": [
        "OrionStarAI/Orion-MoE8x7B"
      ]
    },
    {
      "architecture_id": "InternLM2ForRewardModel",
      "total_models": 1,
      "sample_models": [
        "internlm/internlm2_5-step-prover-critic"
      ]
    },
    {
      "architecture_id": "PanguProMoEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "IntervitensInc/pangu-pro-moe-model"
      ]
    },
    {
      "architecture_id": "NeoLLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "KitsuVp/NeoLLM"
      ]
    },
    {
      "architecture_id": "MiniGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Houzeric/mini-gpt-french"
      ]
    },
    {
      "architecture_id": "TrillionForCausalLM",
      "total_models": 1,
      "sample_models": [
        "trillionlabs/Tri-70B-preview-SFT"
      ]
    },
    {
      "architecture_id": "AeroForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "lmms-lab/Aero-1-Audio"
      ]
    },
    {
      "architecture_id": "SokaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "soka0000/vclm-korean-7b"
      ]
    },
    {
      "architecture_id": "Qwen3ForCausalLMWithSafeguardIN",
      "total_models": 1,
      "sample_models": [
        "liuyilun2000/qwen3-0.6b-safeguard-in"
      ]
    },
    {
      "architecture_id": "TimblHuggingFaceModel",
      "total_models": 1,
      "sample_models": [
        "antalvdb/mblm-chatbot-instruction-prompts-igtree"
      ]
    },
    {
      "architecture_id": "ConditionalGPT2LMHeadModel",
      "total_models": 1,
      "sample_models": [
        "entropy/roberta_zinc_decoder"
      ]
    },
    {
      "architecture_id": "I3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "i3-lab/i3-12m"
      ]
    },
    {
      "architecture_id": "Tharo.GForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Aicraftar/Tharo.G-Eco"
      ]
    },
    {
      "architecture_id": "Qwen3MTPForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Babu420/ninko-pinko-inference"
      ]
    },
    {
      "architecture_id": "MMAlayaMPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "DataCanvas/MMAlaya"
      ]
    },
    {
      "architecture_id": "LumensparkModel",
      "total_models": 1,
      "sample_models": [
        "anto18671/lumenspark"
      ]
    },
    {
      "architecture_id": "BailingMoeLinearForCausalLM",
      "total_models": 1,
      "sample_models": [
        "inclusionAI/Ring-lite-linear-preview"
      ]
    },
    {
      "architecture_id": "YasinForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ysn-rfd/YASIN-Persian-Base"
      ]
    },
    {
      "architecture_id": "ShivikM1ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "navflyer/shivik-m1-2b"
      ]
    },
    {
      "architecture_id": "BunnyMiniCPMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "BAAI/Bunny-v1_0-3B-zh"
      ]
    },
    {
      "architecture_id": "GroveForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ekazakos/grove"
      ]
    },
    {
      "architecture_id": "NeuroReasonerPlanningHead1",
      "total_models": 1,
      "sample_models": [
        "ayjays132/NeuroReasoner-PlanningHead-1"
      ]
    },
    {
      "architecture_id": "AlinlightForCausalLM",
      "total_models": 1,
      "sample_models": [
        "EngineerGL/Alinlight"
      ]
    },
    {
      "architecture_id": "Phi3VForCausalLMMoE",
      "total_models": 1,
      "sample_models": [
        "lamm-mit/Cephalo-Phi-3-MoE-vision-128k-3x4b-beta"
      ]
    },
    {
      "architecture_id": "PathummaAudioModel",
      "total_models": 1,
      "sample_models": [
        "nectec/Pathumma-llm-audio-1.0.0"
      ]
    },
    {
      "architecture_id": "BailingMMNativeForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "inclusionAI/Ming-UniVision-16B-A3B"
      ]
    },
    {
      "architecture_id": "EvaGptForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MTSmash/EvaGPT-German-0.7B"
      ]
    },
    {
      "architecture_id": "LlavaSearchLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "craigwu/seal_vqa_7b"
      ]
    },
    {
      "architecture_id": "CenturioForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "WueNLP/centurio_qwen"
      ]
    },
    {
      "architecture_id": "Grok2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "TevunahAi/grok-2-FP8"
      ]
    },
    {
      "architecture_id": "JatModel",
      "total_models": 1,
      "sample_models": [
        "jat-project/jat"
      ]
    },
    {
      "architecture_id": "LanceAI",
      "total_models": 1,
      "sample_models": [
        "NeuraCraft/Lance-AI"
      ]
    },
    {
      "architecture_id": "TransformerLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "fukugawa/transformer-lm-japanese-0.1b"
      ]
    },
    {
      "architecture_id": "Bagel",
      "total_models": 1,
      "sample_models": [
        "lmms-lab/BAGEL-7B-MoT-ver.LE"
      ]
    },
    {
      "architecture_id": "FrawdLLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "tsingla98/frawdllm-100m"
      ]
    },
    {
      "architecture_id": "VStreamLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "IVGSZ/Flash-VStream-7b"
      ]
    },
    {
      "architecture_id": "LatentRecurrentDepthModel",
      "total_models": 1,
      "sample_models": [
        "codewithdark/latent-recurrent-depth-lm"
      ]
    },
    {
      "architecture_id": "DCFormer",
      "total_models": 1,
      "sample_models": [
        "Caiyun-AI/DCFormer-2.8B"
      ]
    },
    {
      "architecture_id": "ExaoneTDForCausalLM",
      "total_models": 1,
      "sample_models": [
        "nileshmalpeddi/ppo"
      ]
    },
    {
      "architecture_id": "LiberalMind_v1.5",
      "total_models": 1,
      "sample_models": [
        "arcticoneai/Arctic_AI"
      ]
    },
    {
      "architecture_id": "Qwen3GatedForCausalLM",
      "total_models": 1,
      "sample_models": [
        "michal-stefanik/Qwen3-gated-0.6B"
      ]
    },
    {
      "architecture_id": "RWKV7Qwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "recursal/QRWKV7-7B-Instruct"
      ]
    },
    {
      "architecture_id": "QwenImageEditForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "zenlm/zen-image-edit"
      ]
    },
    {
      "architecture_id": "XomdichForCausalLM",
      "total_models": 1,
      "sample_models": [
        "b3x0m/xomdich-mobile"
      ]
    },
    {
      "architecture_id": "Qwen2NomicVisionForCausalLM",
      "total_models": 1,
      "sample_models": [
        "jonathanjordan21/Qwen2.5-Nomic-Vision"
      ]
    },
    {
      "architecture_id": "YiVLForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "yaolily/GenS"
      ]
    },
    {
      "architecture_id": "HaipaiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "rocky1410/haipai-micro"
      ]
    },
    {
      "architecture_id": "TridaForDLM",
      "total_models": 1,
      "sample_models": [
        "trillionlabs/Trida-7B-Preview"
      ]
    },
    {
      "architecture_id": "LatentThinkingModel",
      "total_models": 1,
      "sample_models": [
        "Lapisbird/Llama-adaLR-model-latent-6-by-1"
      ]
    },
    {
      "architecture_id": "i3",
      "total_models": 1,
      "sample_models": [
        "i3-lab/i3-tiny"
      ]
    },
    {
      "architecture_id": "BartForSequenceClassification",
      "total_models": 1,
      "sample_models": [
        "radwa-f/BART-TweetDetox"
      ]
    },
    {
      "architecture_id": "EncoderDecoderModel",
      "total_models": 1,
      "sample_models": [
        "LeoCordoba/beto2beto"
      ]
    },
    {
      "architecture_id": "MLongformerEncoderDecoderForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "DEplain/trimmed_longmbart_docs_apa"
      ]
    },
    {
      "architecture_id": "Qwen2GlideDecoderLayer",
      "total_models": 1,
      "sample_models": [
        "sail/longspec-QwQ-32B-Preview"
      ]
    },
    {
      "architecture_id": "InversionFromHiddenStatesModel",
      "total_models": 1,
      "sample_models": [
        "dill-lab/pils-32-llama2-chat-7b"
      ]
    },
    {
      "architecture_id": "JibayAiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "JibayAi/jibay-s1-en-270m"
      ]
    },
    {
      "architecture_id": "MoELLaVAMistralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "LanguageBind/MoE-LLaVA-OpenChat-7B-4e"
      ]
    },
    {
      "architecture_id": "QMoEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "QuarkML/QMoE-400"
      ]
    },
    {
      "architecture_id": "GRetrieverModel",
      "total_models": 1,
      "sample_models": [
        "alfiannajih/g-retriever-resume-reviewer"
      ]
    },
    {
      "architecture_id": "SpecVisionForCausalLM",
      "total_models": 1,
      "sample_models": [
        "SVECTOR-CORPORATION/Spec-Vision-V1"
      ]
    },
    {
      "architecture_id": "LlavaLladaForMaskedDiffusion",
      "total_models": 1,
      "sample_models": [
        "KonstantinosKK/lavida-llada-v1.0-instruct-hf-transformers"
      ]
    },
    {
      "architecture_id": "AprielHForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ServiceNow-AI/Apriel-H1-15b-Thinker-SFT"
      ]
    },
    {
      "architecture_id": "InfiMMHDModel",
      "total_models": 1,
      "sample_models": [
        "Infi-MM/infimm-hd"
      ]
    },
    {
      "architecture_id": "LiteRTLMArtifactOnly",
      "total_models": 1,
      "sample_models": [
        "dousery/functiongemma-mobile-actions-litertlm"
      ]
    },
    {
      "architecture_id": "SheikhF1LMHeadModel",
      "total_models": 1,
      "sample_models": [
        "Sheikh-F1/Sheikh-F1"
      ]
    },
    {
      "architecture_id": "Nova1ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Smilyai-labs/Nova-1-large"
      ]
    },
    {
      "architecture_id": "LlavaCohereForCausalLM",
      "total_models": 1,
      "sample_models": [
        "maya-multimodal/maya"
      ]
    },
    {
      "architecture_id": "SpectusForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "MS-ML/SpecTUS_pretrained_only"
      ]
    },
    {
      "architecture_id": "ModernBertForSequenceClassification",
      "total_models": 1,
      "sample_models": [
        "opendatalab/meta-rater-professionalism-rating"
      ]
    },
    {
      "architecture_id": "ConfigfarsModel",
      "total_models": 1,
      "sample_models": [
        "Parham9292/Configfars"
      ]
    },
    {
      "architecture_id": "AHAQwen3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "xuan-luo/AHAQwen3-0.6B"
      ]
    },
    {
      "architecture_id": "BartPrefixPropForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "kaifanli/Normalbart-base"
      ]
    },
    {
      "architecture_id": "GemmoeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Crystalcareai/GemMoE-Medium-v0.4"
      ]
    },
    {
      "architecture_id": "ImpPhi3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MILVLG/Imp-v1.5-4B-Phi3"
      ]
    },
    {
      "architecture_id": "EditGPTMistralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "luoruipu1/Volcano-7b"
      ]
    },
    {
      "architecture_id": "OpenBAForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "OpenNLG/OpenBA-V1-Flan"
      ]
    },
    {
      "architecture_id": "MultiHeadGPTNeo",
      "total_models": 1,
      "sample_models": [
        "ibm-research/gpt-neo-125m-multiexit"
      ]
    },
    {
      "architecture_id": "OpenLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "aerner/lm-v2"
      ]
    },
    {
      "architecture_id": "GraphLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Jiabin99/GraphGPT-7B-mix-all"
      ]
    },
    {
      "architecture_id": "NoPEGPTHuggingFaceModel",
      "total_models": 1,
      "sample_models": [
        "andrewdalpino/NoPE-GPT-Small-Base"
      ]
    },
    {
      "architecture_id": "XomdichForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "b3x0m/hyper-xomdich"
      ]
    },
    {
      "architecture_id": "MermaidGPTModel",
      "total_models": 1,
      "sample_models": [
        "Houzeric/text-to-mermaid"
      ]
    },
    {
      "architecture_id": "VerySmollGPT",
      "total_models": 1,
      "sample_models": [
        "igidn/VerySmollGPT-5M-Base"
      ]
    },
    {
      "architecture_id": "BltForCausalLM",
      "total_models": 1,
      "sample_models": [
        "AlekseyCalvin/LYRICAL_MT_ru2en_ByteLatentTransformer_7b"
      ]
    },
    {
      "architecture_id": "ChatGlmForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ybelkada/chatglm3-6b-hf"
      ]
    },
    {
      "architecture_id": "HixtralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "damerajee/hathi-moe-test"
      ]
    },
    {
      "architecture_id": "Enhancedi3Model",
      "total_models": 1,
      "sample_models": [
        "i3-lab/i3-200m-v1"
      ]
    },
    {
      "architecture_id": "MobilintExaone4ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "mobilint/EXAONE-4.0-1.2B"
      ]
    },
    {
      "architecture_id": "HelionOSCForCausalLM",
      "total_models": 1,
      "sample_models": [
        "DeepXR/Helion-OSC"
      ]
    },
    {
      "architecture_id": "PoptorchPipelinedT5ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Jinchen/t5-small-finetuned-xsum"
      ]
    },
    {
      "architecture_id": "LlavaGPTNeoXForCausalLM",
      "total_models": 1,
      "sample_models": [
        "LearnItAnyway/llava-polyglot-ko-1.3b-hf"
      ]
    },
    {
      "architecture_id": "ImpQwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MILVLG/Imp-v1.5-2B-Qwen1.5"
      ]
    },
    {
      "architecture_id": "PartiallySharedLayerModel",
      "total_models": 1,
      "sample_models": [
        "xfcxcxcdfdfd/testdfgdfdfdfv1"
      ]
    },
    {
      "architecture_id": "ByteGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ijktech/ByteGPT-small"
      ]
    },
    {
      "architecture_id": "HybridTinyLlavaForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "hustvl/MaTVLM_0_25_Mamba2"
      ]
    },
    {
      "architecture_id": "DexV1LMHeadModel",
      "total_models": 1,
      "sample_models": [
        "ghosthets/Dex"
      ]
    },
    {
      "architecture_id": "EfficientNetForImageClassification",
      "total_models": 1,
      "sample_models": [
        "meet12341234/medgemma_finetuned_for_Alzheimers"
      ]
    },
    {
      "architecture_id": "OpenGPT",
      "total_models": 1,
      "sample_models": [
        "Achyuth4/FlawlessAI"
      ]
    },
    {
      "architecture_id": "ASVDOPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "hahnyuan/opt-125m-asvd90"
      ]
    },
    {
      "architecture_id": "TroLForCausalLM",
      "total_models": 1,
      "sample_models": [
        "BK-Lee/TroL-3.8B"
      ]
    },
    {
      "architecture_id": "LlavaStableLM_1_6bForCausalLM",
      "total_models": 1,
      "sample_models": [
        "FreedomIntelligence/ALLaVA-StableLM2-1_6B"
      ]
    },
    {
      "architecture_id": "MoLoRAQwenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "BlueZeros/MING-MedCare-7B"
      ]
    },
    {
      "architecture_id": "PolyLLaMAForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Open-Foundation-Models/PolyNorm_1B"
      ]
    },
    {
      "architecture_id": "DiseaseGPTModel",
      "total_models": 1,
      "sample_models": [
        "riadrayhan111/riad-disease-gpt"
      ]
    },
    {
      "architecture_id": "XmodelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "XiaoduoAILab/Xmodel-2.5"
      ]
    },
    {
      "architecture_id": "Qwen3VLRetFinetuneForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "TencentBAC/U-MARVEL-Qwen3VL-4B-Instruct"
      ]
    },
    {
      "architecture_id": "STLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "bjdwh/UrbanGPT"
      ]
    },
    {
      "architecture_id": "PlavaForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "ermu2001/pllava-7b"
      ]
    },
    {
      "architecture_id": "KinoeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "AoiKazama/Kinoe-7B"
      ]
    },
    {
      "architecture_id": "Palmyra_visionForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Writer/palmyra-vision"
      ]
    },
    {
      "architecture_id": "DEXV1LMHeadModel",
      "total_models": 1,
      "sample_models": [
        "dexcommunity/dex"
      ]
    },
    {
      "architecture_id": "HumanVForCausalLM",
      "total_models": 1,
      "sample_models": [
        "humanvprojectceo/nilla-story"
      ]
    },
    {
      "architecture_id": "CLIPVisionMBartForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "flax-community/clip-vit-base-patch32_mbart-large-50"
      ]
    },
    {
      "architecture_id": "GPTJLoraForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Enkhai/gpt-j-6b-8bit-lora"
      ]
    },
    {
      "architecture_id": "BigBrainLanguageModel",
      "total_models": 1,
      "sample_models": [
        "Fal7acy/big-brain-lm"
      ]
    },
    {
      "architecture_id": "MightyLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "openaccess-ai-collective/mighty-llama-1b"
      ]
    },
    {
      "architecture_id": "GLaMMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MBZUAI/GLaMM-RefSeg"
      ]
    },
    {
      "architecture_id": "VideoReferQwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "DAMO-NLP-SG/VideoRefer-7B-stage2.5"
      ]
    },
    {
      "architecture_id": "LigerGSAForCausalLM",
      "total_models": 1,
      "sample_models": [
        "linear-moe-hub/Liger-GSA-8B"
      ]
    },
    {
      "architecture_id": "InferenceMemoryWrapper",
      "total_models": 1,
      "sample_models": [
        "botverse/Titans-MAC-Llama-3-8b-Instruct"
      ]
    },
    {
      "architecture_id": "TAMELM",
      "total_models": 1,
      "sample_models": [
        "reaperdoesntknow/TameForCasualLM"
      ]
    },
    {
      "architecture_id": "ChatGLM2NSAForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Maxtimer97/GLM2NSA"
      ]
    },
    {
      "architecture_id": "Continue1ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "SVECTOR-CORPORATION/Continue-1-OSS"
      ]
    },
    {
      "architecture_id": "MobilintCohere2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "mobilint/c4ai-command-r7b-12-2024"
      ]
    },
    {
      "architecture_id": "MoELLaVAStablelmForCausalLM",
      "total_models": 1,
      "sample_models": [
        "LINs-lab/DynMoE-StableLM-1.6B"
      ]
    },
    {
      "architecture_id": "ArcanaLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "syp115/Arcana"
      ]
    },
    {
      "architecture_id": "Wav2Vec2BertForCTC",
      "total_models": 1,
      "sample_models": [
        "vrclc/W2V2-BERT-withLM-Malayalam-Studio"
      ]
    },
    {
      "architecture_id": "MUDDFormer",
      "total_models": 1,
      "sample_models": [
        "Caiyun-AI/MUDDFormer-2.8B"
      ]
    },
    {
      "architecture_id": "MoRLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "sudeshmu/fine_tune"
      ]
    },
    {
      "architecture_id": "TRMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "recursive-models/recursive1"
      ]
    },
    {
      "architecture_id": "DiffusionVL_Qwen2_5_ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "hustvl/DiffusionVL-Qwen2.5-7B"
      ]
    },
    {
      "architecture_id": "QGPT2LMHeadModel",
      "total_models": 1,
      "sample_models": [
        "TransLL/quant-basetrain"
      ]
    },
    {
      "architecture_id": "TransCoreMQWenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "PCIResearch/TransCore-M"
      ]
    },
    {
      "architecture_id": "MoEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "AnLan577/Dynamic_MoE"
      ]
    },
    {
      "architecture_id": "PlaptModel",
      "total_models": 1,
      "sample_models": [
        "Bindwell/PLAPT_V1"
      ]
    },
    {
      "architecture_id": "LlamaButlerForCausalLM",
      "total_models": 1,
      "sample_models": [
        "akhauriyash/Llama-3.2-3B-Butler"
      ]
    },
    {
      "architecture_id": "Qwen2MMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MentaCapture/qmodel"
      ]
    },
    {
      "architecture_id": "Qwen2ForCausalLMEagle",
      "total_models": 1,
      "sample_models": [
        "thunlp/Qwen2-7B-Instruct-FR-Spec"
      ]
    },
    {
      "architecture_id": "HyperMambaLM",
      "total_models": 1,
      "sample_models": [
        "hoanghai2110/HyperMambaLM-300M"
      ]
    },
    {
      "architecture_id": "RessAiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "RessAI/Onner-300m"
      ]
    },
    {
      "architecture_id": "SaRDinEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MinimaML/SaRDinE-14B8x4P"
      ]
    },
    {
      "architecture_id": "ViTGPT2LMForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "acul3/image-captioning"
      ]
    },
    {
      "architecture_id": "Re_gptForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Langboat/ReGPT-125M-200G"
      ]
    },
    {
      "architecture_id": "FlashGPTNeoXForCausalLM",
      "total_models": 1,
      "sample_models": [
        "vikp/cleaner"
      ]
    },
    {
      "architecture_id": "DCPythia",
      "total_models": 1,
      "sample_models": [
        "Caiyun-AI/DCPythia-6.9B"
      ]
    },
    {
      "architecture_id": "MistsForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "HachiML/Mists-7B-v0.1-not-trained-test"
      ]
    },
    {
      "architecture_id": "DeepstackLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "menglc/deepstack-l-hd-vicuna-7b"
      ]
    },
    {
      "architecture_id": "ElysiumForCausalLM",
      "total_models": 1,
      "sample_models": [
        "sty-yyj/elysium_7b"
      ]
    },
    {
      "architecture_id": "BlastModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "cwoolee/blast-llama-4B"
      ]
    },
    {
      "architecture_id": "DummyLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "cyrilvallez/test_remote_code_dummy_llama"
      ]
    },
    {
      "architecture_id": "BREENForCausalLM",
      "total_models": 1,
      "sample_models": [
        "tianleliphoebe/BREEN"
      ]
    },
    {
      "architecture_id": "DiffLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "kajuma/DiffLlama-1B"
      ]
    },
    {
      "architecture_id": "ExportableGPTScratchForCausalLM",
      "total_models": 1,
      "sample_models": [
        "itarutomy/llm_workshop_hands_on_gpt-model"
      ]
    },
    {
      "architecture_id": "BioPANOmniForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "standardmodelbio/SMB-v1-1.7B-Structure"
      ]
    },
    {
      "architecture_id": "Gremps88ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Gremlin/gremps-444m"
      ]
    },
    {
      "architecture_id": "DistillixForCausalLM",
      "total_models": 1,
      "sample_models": [
        "rileyseaburg/distillix-100m-v0.4"
      ]
    },
    {
      "architecture_id": "MobileBertForPreTraining",
      "total_models": 1,
      "sample_models": [
        "pheepa/mobilebert-uncased-jira-comments"
      ]
    },
    {
      "architecture_id": "NorT5ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "MatiasJ/norgec_nort5"
      ]
    },
    {
      "architecture_id": "PoptorchPipelinedWhisperForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "TheAIchemist13/whisper-tiny-hi-model"
      ]
    },
    {
      "architecture_id": "InfiMMZephyrModel",
      "total_models": 1,
      "sample_models": [
        "Infi-MM/infimm-zephyr"
      ]
    },
    {
      "architecture_id": "CombinedLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ema19/LLAMAdolly-7B-slerp"
      ]
    },
    {
      "architecture_id": "CustomDecoderOnlyT5",
      "total_models": 1,
      "sample_models": [
        "McGill-NLP/codellm_1b_alibi"
      ]
    },
    {
      "architecture_id": "TraXL",
      "total_models": 1,
      "sample_models": [
        "Wonder-Griffin/TraXL"
      ]
    },
    {
      "architecture_id": "Olmo1124ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "shanearora/i-am-a-good-big-instruct-model"
      ]
    },
    {
      "architecture_id": "NebulaXModel",
      "total_models": 1,
      "sample_models": [
        "Agnuxo/NEBULA-X-DEMO"
      ]
    },
    {
      "architecture_id": "ZagrosForCausalLM",
      "total_models": 1,
      "sample_models": [
        "darsadilab/zagros-1.0-quick"
      ]
    },
    {
      "architecture_id": "RiadDiseaseGPTModel",
      "total_models": 1,
      "sample_models": [
        "riadrayhan111/riad-diseases-gpt"
      ]
    },
    {
      "architecture_id": "VLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "HuggingFaceM4/tiny-random-vllama-clip"
      ]
    },
    {
      "architecture_id": "MyXverseForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ssbuild/xverse-13b-chat-int4"
      ]
    },
    {
      "architecture_id": "GPTJMoEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "TearGosling/gptj-4x6b-moe-test"
      ]
    },
    {
      "architecture_id": "IXCLayout",
      "total_models": 1,
      "sample_models": [
        "Yosemat/designvlm"
      ]
    },
    {
      "architecture_id": "HindiCausalLM",
      "total_models": 1,
      "sample_models": [
        "convaiinnovations/hindi-foundational-model-base"
      ]
    },
    {
      "architecture_id": "HIComQwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "lntzm/HICom_7B_qwen25_directg_local43_global32"
      ]
    },
    {
      "architecture_id": "Qwen2WithRegressionHead",
      "total_models": 1,
      "sample_models": [
        "ai-forever/pollux-judge-7b-r"
      ]
    },
    {
      "architecture_id": "HawkQwenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "xjtupanda/HawkVL-2B"
      ]
    },
    {
      "architecture_id": "TinySmartLLMForHF",
      "total_models": 1,
      "sample_models": [
        "HenrySentinel/tinyMind"
      ]
    },
    {
      "architecture_id": "KimiK2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "sohv/nanokimi-mini"
      ]
    },
    {
      "architecture_id": "KORMoMoeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "LDCC/KORMo-19B-MoE"
      ]
    },
    {
      "architecture_id": "PKVGPT",
      "total_models": 1,
      "sample_models": [
        "c-bone/CrystaLLM-pi_SLME"
      ]
    },
    {
      "architecture_id": "PixelreferQwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Alibaba-DAMO-Academy/PixelRefer-7B"
      ]
    },
    {
      "architecture_id": "MiniGPT",
      "total_models": 1,
      "sample_models": [
        "Svenko/ReqAi-small"
      ]
    },
    {
      "architecture_id": "PalmModel",
      "total_models": 1,
      "sample_models": [
        "ell-hol/palm_hf"
      ]
    },
    {
      "architecture_id": "FlaxGPTJForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Thewillonline/Reddit-v3"
      ]
    },
    {
      "architecture_id": "LoRAGPT2LMHeadModel",
      "total_models": 1,
      "sample_models": [
        "Zuckerbird/LoRAgpt2-0.0"
      ]
    },
    {
      "architecture_id": "BatGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MYTH-Lab/BatGPT-15B-sirius"
      ]
    },
    {
      "architecture_id": "LaTrForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "iakarshu/latr-base"
      ]
    },
    {
      "architecture_id": "MixFormerVLSequentialForCausalLM",
      "total_models": 1,
      "sample_models": [
        "cyq1998/vit-phi-1half"
      ]
    },
    {
      "architecture_id": "FSGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "pingzhili/fairseq-dense-125m"
      ]
    },
    {
      "architecture_id": "CustomModel4",
      "total_models": 1,
      "sample_models": [
        "denizyuret-shallowai/foo4a"
      ]
    },
    {
      "architecture_id": "LennaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "mtgv/Lenna-7B"
      ]
    },
    {
      "architecture_id": "GemmaModel",
      "total_models": 1,
      "sample_models": [
        "whjung/gemma-7b-v1.0"
      ]
    },
    {
      "architecture_id": "ArcanalamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "syp115/Arcana_star"
      ]
    },
    {
      "architecture_id": "STLLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Mutonix/Vriptor-STLLM"
      ]
    },
    {
      "architecture_id": "AdaVocabGemmaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "reflectio/adavocab-gemma-2b-512"
      ]
    },
    {
      "architecture_id": "PegaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Tony068/pega-8B"
      ]
    },
    {
      "architecture_id": "AudioOnlyThinker",
      "total_models": 1,
      "sample_models": [
        "chunhuizng/AudioOnlyThinker"
      ]
    },
    {
      "architecture_id": "DeepSeekForCausalLM",
      "total_models": 1,
      "sample_models": [
        "alibayram/turkish-deepseek"
      ]
    },
    {
      "architecture_id": "FridayForCausalLM",
      "total_models": 1,
      "sample_models": [
        "kevin510/friday-4bit"
      ]
    },
    {
      "architecture_id": "LlavaRegistersForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "amildravid4292/llava-llama-3-8b-test-time-registers"
      ]
    },
    {
      "architecture_id": "CRANEAIModel",
      "total_models": 1,
      "sample_models": [
        "veteroner/Novaai"
      ]
    },
    {
      "architecture_id": "AXI with transformers",
      "total_models": 1,
      "sample_models": [
        "pure-team/DeepThink-T1-Tuned"
      ]
    },
    {
      "architecture_id": "HaipaiLM",
      "total_models": 1,
      "sample_models": [
        "rocky1410/haipai-nano"
      ]
    },
    {
      "architecture_id": "BertForCausalLM",
      "total_models": 1,
      "sample_models": [
        "naifenn/gemma-2-2B-it-thinking-function_calling-V0"
      ]
    },
    {
      "architecture_id": "omFlaxT5ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "acul3/dalle-mini-indo-base"
      ]
    },
    {
      "architecture_id": "XLMProphetNetForCausalLM",
      "total_models": 1,
      "sample_models": [
        "patrickvonplaten/xprophetnet-decoder-clm-large-uncased"
      ]
    },
    {
      "architecture_id": "CogViewForCausalLM",
      "total_models": 1,
      "sample_models": [
        "valhalla/cogview-gpt2-test"
      ]
    },
    {
      "architecture_id": "GreedyModel",
      "total_models": 1,
      "sample_models": [
        "kibrq/greedy-intersection"
      ]
    },
    {
      "architecture_id": "NeuralnetForCausalLM",
      "total_models": 1,
      "sample_models": [
        "metadeeai/neural-net"
      ]
    },
    {
      "architecture_id": "CSGWukongForCausalLM",
      "total_models": 1,
      "sample_models": [
        "opencsg/csg-wukong-1B-VL-v0.1"
      ]
    },
    {
      "architecture_id": "MoELLaVAPhiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "LINs-lab/DynMoE-Phi-2-2.7B"
      ]
    },
    {
      "architecture_id": "GraphsGPTForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "DaizeDong/GraphsGPT-1W-C"
      ]
    },
    {
      "architecture_id": "NeRFLLMLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "andreamaduzzi/LLaNA-7B"
      ]
    },
    {
      "architecture_id": "OLMoModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "zehui127/omni_test_simple"
      ]
    },
    {
      "architecture_id": "SealionAudio",
      "total_models": 1,
      "sample_models": [
        "scb10x/Llama-SEA-LION-TH-audio-preview"
      ]
    },
    {
      "architecture_id": "SmolLM3ModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "bnb-community/SmolLM3-3B-Base-bnb-4bit"
      ]
    },
    {
      "architecture_id": "IlluminatorLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "Anipal/iLLuMinator"
      ]
    },
    {
      "architecture_id": "Sam2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Smilyai-labs/Sam-2.0"
      ]
    },
    {
      "architecture_id": "Phi3ForSequenceClassification",
      "total_models": 1,
      "sample_models": [
        "fred-baseten/phi-4-seq"
      ]
    },
    {
      "architecture_id": "ContinuousQwen3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Transluce/input_ablation_qwen3_8b_qwen3_8b_hint"
      ]
    },
    {
      "architecture_id": "LongformerBartWithDoctypeForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "metamong1/longbartwithdoctype"
      ]
    },
    {
      "architecture_id": "MvpForCausalLM",
      "total_models": 1,
      "sample_models": [
        "hf-tiny-model-private/tiny-random-MvpForCausalLM"
      ]
    },
    {
      "architecture_id": "RemBertForCausalLM",
      "total_models": 1,
      "sample_models": [
        "hf-tiny-model-private/tiny-random-RemBertForCausalLM"
      ]
    },
    {
      "architecture_id": "XLMRobertaXLForCausalLM",
      "total_models": 1,
      "sample_models": [
        "hf-tiny-model-private/tiny-random-XLMRobertaXLForCausalLM"
      ]
    },
    {
      "architecture_id": "BackpackGPT2LMHeadModel",
      "total_models": 1,
      "sample_models": [
        "lora-x/backpack-gpt2"
      ]
    },
    {
      "architecture_id": "UdopUnimodelForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "imjliao/udop"
      ]
    },
    {
      "architecture_id": "GazelleForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "tincans-ai/gazelle-v0.2"
      ]
    },
    {
      "architecture_id": "creekForCausalLM",
      "total_models": 1,
      "sample_models": [
        "maheer/creek"
      ]
    },
    {
      "architecture_id": "XMixtralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Hannibal046/xrag-moe"
      ]
    },
    {
      "architecture_id": "MoTLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "jaszczur/mixture_of_tokens"
      ]
    },
    {
      "architecture_id": "HFOpenMoeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "pingzhili/openmoe-8b-native-pt"
      ]
    },
    {
      "architecture_id": "TraXLMistralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Wonder-Griffin/TraXLMistralv1"
      ]
    },
    {
      "architecture_id": "HandsOnVLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Kami-code/handsonvlm-7b"
      ]
    },
    {
      "architecture_id": "MiniGeminiGemmaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "isitcoding/mini_Gemini2b_poisoned"
      ]
    },
    {
      "architecture_id": "Aicraftar-Tharo.G-ConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Aicraftar/Tharo.G-Neo"
      ]
    },
    {
      "architecture_id": "XylariaTransformer",
      "total_models": 1,
      "sample_models": [
        "Lap1official/Xylaria-1.8"
      ]
    },
    {
      "architecture_id": "LLAMIAFluxForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "ssingh22/LLAMIAFlux-7b-unprojector-inverted"
      ]
    },
    {
      "architecture_id": "MiniDecoderModel",
      "total_models": 1,
      "sample_models": [
        "Pavloria/mini-language-model"
      ]
    },
    {
      "architecture_id": "SMDMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Xssama/SMDM_SFT_GSM8K_clean"
      ]
    },
    {
      "architecture_id": "myOPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "underactuated/opt-125m_ft_test"
      ]
    },
    {
      "architecture_id": "Qwen2TSForCausalLM",
      "total_models": 1,
      "sample_models": [
        "xiezhe24/ChatTS-14B-GPTQ-Int4"
      ]
    },
    {
      "architecture_id": "MatchcommentaryModel",
      "total_models": 1,
      "sample_models": [
        "abocide/matchcommentary"
      ]
    },
    {
      "architecture_id": "NeuralQuantumNQLM",
      "total_models": 1,
      "sample_models": [
        "NeuralQuantum/nqlm"
      ]
    },
    {
      "architecture_id": "Qwen2_5_MemoryForCausalLM",
      "total_models": 1,
      "sample_models": [
        "WeijianQi1999/Qwen25-1-5B-Memory-ALFWORLD-SFT"
      ]
    },
    {
      "architecture_id": "DIT",
      "total_models": 1,
      "sample_models": [
        "haoyangzheng/didi-instruct-small"
      ]
    },
    {
      "architecture_id": "GeoVForCausalLM",
      "total_models": 1,
      "sample_models": [
        "GeoV/GeoV-9b-r2"
      ]
    },
    {
      "architecture_id": "GraphT5TransformerForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "haitengzhao/gimlet"
      ]
    },
    {
      "architecture_id": "SAFFULMHeadModel",
      "total_models": 1,
      "sample_models": [
        "haoranzhao419/saffu-head"
      ]
    },
    {
      "architecture_id": "MyChatGLMForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "ssbuild/chatglm2-6b-int4"
      ]
    },
    {
      "architecture_id": "CompressedLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "spachava/mini_open_llama"
      ]
    },
    {
      "architecture_id": "GptNeoForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Tony068/Ggpt"
      ]
    },
    {
      "architecture_id": "TransformerModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "crumb/GLORT2"
      ]
    },
    {
      "architecture_id": "CognitiveAgentForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Or4cl3-1/Cognitive-Agent-Gemma_7b"
      ]
    },
    {
      "architecture_id": "PagnolXlForCausalLM",
      "total_models": 1,
      "sample_models": [
        "lightonai/pagnol-xl"
      ]
    },
    {
      "architecture_id": "DeepseekFixedForCausalLM",
      "total_models": 1,
      "sample_models": [
        "vltnmmdv/deepseek-moe-16b-base"
      ]
    },
    {
      "architecture_id": "NDLMOEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Ndlcwx/NDLMoe_1.3B-base"
      ]
    },
    {
      "architecture_id": "GPT2hlcLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "hlab/gpt2sml-hlc-twt-wassa-adapted"
      ]
    },
    {
      "architecture_id": "Mahler60/Prueba",
      "total_models": 1,
      "sample_models": [
        "Mahler60/Prueba"
      ]
    },
    {
      "architecture_id": "SCANForCausalLM",
      "total_models": 1,
      "sample_models": [
        "zaydzuhri/scan-16M-test"
      ]
    },
    {
      "architecture_id": "LlamaSkipConnectionForCausalLM",
      "total_models": 1,
      "sample_models": [
        "vkkhare/llama-skip"
      ]
    },
    {
      "architecture_id": "BucketMemoryModel",
      "total_models": 1,
      "sample_models": [
        "moelanoby/Kok-GPT"
      ]
    },
    {
      "architecture_id": "FusionPhiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "starriver030515/FUSION-Phi3.5-3B"
      ]
    },
    {
      "architecture_id": "KVLatentForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Charlie81/SkipMoE"
      ]
    },
    {
      "architecture_id": "SSLLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "sausheong/ssllm_hf"
      ]
    },
    {
      "architecture_id": "TPULlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "benjamin/Qwen3-14B-flax"
      ]
    },
    {
      "architecture_id": "MixtureOfRecursions",
      "total_models": 1,
      "sample_models": [
        "Girinath11/MixtureofRecursionwithRouter"
      ]
    },
    {
      "architecture_id": "DeepseekV3ForCausalLMNextN",
      "total_models": 1,
      "sample_models": [
        "chutesai/DeepSeek-R1T-Chimera-NextN"
      ]
    },
    {
      "architecture_id": "LlavaGemmaForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "AmeenAli023/test1"
      ]
    },
    {
      "architecture_id": "JinsooLLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "DokHee/jinsoo-llm-1b-korean"
      ]
    },
    {
      "architecture_id": "RotoBARTForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "pere/flax-bart-nb-nn"
      ]
    },
    {
      "architecture_id": "H3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "nielsr/H3-125m"
      ]
    },
    {
      "architecture_id": "MemBartForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "qywu/membart-large"
      ]
    },
    {
      "architecture_id": "XmodForCausalLM",
      "total_models": 1,
      "sample_models": [
        "hf-tiny-model-private/tiny-random-XmodForCausalLM"
      ]
    },
    {
      "architecture_id": "GPT2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "wjn1996/hugnlp-hugchat-gpt2-large"
      ]
    },
    {
      "architecture_id": "LlavaLlamaImageBindSelectForCausalLM",
      "total_models": 1,
      "sample_models": [
        "dreamerlin/chatbind-7b-delta"
      ]
    },
    {
      "architecture_id": "InstructDittoLMForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "IDEA-CCNL/Ziya-Visual-14B-Chat"
      ]
    },
    {
      "architecture_id": "GPTBigCodeForSequenceClassification",
      "total_models": 1,
      "sample_models": [
        "Myrax3000/starcoderbase1b-personal-copilot-A100-ibanity-lib"
      ]
    },
    {
      "architecture_id": "ZebraForCausalLM",
      "total_models": 1,
      "sample_models": [
        "sogeeking/zebra-burgers-auto"
      ]
    },
    {
      "architecture_id": "UllavaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "jinxu95/ullava"
      ]
    },
    {
      "architecture_id": "HeteroLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Jiabin99/HiGPT"
      ]
    },
    {
      "architecture_id": "MistralDenseFormerForCausalLM",
      "total_models": 1,
      "sample_models": [
        "winglian/mistral-denseformer-7b-pretrained"
      ]
    },
    {
      "architecture_id": "LitaLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "nateraw/lita"
      ]
    },
    {
      "architecture_id": "HuskyForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Liang-ZX/Embodied_family_7b"
      ]
    },
    {
      "architecture_id": "SimpleModel",
      "total_models": 1,
      "sample_models": [
        "toind/tao9llm"
      ]
    },
    {
      "architecture_id": "LlavaQwen2ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "zuijiang/llava-qwen1.5-14B-chat"
      ]
    },
    {
      "architecture_id": "SDLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "dhanesh123in/my_awesome_eli5_clm-model-large-no-mlp-attn-36"
      ]
    },
    {
      "architecture_id": "OracleModel",
      "total_models": 1,
      "sample_models": [
        "underwater45/Oracle-Prototype"
      ]
    },
    {
      "architecture_id": "ZeusModel",
      "total_models": 1,
      "sample_models": [
        "Wonder-Griffin/zeus-model"
      ]
    },
    {
      "architecture_id": "MyQWenLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "yy12138/yuanqianwe"
      ]
    },
    {
      "architecture_id": "CustomModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "diabolic6045/Ion-LLM-Base"
      ]
    },
    {
      "architecture_id": "MiniTransformerModel",
      "total_models": 1,
      "sample_models": [
        "GranuAI/granu-mini"
      ]
    },
    {
      "architecture_id": "GPTLanguageModel",
      "total_models": 1,
      "sample_models": [
        "Ma7ee7/WikiGPT-25M"
      ]
    },
    {
      "architecture_id": "LlamaLadderForCausalLM",
      "total_models": 1,
      "sample_models": [
        "nanami/ladder-last16L-llama3.1-8binstruct-sft4k-stage2v03-bsize32-rkl8b"
      ]
    },
    {
      "architecture_id": "VrindaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ekoahamdutivnasti/VRINDA"
      ]
    },
    {
      "architecture_id": "Phi3WithVectorMemoryForCausalLM",
      "total_models": 1,
      "sample_models": [
        "moelanoby/phi3-mini-M2"
      ]
    },
    {
      "architecture_id": "HuggingFaceCompatibleModel",
      "total_models": 1,
      "sample_models": [
        "Mostafa8Mehrabi/custom-57m-language-model"
      ]
    },
    {
      "architecture_id": "MiniCPMV",
      "total_models": 1,
      "sample_models": [
        "whalezzz/MM-RAIT-MiniCPM-V-2_6"
      ]
    },
    {
      "architecture_id": "GatorForCausalLM",
      "total_models": 1,
      "sample_models": [
        "kunjcr2/GatorGPT2"
      ]
    },
    {
      "architecture_id": "IBCEGPT2LowRank",
      "total_models": 1,
      "sample_models": [
        "jackal79/gpt2-ibce-lowrank-192"
      ]
    },
    {
      "architecture_id": "FinGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Starfish55/fingpt-complete"
      ]
    },
    {
      "architecture_id": "SLLamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "aiintelligentsystems/sllama"
      ]
    },
    {
      "architecture_id": "CustomModel5",
      "total_models": 1,
      "sample_models": [
        "denizyuret-shallowai/foo5"
      ]
    },
    {
      "architecture_id": "QGPT2LMHeadModel_softmax",
      "total_models": 1,
      "sample_models": [
        "TransLL/quant-softmaxtrain"
      ]
    },
    {
      "architecture_id": "QGPT2LMHeadModel_gelu_softmax",
      "total_models": 1,
      "sample_models": [
        "TransLL/quant-gelu_softmaxtrain"
      ]
    },
    {
      "architecture_id": "LuminaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "communityai/apt_zp_v1"
      ]
    },
    {
      "architecture_id": "BharataiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Recag/Bharatai-v-2"
      ]
    },
    {
      "architecture_id": "ViLT5ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "pingzhili/vil-t5-base-clip-vit-base-patch32-mlp"
      ]
    },
    {
      "architecture_id": "RBDashLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "RBDash-Team/rbdash-v1-13b"
      ]
    },
    {
      "architecture_id": "ElasticT5ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "wonjeongho/t5-wmt16-ro-en"
      ]
    },
    {
      "architecture_id": "MLlavaForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Mantis-VL/mllava_mllava_nlvr2_debug_4096"
      ]
    },
    {
      "architecture_id": "VMistralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "HuggingFaceM4/tiny-random-vmistral-m4-split-lm_head"
      ]
    },
    {
      "architecture_id": "LlamaHydraForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Tweeties/tweety-tatar-hydra-mt-7b-v24a"
      ]
    },
    {
      "architecture_id": "GPT2HeadWithValueModel",
      "total_models": 1,
      "sample_models": [
        "tealgreen0503/japanese-gpt2-medium-ppo-araisan"
      ]
    },
    {
      "architecture_id": "AdaVocabGemmaforCausalLM",
      "total_models": 1,
      "sample_models": [
        "reflectio/AdaVocab-Gemma-2b-1024"
      ]
    },
    {
      "architecture_id": "OpenLMforCausalLM",
      "total_models": 1,
      "sample_models": [
        "marianna13/c4_original_openlm_3b"
      ]
    },
    {
      "architecture_id": "PrivateWhisperForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Louagyd/exp_rvq17"
      ]
    },
    {
      "architecture_id": "AxonForCausalLM",
      "total_models": 1,
      "sample_models": [
        "apd-jmorinelli/axon-test-model"
      ]
    },
    {
      "architecture_id": "SVDLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "OSU-AIoT-MLSys-Lab/test_model"
      ]
    },
    {
      "architecture_id": "GPTBigCodeModel",
      "total_models": 1,
      "sample_models": [
        "Colby/starcoder-manhan-corpus"
      ]
    },
    {
      "architecture_id": "SpeechUnitModel",
      "total_models": 1,
      "sample_models": [
        "AlexHung29629/test_speech_unit"
      ]
    },
    {
      "architecture_id": "VulavulaLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "clemsadand/saved_model"
      ]
    },
    {
      "architecture_id": "TCMoEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "stiger1000/TC-MoE"
      ]
    },
    {
      "architecture_id": "Llama3CustomForCausalLM",
      "total_models": 1,
      "sample_models": [
        "blausher/custom-llama"
      ]
    },
    {
      "architecture_id": "GexQwenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MosRat/Gex_V1"
      ]
    },
    {
      "architecture_id": "ArlowForCausalLM",
      "total_models": 1,
      "sample_models": [
        "yuchenxie/arlowgpt-dummy-weights"
      ]
    },
    {
      "architecture_id": "DecoderOnlyTransformer",
      "total_models": 1,
      "sample_models": [
        "Santiago1712/Modell_TinySchakespeare"
      ]
    },
    {
      "architecture_id": "RenneeLlamaHFWrapper",
      "total_models": 1,
      "sample_models": [
        "rvishakhs/renee_gpt_v1"
      ]
    },
    {
      "architecture_id": "Qwen2ForCausalLMNode",
      "total_models": 1,
      "sample_models": [
        "Allen-UQ/Qwen2.5-7B-Instruct-GRPO-Nei-Tokens-k20"
      ]
    },
    {
      "architecture_id": "PatramForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Prince-1/patram-7b-instruct"
      ]
    },
    {
      "architecture_id": "MoLMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "talphaidze/molm-fineweb-edu-scientific_corr"
      ]
    },
    {
      "architecture_id": "GPTOSSMoEModel",
      "total_models": 1,
      "sample_models": [
        "lixinso/xinslm-gpt-oss-moe-micro"
      ]
    },
    {
      "architecture_id": "Qwen3MoeModel",
      "total_models": 1,
      "sample_models": [
        "cs2764/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated-ao-int8wo"
      ]
    },
    {
      "architecture_id": "NSAForCausalLM",
      "total_models": 1,
      "sample_models": [
        "seconds-0/nsa-117m-byte"
      ]
    },
    {
      "architecture_id": "LizardForCausalLM",
      "total_models": 1,
      "sample_models": [
        "LizardModels/Lizard-17m"
      ]
    },
    {
      "architecture_id": "VeronicaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MhaWay/Veronica"
      ]
    },
    {
      "architecture_id": "GPTBertForMaskedLM",
      "total_models": 1,
      "sample_models": [
        "haznitrama/babybabellm-multi_gpu-gpt_bert-eng-main-causal"
      ]
    },
    {
      "architecture_id": "NEEDConversationalModel",
      "total_models": 1,
      "sample_models": [
        "yogami9/need-ai-conversational-model"
      ]
    },
    {
      "architecture_id": "DenseModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "AlgoDriveAI/tinymathstories-130m"
      ]
    },
    {
      "architecture_id": "SanaLLM",
      "total_models": 1,
      "sample_models": [
        "kurumikz/DumbLLM-12M"
      ]
    },
    {
      "architecture_id": "SliderGPT",
      "total_models": 1,
      "sample_models": [
        "c-bone/CrystaLLM-pi_COD-XRD"
      ]
    },
    {
      "architecture_id": "DeepseekOcrForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Molbap/DSOCR_HF"
      ]
    },
    {
      "architecture_id": "GheyaInnovForCausalLM",
      "total_models": 1,
      "sample_models": [
        "LLM-CLEM/Premier-gheya-innov"
      ]
    },
    {
      "architecture_id": "LlamaWithIntervention",
      "total_models": 1,
      "sample_models": [
        "Transluce/features_explain_llama3.1_8b_llama3_8b"
      ]
    },
    {
      "architecture_id": "myQwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "YuanZ77/DeepSeek-R1-Distill-Qwen-7B-SynTS"
      ]
    },
    {
      "architecture_id": "GPT2",
      "total_models": 1,
      "sample_models": [
        "Corianas/64CharGPT"
      ]
    },
    {
      "architecture_id": "GPTNeoXLongForCausalLM",
      "total_models": 1,
      "sample_models": [
        "emozilla/pythia-long-6.9b-scifi-fantasy-673-p6144_c1920_y8192-epoch4"
      ]
    },
    {
      "architecture_id": "MegatronBertForCausalLM",
      "total_models": 1,
      "sample_models": [
        "hf-tiny-model-private/tiny-random-MegatronBertForCausalLM"
      ]
    },
    {
      "architecture_id": "BertForMTSparseFFDEditModel",
      "total_models": 1,
      "sample_models": [
        "zhk/G-SPEED"
      ]
    },
    {
      "architecture_id": "CustomModel3",
      "total_models": 1,
      "sample_models": [
        "denizyuret-shallowai/foo3a"
      ]
    },
    {
      "architecture_id": "Speech2Text2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "pirxus/s2t2_decoder_base"
      ]
    },
    {
      "architecture_id": "NextChatForCausalLM",
      "total_models": 1,
      "sample_models": [
        "AoZhang/nextchat-7b-336-stage2"
      ]
    },
    {
      "architecture_id": "PTPForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "princeton-nlp/ptp"
      ]
    },
    {
      "architecture_id": "MoeLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Charlie911/llama2_vicuna1.5"
      ]
    },
    {
      "architecture_id": "EffLongT5ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "jvelja/t5-bigpatent"
      ]
    },
    {
      "architecture_id": "XionicForCausalLM",
      "total_models": 1,
      "sample_models": [
        "leeloolee/boosted-qwen2-7b"
      ]
    },
    {
      "architecture_id": "Moss2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "red-fox-yj/llava-moss2-2_5b-chat"
      ]
    },
    {
      "architecture_id": "LlavaMonetForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MonetLLM/visionmonet-vd-1.4B-100BT-hf"
      ]
    },
    {
      "architecture_id": "LlamaTTSForCausalLM",
      "total_models": 1,
      "sample_models": [
        "anthony-wss/llama-3.2-1b-tts"
      ]
    },
    {
      "architecture_id": "EryonForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Gargaz/Inventa"
      ]
    },
    {
      "architecture_id": "LLaMAModel",
      "total_models": 1,
      "sample_models": [
        "kadeyshvili/my_implementation_llama_new_new5"
      ]
    },
    {
      "architecture_id": "nGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "p2o6e100/c4ngpt800m"
      ]
    },
    {
      "architecture_id": "MyPhiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "sh-haruta/phi2_prune_test"
      ]
    },
    {
      "architecture_id": "MyQwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "sh-haruta/qwen2_prune_test"
      ]
    },
    {
      "architecture_id": "LstmForCausalLM",
      "total_models": 1,
      "sample_models": [
        "AlexHung29629/lstm_v1"
      ]
    },
    {
      "architecture_id": "Wbot_1_5",
      "total_models": 1,
      "sample_models": [
        "WZH-Team/Wbot-1-5-small"
      ]
    },
    {
      "architecture_id": "KW2LMHeadModel",
      "total_models": 1,
      "sample_models": [
        "pritam77/kiwiii-0.1-16k"
      ]
    },
    {
      "architecture_id": "ArgonneModelParallel",
      "total_models": 1,
      "sample_models": [
        "PursuitOfDataScience/Argonne-1.0-Instruct"
      ]
    },
    {
      "architecture_id": "VllmTFBQwen2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "n1h111sm/TFB-Qwen2.5-3B-Instruct"
      ]
    },
    {
      "architecture_id": "TitansMACTransformer",
      "total_models": 1,
      "sample_models": [
        "himanshu-skid19/titans-mac-transformer"
      ]
    },
    {
      "architecture_id": "TyneRoxModel",
      "total_models": 1,
      "sample_models": [
        "bobboyms/tynerox"
      ]
    },
    {
      "architecture_id": "KBLaMPhi3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "EmanuelBP/kblam_phi3_01"
      ]
    },
    {
      "architecture_id": "LlamaMLAForCausalLM",
      "total_models": 1,
      "sample_models": [
        "BarraHome/llama3_2-1B-deepseek"
      ]
    },
    {
      "architecture_id": "EnsembleModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ZeeeWP/Qwen3-8B_Qwen3-06B"
      ]
    },
    {
      "architecture_id": "Qwen3ForSequenceClassification",
      "total_models": 1,
      "sample_models": [
        "yangdaye/Qwen3-0.6B-Instruct-YXY"
      ]
    },
    {
      "architecture_id": "EditableQwen3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "fdfdfddsfdfdfd/qwen3-0.6B-editable-demo"
      ]
    },
    {
      "architecture_id": "LlavaMinervaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "aimagelab/LLaVA-MORE-Minerva"
      ]
    },
    {
      "architecture_id": "Qwen2ForCausalLMwithHRM",
      "total_models": 1,
      "sample_models": [
        "FlippyDora/Qwen2_5_3B_inst_hrm_init"
      ]
    },
    {
      "architecture_id": "FSDPLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "smohammadi/bf16-llama-3B"
      ]
    },
    {
      "architecture_id": "T5GemmaForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "MLGResearch/cleaver_t5g_ss"
      ]
    },
    {
      "architecture_id": "GPTModelForTextGeneration",
      "total_models": 1,
      "sample_models": [
        "adityamore2004/Final-gpt-124"
      ]
    },
    {
      "architecture_id": "AdaptiveRiverLM",
      "total_models": 1,
      "sample_models": [
        "Alienanthony/ROE_EDU_BASE_Undercooked"
      ]
    },
    {
      "architecture_id": "TinyLLM",
      "total_models": 1,
      "sample_models": [
        "anujbhatt4ai/tiny-math-llm"
      ]
    },
    {
      "architecture_id": "BagelForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "sensenova/SenseNova-SI-1.1-BAGEL-7B-MoT"
      ]
    },
    {
      "architecture_id": "MusicFlamingoForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "lashahub/music-flamingo"
      ]
    },
    {
      "architecture_id": "GPTNeoModel",
      "total_models": 1,
      "sample_models": [
        "thirupathibandam/gpt-neo-125m"
      ]
    },
    {
      "architecture_id": "SledForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "rusheeliyer/sled_mt5-base-7"
      ]
    },
    {
      "architecture_id": "TuringMMForCausalLM",
      "total_models": 1,
      "sample_models": [
        "lightyear-turing/TuringMM-34B-Chat"
      ]
    },
    {
      "architecture_id": "LOCOSTForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "flbbb/locost-gsg-pretrained"
      ]
    },
    {
      "architecture_id": "DribbleLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "awilliamson/dribble-llama"
      ]
    },
    {
      "architecture_id": "MoeModel",
      "total_models": 1,
      "sample_models": [
        "kanhatakeyama/TestMoE"
      ]
    },
    {
      "architecture_id": "Int8LlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "robertgshaw2/tinyllama-sq-test"
      ]
    },
    {
      "architecture_id": "TCVForCausalLM",
      "total_models": 1,
      "sample_models": [
        "AhmadShapiro/tcv-phi3"
      ]
    },
    {
      "architecture_id": "PllavaForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "gutianpei/pllava_lora16"
      ]
    },
    {
      "architecture_id": "MovaLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "zongzhuofan/llama3-mova-8b"
      ]
    },
    {
      "architecture_id": "MoELLaVAQwen1_5ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "tuanio/MoE-LLaVA-Qwen1.5-1.8Bx4-Top2"
      ]
    },
    {
      "architecture_id": "GPTForHF",
      "total_models": 1,
      "sample_models": [
        "Uniform-Distribution/uniform-test-162M-base"
      ]
    },
    {
      "architecture_id": "MinGRUForCausalLM",
      "total_models": 1,
      "sample_models": [
        "symanto/mingru-shakespeare"
      ]
    },
    {
      "architecture_id": "Miridih_LlavaForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Jooyoung1/Miridih-Llava-7b-hf"
      ]
    },
    {
      "architecture_id": "LLaMaModelHub",
      "total_models": 1,
      "sample_models": [
        "Alexanders/custom-llama-hse-casual-lm3"
      ]
    },
    {
      "architecture_id": "QPhiForCausalLM",
      "total_models": 1,
      "sample_models": [
        "kaizen9/test-5662"
      ]
    },
    {
      "architecture_id": "ModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ikkiren/research_merged_layer"
      ]
    },
    {
      "architecture_id": "AveyForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Hibiki711/automodel-test0"
      ]
    },
    {
      "architecture_id": "RepeatedForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Rano23/OpenR1-Qwen-7B-Math-repeat-B2048"
      ]
    },
    {
      "architecture_id": "AnubisMoeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "RaghuCourage9605/Anubis-559M"
      ]
    },
    {
      "architecture_id": "ConvaiCausalLM",
      "total_models": 1,
      "sample_models": [
        "convaiinnovations/hindi-causal-lm"
      ]
    },
    {
      "architecture_id": "MoonshotKimiaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Seungyoun/Kimi-Audio-7B-Instruct"
      ]
    },
    {
      "architecture_id": "RewardModel",
      "total_models": 1,
      "sample_models": [
        "Satori-reasoning/Satori-RM-7B"
      ]
    },
    {
      "architecture_id": "EMGForCausalLM",
      "total_models": 1,
      "sample_models": [
        "NeTS-lab/emg-10m-conv_test"
      ]
    },
    {
      "architecture_id": "LIMeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "gudleifrr/lime_659"
      ]
    },
    {
      "architecture_id": "GenerativePromptLlama",
      "total_models": 1,
      "sample_models": [
        "KatjaK/Llama-3B-PT-Baseline"
      ]
    },
    {
      "architecture_id": "MuonGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "efittschen/MuonGPT-100M_2750"
      ]
    },
    {
      "architecture_id": "PositionXLNetForCausalLM",
      "total_models": 1,
      "sample_models": [
        "efittschen/xlnet_2o"
      ]
    },
    {
      "architecture_id": "GPTMiniForCausalLM",
      "total_models": 1,
      "sample_models": [
        "JonusNattapong/gptoss-mini-thaichat"
      ]
    },
    {
      "architecture_id": "Qwen3AgenticForCausalLM",
      "total_models": 1,
      "sample_models": [
        "BroAlanTaps/Qwen3-Agentic-9B"
      ]
    },
    {
      "architecture_id": "YForCausalLM1_1",
      "total_models": 1,
      "sample_models": [
        "SnifferCaptain/YModel1.1"
      ]
    },
    {
      "architecture_id": "CuriousForCausalLM",
      "total_models": 1,
      "sample_models": [
        "curiousteam/adam"
      ]
    },
    {
      "architecture_id": "BitSkipV1ForCausalLMWithEarlyExit",
      "total_models": 1,
      "sample_models": [
        "Ram07/bitskip-v1-earlyexit"
      ]
    },
    {
      "architecture_id": "MaplePTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "canxp-ai/canxpai-maplept-mini-v1"
      ]
    },
    {
      "architecture_id": "WikiMiniModel",
      "total_models": 1,
      "sample_models": [
        "karthyick/tinystories-24.5m-article-generation"
      ]
    },
    {
      "architecture_id": "BrumbyForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ModelCloud/Brumby-14B-Base-GPTQMODEL-W4A16"
      ]
    },
    {
      "architecture_id": "CelerityLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "melhoushi/celerity_270m_20tpp"
      ]
    },
    {
      "architecture_id": "OuroForCausalLMQuantized",
      "total_models": 1,
      "sample_models": [
        "rayf-07/Ouro-2.6B_smoothquant_W8A8"
      ]
    },
    {
      "architecture_id": "MuxX11ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "megharudushi/Mux-X11"
      ]
    },
    {
      "architecture_id": "CustomTagalogLLM",
      "total_models": 1,
      "sample_models": [
        "MaAIos/Henyo-70M"
      ]
    },
    {
      "architecture_id": "A2DQwen2LMHeadModel",
      "total_models": 1,
      "sample_models": [
        "ChaosAIVision/Qwen2.5-1.5B-ddlm-bd3lm-pretrain-5550-vi"
      ]
    },
    {
      "architecture_id": "CustomModel",
      "total_models": 1,
      "sample_models": [
        "denizyuret-shallowai/foo4"
      ]
    },
    {
      "architecture_id": "LlavaBaichuan2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "vivym/llava-baichuan2-7b"
      ]
    },
    {
      "architecture_id": "SymbolicGPTLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "DataAnalyticsLab/SymbolicGPT-1Vars-30Points-1Ys"
      ]
    },
    {
      "architecture_id": "LlavaT5ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "zhiqiulin/clip-flant5-xl-gpt4v"
      ]
    },
    {
      "architecture_id": "PhiMoLForCausalLM",
      "total_models": 1,
      "sample_models": [
        "maidacundo/text2sql_phi_molora"
      ]
    },
    {
      "architecture_id": "GPTLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "greyfoss/simple-gpt-doupo"
      ]
    },
    {
      "architecture_id": "DartForCausalLM",
      "total_models": 1,
      "sample_models": [
        "p1atdev/dart-base-random"
      ]
    },
    {
      "architecture_id": "CatsModel",
      "total_models": 1,
      "sample_models": [
        "thrunlab/cats_exp"
      ]
    },
    {
      "architecture_id": "MemoryModel",
      "total_models": 1,
      "sample_models": [
        "ssaha3/memory-model"
      ]
    },
    {
      "architecture_id": "BITForCausalLM",
      "total_models": 1,
      "sample_models": [
        "kaizen9/llama-finetuned-norm-only-fp16"
      ]
    },
    {
      "architecture_id": "TransformerWithPruningForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Erland/softpick-pruned-340M-4096-model"
      ]
    },
    {
      "architecture_id": "OptrixForCausalLM",
      "total_models": 1,
      "sample_models": [
        "SVECTOR-CORPORATION/Optrix-1-0257"
      ]
    },
    {
      "architecture_id": "PicaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "AlexHung29629/pica_model"
      ]
    },
    {
      "architecture_id": "InternVLAN1ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "InternRobotics/InternVLA-N1-Preview"
      ]
    },
    {
      "architecture_id": "BuddyGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "learn2pro/buddygpt-0.3b-chat"
      ]
    },
    {
      "architecture_id": "VSBForCausalLM",
      "total_models": 1,
      "sample_models": [
        "itsankitkp/vsb-200m1"
      ]
    },
    {
      "architecture_id": "AyaVisionForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "Jaward/afri-aya-vision-krio-8b"
      ]
    },
    {
      "architecture_id": "BitSkipV3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Ram07/bitskip-v3-earlyexit"
      ]
    },
    {
      "architecture_id": "LlamaSparseForCausalLM",
      "total_models": 1,
      "sample_models": [
        "seele123/DeepSeek-R1-Distill-Llama-8B-TEAL"
      ]
    },
    {
      "architecture_id": "MatformerForCausalLM",
      "total_models": 1,
      "sample_models": [
        "CCC-Unito/BAMBINO-0.1"
      ]
    },
    {
      "architecture_id": "MeghaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "MeghaOS/Megha"
      ]
    },
    {
      "architecture_id": "RustNNGPT",
      "total_models": 1,
      "sample_models": [
        "alexandercoop/tiny_shakespeare"
      ]
    },
    {
      "architecture_id": "MoEQwen3B",
      "total_models": 1,
      "sample_models": [
        "Kiy-K/fyodor-agentic-v1.1"
      ]
    },
    {
      "architecture_id": "PTTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Raziel1234/PTT-1-110M"
      ]
    },
    {
      "architecture_id": "NovaVoidComplete",
      "total_models": 1,
      "sample_models": [
        "Sqersters/Void"
      ]
    },
    {
      "architecture_id": "AxonModel",
      "total_models": 1,
      "sample_models": [
        "oscar128372/axon-nano-6m"
      ]
    },
    {
      "architecture_id": "ResnetModelForImageClassification",
      "total_models": 1,
      "sample_models": [
        "zpbrent/test"
      ]
    },
    {
      "architecture_id": "TomatoForCausalLM",
      "total_models": 1,
      "sample_models": [
        "OneJz/tomato-8b"
      ]
    },
    {
      "architecture_id": "PhixtralForCausalLM",
      "total_models": 1,
      "sample_models": [
        "mzbac/phi2-2x4-hf-4bit-mlx"
      ]
    },
    {
      "architecture_id": "LlavaGPT2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "res1235/MoE-LLaVA-llm-jp-1.3b-v1.0"
      ]
    },
    {
      "architecture_id": "NDLForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Ndlcwx/NDLSLM_0.8B-base"
      ]
    },
    {
      "architecture_id": "ExpandedJetMoEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "thomasgauthier/expanded-jetmoe-untrained"
      ]
    },
    {
      "architecture_id": "Model",
      "total_models": 1,
      "sample_models": [
        "EduardoSlonski/test_clever_3"
      ]
    },
    {
      "architecture_id": "GobbledygookForCausalLM",
      "total_models": 1,
      "sample_models": [
        "mrsamsami/llama3-8b-overfit-mmlu-test"
      ]
    },
    {
      "architecture_id": "LongformerEncoderBARTDecoderForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "nguyenkhoa/LongBartRecipe1M_with_title_v3.1"
      ]
    },
    {
      "architecture_id": "LLaMA_model",
      "total_models": 1,
      "sample_models": [
        "TmBoris/custom-llama2"
      ]
    },
    {
      "architecture_id": "NDAForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Q-bert/nda-tiny"
      ]
    },
    {
      "architecture_id": "ImbalanceTexxWithBertForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "twanghcmut/imbalancetexx-ver2-1"
      ]
    },
    {
      "architecture_id": "RafflesiaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "RafflesiaAI/Rafflesia1-7B"
      ]
    },
    {
      "architecture_id": "AuroraForCausalLM",
      "total_models": 1,
      "sample_models": [
        "scrumlaltda/sl-aurora-01KG"
      ]
    },
    {
      "architecture_id": "Qwen2VLExtendedForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "gzpeng1984/Qwen-audio"
      ]
    },
    {
      "architecture_id": "Qwen2LayerwiseSAEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Vadim21221/qwen2_1_5b-instruct-layerwise-sae_lr_1e_7"
      ]
    },
    {
      "architecture_id": "UnivaQwen2p5VLForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "UCSC-VLAA/gpt-image-edit-finetune-t5-only"
      ]
    },
    {
      "architecture_id": "GPTBERTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Jorgeis1/jorge-babylm-10m-gptbert"
      ]
    },
    {
      "architecture_id": "MtGemmaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "mremila/gemma-2b-MT"
      ]
    },
    {
      "architecture_id": "MBZTestModelForCausalLM",
      "total_models": 1,
      "sample_models": [
        "michaelbzhu/test-7.6B-base"
      ]
    },
    {
      "architecture_id": "Gemma2CapreseForCausalLM",
      "total_models": 1,
      "sample_models": [
        "hdong0/gemma2_dummy_cap"
      ]
    },
    {
      "architecture_id": "HFGPTModel",
      "total_models": 1,
      "sample_models": [
        "FernandoHPaiva/modeloChat_vireya"
      ]
    },
    {
      "architecture_id": "BWQwen3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "toilaluan/Qwen3-0.6B-BW"
      ]
    },
    {
      "architecture_id": "AriesForCausalLM",
      "total_models": 1,
      "sample_models": [
        "navflyer/aries-reasoning-1.5b"
      ]
    },
    {
      "architecture_id": "LlamaMixLoRAForCausalLM",
      "total_models": 1,
      "sample_models": [
        "twanghcmut/llama-mixlora-2"
      ]
    },
    {
      "architecture_id": "HenyoModel",
      "total_models": 1,
      "sample_models": [
        "MaAIos/Henyo-153M"
      ]
    },
    {
      "architecture_id": "FM9GForCausalLM",
      "total_models": 1,
      "sample_models": [
        "bowmanhan/jiuge-9G4B"
      ]
    },
    {
      "architecture_id": "LlasaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "bezzam/Llasa-1B"
      ]
    },
    {
      "architecture_id": "HyperpartisanModel",
      "total_models": 1,
      "sample_models": [
        "Joshua4a/xlm-roberta-hp-ftemb"
      ]
    },
    {
      "architecture_id": "CustomBioGptForCausalLM",
      "total_models": 1,
      "sample_models": [
        "cja5553/BJH-perioperative-notes-bioGPT"
      ]
    },
    {
      "architecture_id": "pzdrk-reasoning",
      "total_models": 1,
      "sample_models": [
        "pzdrk/pzdrk-R1"
      ]
    },
    {
      "architecture_id": "Chromos-AGI",
      "total_models": 1,
      "sample_models": [
        "ArcOffical/ChromosAGI-X1"
      ]
    },
    {
      "architecture_id": "CoffeeChatAI",
      "total_models": 1,
      "sample_models": [
        "topboykrepta/CoffeeChatAI"
      ]
    },
    {
      "architecture_id": "FixedEnhancedHybridTransformer",
      "total_models": 1,
      "sample_models": [
        "shivash/enhanced-hybrid-transformer-fixed-1758805039"
      ]
    },
    {
      "architecture_id": "SofanorForCausalLM",
      "total_models": 1,
      "sample_models": [
        "sofanorai/sfnr-llm"
      ]
    },
    {
      "architecture_id": "YAZHLMHeadModel",
      "total_models": 1,
      "sample_models": [
        "KavyaSwethaJ/Yazh"
      ]
    },
    {
      "architecture_id": "GliDeForCausalLM",
      "total_models": 1,
      "sample_models": [
        "cxdu/glide-vicuna7b"
      ]
    },
    {
      "architecture_id": "SermentalForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Sermental/Sermental-Instruct-7.3B"
      ]
    },
    {
      "architecture_id": "NllbForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "tachicart/nllb-ft-darija"
      ]
    },
    {
      "architecture_id": "ABU2HEADMODEL",
      "total_models": 1,
      "sample_models": [
        "botintel-community/abu-ai-001"
      ]
    },
    {
      "architecture_id": "BertForQuestionAnswering",
      "total_models": 1,
      "sample_models": [
        "Qusaiiii/NewAccountant"
      ]
    },
    {
      "architecture_id": "RLlamaForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ledong0110/RLlama-3.2-3B-Base"
      ]
    },
    {
      "architecture_id": "OkamelaAIForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ZeppelinCorp/Okamela"
      ]
    },
    {
      "architecture_id": "QWenForCausalLM",
      "total_models": 1,
      "sample_models": [
        "Somya1834/fc-deepseek-finetuned-50"
      ]
    },
    {
      "architecture_id": "jarvis-x core",
      "total_models": 1,
      "sample_models": [
        "vhaan/jarvis-x-identity-model"
      ]
    },
    {
      "architecture_id": "NGen3ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "TNSA/NGen3-1B"
      ]
    },
    {
      "architecture_id": "Optimus3ForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "xieyuquan/Optimus3-32B-SFT"
      ]
    },
    {
      "architecture_id": "Mycoach",
      "total_models": 1,
      "sample_models": [
        "lior23/Mycoach"
      ]
    },
    {
      "architecture_id": "NGen3ForCasualLM",
      "total_models": 1,
      "sample_models": [
        "TNSA/NGen3-10B"
      ]
    },
    {
      "architecture_id": "MultiModalSuperModel",
      "total_models": 1,
      "sample_models": [
        "ArcOffical/Arc-Ultra"
      ]
    },
    {
      "architecture_id": "ConditionalGPT",
      "total_models": 1,
      "sample_models": [
        "c-bone/CrystaLLM-pi_bandgap"
      ]
    },
    {
      "architecture_id": "CustomLoRAModel",
      "total_models": 1,
      "sample_models": [
        "hson1003/50cu_ethics_model"
      ]
    },
    {
      "architecture_id": "GptOssVLForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "AlexHung29629/gpt-oss-20b-vl-sft-output-8"
      ]
    },
    {
      "architecture_id": "TrouterForCausalLM",
      "total_models": 1,
      "sample_models": [
        "OpenTrouter/Trouter-20b"
      ]
    },
    {
      "architecture_id": "PhoGPTForCausalLM",
      "total_models": 1,
      "sample_models": [
        "LilPhat23/PhoGPT"
      ]
    },
    {
      "architecture_id": "LlamaOraForCausalLM",
      "total_models": 1,
      "sample_models": [
        "oracomputing/LlamaOra-6.4B-Instruct-FP8"
      ]
    },
    {
      "architecture_id": "ShivikM2ForCausalLM",
      "total_models": 1,
      "sample_models": [
        "ziadrone/shivik-m2-2b"
      ]
    },
    {
      "architecture_id": "Qwen2_5OmniForConditionalGeneration",
      "total_models": 1,
      "sample_models": [
        "XD-MU/ScriptAgent"
      ]
    },
    {
      "architecture_id": "ExaoneMoEForCausalLM",
      "total_models": 1,
      "sample_models": [
        "mlx-community/K-EXAONE-236B-A23B-6bit"
      ]
    }
  ]
}