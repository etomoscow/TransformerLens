"""Gemma-3 specialized attention bridge.

Gemma-3 uses a unique dual RoPE (Rotary Position Embedding) system that requires
position_embeddings to be generated from the model's rotary_emb component rather
than using the standard (cos, sin) tuple format.
"""

from __future__ import annotations

from typing import Any, Dict, Optional

import torch

from transformer_lens.model_bridge.generalized_components.attention import (
    AttentionBridge,
)


class PositionEmbeddingsAttentionBridge(AttentionBridge):
    """Attention bridge for models that require position embeddings (e.g., Gemma-3).

    Some models use specialized position embedding systems (like Gemma-3's dual RoPE)
    which require position_embeddings to be generated in a specific format that differs
    from standard RoPE models.

    The position_embeddings are generated by calling the model's rotary_emb
    component with dummy Q/K tensors and position_ids.
    """

    def __init__(
        self,
        name: str,
        config: Any,
        submodules: Optional[Dict[str, Any]] = None,
        **kwargs,
    ):
        """Initialize Gemma-3 attention bridge.

        Args:
            name: Component name
            config: Model configuration
            submodules: Dictionary of subcomponents
            **kwargs: Additional arguments passed to AttentionBridge
        """
        # Gemma-3 always requires position_embeddings and attention_mask
        kwargs["requires_position_embeddings"] = True
        kwargs["requires_attention_mask"] = True

        # Maintain native attention to prevent parent from wrapping HF forward
        # PositionEmbeddingsAttentionBridge has its own forward that handles these correctly
        kwargs["maintain_native_attention"] = True

        super().__init__(name, config, submodules, **kwargs)

        # Reference to model's rotary_emb component (will be set later)
        self._rotary_emb = None

    def set_rotary_emb(self, rotary_emb):
        """Set reference to the model's rotary embedding component.

        Args:
            rotary_emb: The model's rotary_emb component (from model.model.rotary_emb)
        """
        self._rotary_emb = rotary_emb

    def get_random_inputs(
        self,
        batch_size: int = 2,
        seq_len: int = 8,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> Dict[str, Any]:
        """Generate random inputs for Gemma-3 attention testing.

        Gemma-3's position_embeddings are generated by calling rotary_emb(seq_len, device)
        which returns a tuple of (cos, sin) tensors with shape [seq_len, head_dim].

        Args:
            batch_size: Batch size for generated inputs
            seq_len: Sequence length for generated inputs
            device: Device to place tensors on
            dtype: Dtype for generated tensors

        Returns:
            Dictionary with keys: hidden_states, position_embeddings, attention_mask
        """
        if device is None:
            device = torch.device("cpu")
        if dtype is None:
            dtype = torch.float32

        # Generate base hidden_states
        d_model = self.config.d_model if self.config and hasattr(self.config, "d_model") else 1152
        inputs: Dict[str, Any] = {
            "hidden_states": torch.randn(batch_size, seq_len, d_model, device=device, dtype=dtype)
        }

        # Generate Gemma-3-specific position_embeddings
        # Gemma-3 attention expects position_embeddings as a tuple of (cos, sin) with shape [1, seq_len, head_dim]
        # Note: batch dimension is ALWAYS 1, regardless of actual batch_size
        num_heads = (
            self.config.num_attention_heads
            if self.config and hasattr(self.config, "num_attention_heads")
            else 4
        )
        head_dim = self.config.head_dim if self.config and hasattr(self.config, "head_dim") else 256

        # Create dummy Q/K tensor and position_ids with batch=1
        dummy_qk = torch.randn(1, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        position_ids = torch.arange(seq_len, device=device).unsqueeze(0)  # [1, seq_len]

        # Generate position_embeddings using rotary_emb
        if self._rotary_emb is not None:
            # Use the model's actual rotary_emb component
            try:
                position_embeddings = self._rotary_emb(dummy_qk, position_ids)
                inputs["position_embeddings"] = position_embeddings
            except Exception as e:
                # Fallback: generate dummy position_embeddings with correct shape
                # Gemma-3 expects tuple of 2 tensors, each [1, seq, head_dim]
                cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                inputs["position_embeddings"] = (cos, sin)
        else:
            # Fallback if rotary_emb not available: generate dummy position_embeddings
            # Gemma-3 expects tuple of 2 tensors, each [1, seq, head_dim]
            cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
            sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
            inputs["position_embeddings"] = (cos, sin)

        # Generate attention_mask
        # Gemma-3 attention expects attention_mask to be None when called directly
        inputs["attention_mask"] = None

        return inputs

    def setup_no_processing_hooks(self) -> None:
        """Wrap HF attention forward to inject required position_embeddings and attention_mask.

        This wrapping is necessary because some code paths (e.g., weight modification tests)
        may call the HF attention's forward directly, bypassing the bridge's forward method.
        """
        if self.original_component is None:
            return

        # Store reference to original HF forward
        original_hf_forward = self.original_component.forward

        def wrapped_forward(*args, **kwargs):
            """Wrapper that injects position_embeddings and attention_mask if missing."""
            # Generate position_embeddings if missing
            if "position_embeddings" not in kwargs or kwargs["position_embeddings"] is None:
                # Need hidden_states to determine batch size and sequence length
                hidden_states = kwargs.get("hidden_states") or (args[0] if len(args) > 0 else None)
                if hidden_states is not None:
                    batch_size, seq_len, _ = hidden_states.shape
                    device = hidden_states.device
                    dtype = hidden_states.dtype

                    # Generate position_embeddings using rotary_emb
                    if self._rotary_emb is not None:
                        head_dim = (
                            self.config.head_dim
                            if self.config and hasattr(self.config, "head_dim")
                            else 256
                        )
                        num_heads = (
                            self.config.num_attention_heads
                            if self.config and hasattr(self.config, "num_attention_heads")
                            else 4
                        )
                        dummy_qk = torch.randn(
                            1, seq_len, num_heads, head_dim, device=device, dtype=dtype
                        )
                        position_ids = torch.arange(seq_len, device=device).unsqueeze(0)

                        try:
                            kwargs["position_embeddings"] = self._rotary_emb(dummy_qk, position_ids)
                        except Exception:
                            cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                            sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                            kwargs["position_embeddings"] = (cos, sin)
                    else:
                        head_dim = (
                            self.config.head_dim
                            if self.config and hasattr(self.config, "head_dim")
                            else 256
                        )
                        cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                        sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                        kwargs["position_embeddings"] = (cos, sin)

            # Ensure attention_mask is set
            if "attention_mask" not in kwargs:
                kwargs["attention_mask"] = None

            # Call original forward
            return original_hf_forward(*args, **kwargs)

        # Replace HF forward with wrapper
        self.original_component.forward = wrapped_forward
        self._hf_forward_wrapped = True

        # Still setup hook_z reshaping if needed
        if hasattr(self, "o") and self.o is not None and hasattr(self.config, "n_heads"):
            self._setup_hook_z_reshape()

    def forward(self, *args: Any, **kwargs: Any) -> Any:
        """Simplified forward pass - minimal wrapping around original component.

        This override strips back the complex processing in AttentionBridge._process_output()
        to do minimal wrapping: hook_in → delegate → hook_out.

        This ensures we match HuggingFace's exact output without any intermediate
        pattern extraction or re-application logic.

        Args:
            *args: Input arguments to pass to the original component
            **kwargs: Input keyword arguments to pass to the original component

        Returns:
            The output from the original component, with only input/output hooks applied
        """
        if self.original_component is None:
            raise RuntimeError(
                f"Original component not set for {self.name}. Call set_original_component() first."
            )

        # Get the target dtype from the original component's parameters
        target_dtype = None
        try:
            target_dtype = next(self.original_component.parameters()).dtype
        except StopIteration:
            # Component has no parameters, keep inputs as-is
            pass

        # Apply input hook to hidden_states
        # Gemma3Attention expects hidden_states kwarg, but in compatibility mode
        # it may be passed as a positional argument
        if "hidden_states" in kwargs:
            hooked = self.hook_in(kwargs["hidden_states"])
            # Cast to target dtype if needed
            if (
                target_dtype is not None
                and isinstance(hooked, torch.Tensor)
                and hooked.is_floating_point()
            ):
                hooked = hooked.to(dtype=target_dtype)
            kwargs["hidden_states"] = hooked
        elif len(args) > 0:
            # hidden_states passed as first positional argument (compatibility mode)
            hooked = self.hook_in(args[0])
            # Cast to target dtype if needed
            if (
                target_dtype is not None
                and isinstance(hooked, torch.Tensor)
                and hooked.is_floating_point()
            ):
                hooked = hooked.to(dtype=target_dtype)
            args = (hooked,) + args[1:]
            # Move it to kwargs for HF compatibility
            kwargs["hidden_states"] = args[0]
            args = args[1:]
        else:
            raise ValueError("Gemma3Attention requires hidden_states argument")

        # Generate position_embeddings if missing (required for Gemma-3)
        if "position_embeddings" not in kwargs or kwargs["position_embeddings"] is None:
            # Get hidden_states to determine batch size and sequence length
            hidden_states = kwargs["hidden_states"]
            batch_size, seq_len, _ = hidden_states.shape
            device = hidden_states.device
            dtype = hidden_states.dtype

            # Generate position_embeddings using rotary_emb
            if self._rotary_emb is not None:
                # Create dummy Q/K tensor and position_ids with batch=1
                head_dim = (
                    self.config.head_dim
                    if self.config and hasattr(self.config, "head_dim")
                    else 256
                )
                num_heads = (
                    self.config.num_attention_heads
                    if self.config and hasattr(self.config, "num_attention_heads")
                    else 4
                )
                dummy_qk = torch.randn(1, seq_len, num_heads, head_dim, device=device, dtype=dtype)
                position_ids = torch.arange(seq_len, device=device).unsqueeze(0)  # [1, seq_len]

                try:
                    kwargs["position_embeddings"] = self._rotary_emb(dummy_qk, position_ids)
                except Exception:
                    # Fallback: generate dummy position_embeddings with correct shape
                    cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                    sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                    kwargs["position_embeddings"] = (cos, sin)
            else:
                # Fallback if rotary_emb not available
                head_dim = (
                    self.config.head_dim
                    if self.config and hasattr(self.config, "head_dim")
                    else 256
                )
                cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                kwargs["position_embeddings"] = (cos, sin)

        # Ensure attention_mask is set (Gemma-3 expects None when not provided)
        if "attention_mask" not in kwargs:
            kwargs["attention_mask"] = None

        # Request attention weights to fire hook_attn_scores and hook_pattern
        # This is needed for HookedTransformer compatibility
        kwargs["output_attentions"] = True

        # Forward through original component with output_attentions=True
        output = self.original_component(*args, **kwargs)

        # Extract attention weights if available and fire hooks
        # HF returns tuple: (hidden_states, attention_weights, ...)
        if isinstance(output, tuple) and len(output) >= 2:
            attn_weights = output[1]

            # Fire attention pattern hooks if weights are available
            if attn_weights is not None:
                # For HookedTransformer compatibility, we need both attn_scores (pre-softmax)
                # and attn_pattern (post-softmax). HF only gives us post-softmax.
                # We can approximate pre-softmax by taking the inverse softmax, but this
                # is only for hook compatibility - the actual computation still uses HF's values.

                # Fire hook_pattern with the attention weights (post-softmax)
                _ = self.hook_pattern(attn_weights)

                # For hook_attn_scores, we need pre-softmax values. Since HF doesn't provide these
                # when using SDPA, we'll compute them approximately using log + scaling.
                # This is only for observability - the actual forward pass still uses HF's exact computation.
                with torch.no_grad():
                    # Convert from probabilities back to logits (approximate)
                    # attn_scores ≈ log(attn_weights) * sqrt(d_head)
                    # This won't be exact but allows the hook to fire for interpretability
                    d_head = (
                        self.config.head_dim
                        if self.config and hasattr(self.config, "head_dim")
                        else 256
                    )
                    # Avoid log(0) by adding small epsilon
                    attn_scores_approx = torch.log(attn_weights + 1e-10) * (d_head**0.5)

                # Fire hook_attn_scores with approximate pre-softmax scores
                # Note: This is detached from the computational graph, so it doesn't affect gradients
                _ = self.hook_attn_scores(attn_scores_approx)

        # Apply hook_out to the output
        # HF attention returns a tuple, but the decoder layer expects exactly 2 values:
        # (hidden_states, attention_weights)
        # Some models may return additional values (e.g., past_key_value for caching)
        # but we must only return the first 2 to match what the decoder layer expects
        if isinstance(output, tuple) and len(output) >= 2:
            # Return only first 2 elements: (hooked_hidden_states, attention_weights)
            output = (self.hook_out(output[0]), output[1])
        elif isinstance(output, tuple) and len(output) == 1:
            # Single element tuple (unusual but handle it)
            output = (self.hook_out(output[0]),)
        else:
            # Not a tuple, just hook the output directly
            output = self.hook_out(output)

        return output
