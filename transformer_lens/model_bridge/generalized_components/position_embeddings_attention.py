"""Gemma-3 specialized attention bridge.

Gemma-3 uses a unique dual RoPE (Rotary Position Embedding) system that requires
position_embeddings to be generated from the model's rotary_emb component rather
than using the standard (cos, sin) tuple format.
"""
from __future__ import annotations

import warnings
from typing import Any, Dict, Optional

import torch
import transformers.models.gemma2.modeling_gemma2 as gemma2_module

from transformer_lens.hook_points import HookPoint
from transformer_lens.model_bridge.generalized_components.attention import (
    AttentionBridge,
)


class PositionEmbeddingsAttentionBridge(AttentionBridge):
    """Attention bridge for models that require position embeddings (e.g., Gemma-3).

    Some models use specialized position embedding systems (like Gemma-3's dual RoPE)
    which require position_embeddings to be generated in a specific format that differs
    from standard RoPE models.

    The position_embeddings are generated by calling the model's rotary_emb
    component with dummy Q/K tensors and position_ids.
    """

    def __init__(
        self, name: str, config: Any, submodules: Optional[Dict[str, Any]] = None, **kwargs
    ):
        """Initialize Gemma-3 attention bridge.

        Args:
            name: Component name
            config: Model configuration
            submodules: Dictionary of subcomponents
            **kwargs: Additional arguments passed to AttentionBridge
        """
        kwargs["requires_position_embeddings"] = True
        kwargs["requires_attention_mask"] = True
        kwargs["maintain_native_attention"] = True
        super().__init__(name, config, submodules, **kwargs)
        self._rotary_emb = None
        # Add hooks for cos and sin to match HookedTransformer pattern
        self.hook_cos = HookPoint()
        self.hook_sin = HookPoint()

    def set_rotary_emb(self, rotary_emb):
        """Set reference to the model's rotary embedding component.

        Args:
            rotary_emb: The model's rotary_emb component (from model.model.rotary_emb)
        """
        self._rotary_emb = rotary_emb

    def _apply_position_embedding_hooks(self, position_embeddings):
        """Apply hooks to position embeddings (cos, sin tuple).

        Extracts cos and sin from the position_embeddings tuple and passes them
        through hook_cos and hook_sin to match HookedTransformer's behavior.

        Args:
            position_embeddings: Tuple of (cos, sin) tensors

        Returns:
            Tuple of (hooked_cos, hooked_sin) tensors
        """
        if isinstance(position_embeddings, tuple) and len(position_embeddings) == 2:
            cos, sin = position_embeddings
            # Apply hooks to match HookedTransformer's rotary_cos/rotary_sin pattern
            hooked_cos = self.hook_cos(cos)
            hooked_sin = self.hook_sin(sin)
            return (hooked_cos, hooked_sin)
        return position_embeddings

    def get_random_inputs(
        self,
        batch_size: int = 2,
        seq_len: int = 8,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> Dict[str, Any]:
        """Generate random inputs for Gemma-3 attention testing.

        Gemma-3's position_embeddings are generated by calling rotary_emb(seq_len, device)
        which returns a tuple of (cos, sin) tensors with shape [seq_len, head_dim].

        Args:
            batch_size: Batch size for generated inputs
            seq_len: Sequence length for generated inputs
            device: Device to place tensors on
            dtype: Dtype for generated tensors

        Returns:
            Dictionary with keys: hidden_states, position_embeddings, attention_mask
        """
        if device is None:
            device = torch.device("cpu")
        if dtype is None:
            dtype = torch.float32
        d_model = self.config.d_model if self.config and hasattr(self.config, "d_model") else 1152
        inputs: Dict[str, Any] = {
            "hidden_states": torch.randn(batch_size, seq_len, d_model, device=device, dtype=dtype)
        }
        num_heads = (
            self.config.num_attention_heads
            if self.config and hasattr(self.config, "num_attention_heads")
            else 4
        )
        head_dim = self.config.head_dim if self.config and hasattr(self.config, "head_dim") else 256
        dummy_qk = torch.randn(1, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        position_ids = torch.arange(seq_len, device=device).unsqueeze(0)
        if self._rotary_emb is not None:
            try:
                position_embeddings = self._rotary_emb(dummy_qk, position_ids)
                inputs["position_embeddings"] = position_embeddings
            except Exception as e:
                cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                inputs["position_embeddings"] = (cos, sin)
        else:
            cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
            sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
            inputs["position_embeddings"] = (cos, sin)
        inputs["attention_mask"] = None
        return inputs

    def setup_hook_compatibility(self) -> None:
        """Setup hook compatibility transformations to match HookedTransformer behavior.

        For models requiring position embeddings (like Gemma with RoPE), this wraps the
        HF attention forward to inject required position_embeddings and attention_mask.
        This is necessary because some code paths may call the HF attention directly,
        bypassing the bridge's forward method.

        Also sets up rotary embedding hooks (hook_rot_q, hook_rot_k) by monkey-patching
        the HF attention's forward to intercept Q/K after rotary position embeddings are applied.

        Also sets up Q/K/V/Z hook reshaping like the base AttentionBridge.

        This is called during Bridge.__init__ and should always be run.
        Note: This method is idempotent - can be called multiple times safely.
        """
        if self.original_component is None:
            return
        original_hf_forward = self.original_component.forward

        def wrapped_forward(*args, **kwargs):
            """Wrapper that injects position_embeddings, attention_mask, and rotary hooks."""
            if "position_embeddings" not in kwargs or kwargs["position_embeddings"] is None:
                hidden_states = kwargs.get("hidden_states") or (args[0] if len(args) > 0 else None)
                if hidden_states is not None:
                    batch_size, seq_len, _ = hidden_states.shape
                    device = hidden_states.device
                    dtype = hidden_states.dtype
                    if self._rotary_emb is not None:
                        head_dim = (
                            self.config.head_dim
                            if self.config and hasattr(self.config, "head_dim")
                            else 256
                        )
                        num_heads = (
                            self.config.num_attention_heads
                            if self.config and hasattr(self.config, "num_attention_heads")
                            else 4
                        )
                        dummy_qk = torch.randn(
                            1, seq_len, num_heads, head_dim, device=device, dtype=dtype
                        )
                        position_ids = torch.arange(seq_len, device=device).unsqueeze(0)
                        try:
                            position_embeddings = self._rotary_emb(dummy_qk, position_ids)
                            # Apply hooks to cos and sin
                            kwargs["position_embeddings"] = self._apply_position_embedding_hooks(
                                position_embeddings
                            )
                        except Exception as e:
                            warnings.warn(f"Rotary embedding call failed: {e}, using fallback")
                            cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                            sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                            # Apply hooks to fallback cos/sin
                            kwargs["position_embeddings"] = self._apply_position_embedding_hooks(
                                (cos, sin)
                            )
                    else:
                        head_dim = (
                            self.config.head_dim
                            if self.config and hasattr(self.config, "head_dim")
                            else 256
                        )
                        cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                        sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                        # Apply hooks to fallback cos/sin
                        kwargs["position_embeddings"] = self._apply_position_embedding_hooks(
                            (cos, sin)
                        )
            if "attention_mask" not in kwargs:
                kwargs["attention_mask"] = None
            original_apply_rotary = gemma2_module.apply_rotary_pos_emb

            def hooked_apply_rotary(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
                """Wrapper that applies rotary hooks BEFORE rotation (matching HookedTransformer)."""
                if hasattr(self, "hook_rot_q"):
                    q = self.hook_rot_q(q)
                if hasattr(self, "hook_rot_k"):
                    k = self.hook_rot_k(k)
                rotated_q, rotated_k = original_apply_rotary(
                    q, k, cos, sin, position_ids, unsqueeze_dim
                )
                return (rotated_q, rotated_k)

            gemma2_module.apply_rotary_pos_emb = hooked_apply_rotary
            try:
                result = original_hf_forward(*args, **kwargs)
            finally:
                gemma2_module.apply_rotary_pos_emb = original_apply_rotary
            return result

        self.original_component.forward = wrapped_forward
        self._hf_forward_wrapped = True
        if hasattr(self.config, "n_heads"):
            self._setup_qkv_hook_reshaping()

    def forward(self, *args: Any, **kwargs: Any) -> Any:
        """Simplified forward pass - minimal wrapping around original component.

        This override strips back the complex processing in AttentionBridge._process_output()
        to do minimal wrapping: hook_in â†’ delegate â†’ hook_out.

        This ensures we match HuggingFace's exact output without any intermediate
        pattern extraction or re-application logic.

        Args:
            *args: Input arguments to pass to the original component
            **kwargs: Input keyword arguments to pass to the original component

        Returns:
            The output from the original component, with only input/output hooks applied
        """
        if self.original_component is None:
            raise RuntimeError(
                f"Original component not set for {self.name}. Call set_original_component() first."
            )
        target_dtype = None
        try:
            target_dtype = next(self.original_component.parameters()).dtype
        except StopIteration:
            pass
        if "hidden_states" in kwargs:
            hooked = self.hook_in(kwargs["hidden_states"])
            if (
                target_dtype is not None
                and isinstance(hooked, torch.Tensor)
                and hooked.is_floating_point()
            ):
                hooked = hooked.to(dtype=target_dtype)
            kwargs["hidden_states"] = hooked
        elif len(args) > 0:
            hooked = self.hook_in(args[0])
            if (
                target_dtype is not None
                and isinstance(hooked, torch.Tensor)
                and hooked.is_floating_point()
            ):
                hooked = hooked.to(dtype=target_dtype)
            args = (hooked,) + args[1:]
            kwargs["hidden_states"] = args[0]
            args = args[1:]
        else:
            raise ValueError("Gemma3Attention requires hidden_states argument")
        if "position_embeddings" not in kwargs or kwargs["position_embeddings"] is None:
            hidden_states = kwargs["hidden_states"]
            batch_size, seq_len, _ = hidden_states.shape
            device = hidden_states.device
            dtype = hidden_states.dtype
            if self._rotary_emb is not None:
                head_dim = (
                    self.config.head_dim
                    if self.config and hasattr(self.config, "head_dim")
                    else 256
                )
                num_heads = (
                    self.config.num_attention_heads
                    if self.config and hasattr(self.config, "num_attention_heads")
                    else 4
                )
                dummy_qk = torch.randn(1, seq_len, num_heads, head_dim, device=device, dtype=dtype)
                position_ids = torch.arange(seq_len, device=device).unsqueeze(0)
                try:
                    position_embeddings = self._rotary_emb(dummy_qk, position_ids)
                    # Apply hooks to cos and sin
                    kwargs["position_embeddings"] = self._apply_position_embedding_hooks(
                        position_embeddings
                    )
                except Exception:
                    cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                    sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                    # Apply hooks to fallback cos/sin
                    kwargs["position_embeddings"] = self._apply_position_embedding_hooks((cos, sin))
            else:
                head_dim = (
                    self.config.head_dim
                    if self.config and hasattr(self.config, "head_dim")
                    else 256
                )
                cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                # Apply hooks to fallback cos/sin
                kwargs["position_embeddings"] = self._apply_position_embedding_hooks((cos, sin))
        if "attention_mask" not in kwargs:
            kwargs["attention_mask"] = None
        kwargs["output_attentions"] = True
        output = self.original_component(*args, **kwargs)
        if isinstance(output, tuple) and len(output) >= 2:
            attn_weights = output[1]
            if attn_weights is not None:
                _ = self.hook_pattern(attn_weights)
                with torch.no_grad():
                    d_head = (
                        self.config.head_dim
                        if self.config and hasattr(self.config, "head_dim")
                        else 256
                    )
                    attn_scores_approx = torch.log(attn_weights + 1e-10) * d_head**0.5
                _ = self.hook_attn_scores(attn_scores_approx)
        if isinstance(output, tuple) and len(output) >= 2:
            output = (self.hook_out(output[0]), output[1])
        elif isinstance(output, tuple) and len(output) == 1:
            output = (self.hook_out(output[0]),)
        else:
            output = self.hook_out(output)
        return output
