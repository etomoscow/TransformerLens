"""Gemma-3 specialized attention bridge.

Gemma-3 uses a unique dual RoPE (Rotary Position Embedding) system that requires
position_embeddings to be generated from the model's rotary_emb component rather
than using the standard (cos, sin) tuple format.
"""

from __future__ import annotations

from typing import Any, Dict, Optional

import torch

from transformer_lens.model_bridge.generalized_components.attention import AttentionBridge


class Gemma3AttentionBridge(AttentionBridge):
    """Specialized attention bridge for Gemma-3 models.

    Gemma-3 uses dual RoPE (global + local) which requires position_embeddings
    to be generated in a specific format that differs from standard RoPE models.

    The position_embeddings are generated by calling the model's rotary_emb
    component with dummy Q/K tensors and position_ids.
    """

    def __init__(
        self,
        name: str,
        config: Any,
        submodules: Optional[Dict[str, Any]] = None,
        **kwargs,
    ):
        """Initialize Gemma-3 attention bridge.

        Args:
            name: Component name
            config: Model configuration
            submodules: Dictionary of subcomponents
            **kwargs: Additional arguments passed to AttentionBridge
        """
        # Gemma-3 always requires position_embeddings and attention_mask
        kwargs['requires_position_embeddings'] = True
        kwargs['requires_attention_mask'] = True

        super().__init__(name, config, submodules, **kwargs)

        # Reference to model's rotary_emb component (will be set later)
        self._rotary_emb = None

    def set_rotary_emb(self, rotary_emb):
        """Set reference to the model's rotary embedding component.

        Args:
            rotary_emb: The model's rotary_emb component (from model.model.rotary_emb)
        """
        self._rotary_emb = rotary_emb

    def get_random_inputs(
        self,
        batch_size: int = 2,
        seq_len: int = 8,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> Dict[str, Any]:
        """Generate random inputs for Gemma-3 attention testing.

        Gemma-3's position_embeddings are generated by calling rotary_emb(seq_len, device)
        which returns a tuple of (cos, sin) tensors with shape [seq_len, head_dim].

        Args:
            batch_size: Batch size for generated inputs
            seq_len: Sequence length for generated inputs
            device: Device to place tensors on
            dtype: Dtype for generated tensors

        Returns:
            Dictionary with keys: hidden_states, position_embeddings, attention_mask
        """
        if device is None:
            device = torch.device("cpu")
        if dtype is None:
            dtype = torch.float32

        # Generate base hidden_states
        d_model = self.config.d_model if self.config and hasattr(self.config, "d_model") else 1152
        inputs = {
            "hidden_states": torch.randn(batch_size, seq_len, d_model, device=device, dtype=dtype)
        }

        # Generate Gemma-3-specific position_embeddings
        # Gemma-3 attention expects position_embeddings as a tuple of (cos, sin) with shape [1, seq_len, head_dim]
        # Note: batch dimension is ALWAYS 1, regardless of actual batch_size
        num_heads = self.config.num_attention_heads if self.config and hasattr(self.config, "num_attention_heads") else 4
        head_dim = self.config.head_dim if self.config and hasattr(self.config, "head_dim") else 256

        # Create dummy Q/K tensor and position_ids with batch=1
        dummy_qk = torch.randn(1, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        position_ids = torch.arange(seq_len, device=device).unsqueeze(0)  # [1, seq_len]

        # Generate position_embeddings using rotary_emb
        if self._rotary_emb is not None:
            # Use the model's actual rotary_emb component
            try:
                position_embeddings = self._rotary_emb(dummy_qk, position_ids)
                inputs["position_embeddings"] = position_embeddings
            except Exception as e:
                # Fallback: generate dummy position_embeddings with correct shape
                # Gemma-3 expects tuple of 2 tensors, each [1, seq, head_dim]
                cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                inputs["position_embeddings"] = (cos, sin)
        else:
            # Fallback if rotary_emb not available: generate dummy position_embeddings
            # Gemma-3 expects tuple of 2 tensors, each [1, seq, head_dim]
            cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
            sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
            inputs["position_embeddings"] = (cos, sin)

        # Generate attention_mask
        # Gemma-3 attention expects attention_mask to be None when called directly
        inputs["attention_mask"] = None

        return inputs
