"""Gemma-3 specialized attention bridge.

Gemma-3 uses a unique dual RoPE (Rotary Position Embedding) system that requires
position_embeddings to be generated from the model's rotary_emb component rather
than using the standard (cos, sin) tuple format.
"""

from __future__ import annotations

from typing import Any, Dict, Optional

import torch

from transformer_lens.model_bridge.generalized_components.attention import (
    AttentionBridge,
)


class PositionEmbeddingsAttentionBridge(AttentionBridge):
    """Attention bridge for models that require position embeddings (e.g., Gemma-3).

    Some models use specialized position embedding systems (like Gemma-3's dual RoPE)
    which require position_embeddings to be generated in a specific format that differs
    from standard RoPE models.

    The position_embeddings are generated by calling the model's rotary_emb
    component with dummy Q/K tensors and position_ids.
    """

    def __init__(
        self,
        name: str,
        config: Any,
        submodules: Optional[Dict[str, Any]] = None,
        **kwargs,
    ):
        """Initialize Gemma-3 attention bridge.

        Args:
            name: Component name
            config: Model configuration
            submodules: Dictionary of subcomponents
            **kwargs: Additional arguments passed to AttentionBridge
        """
        # Gemma-3 always requires position_embeddings and attention_mask
        kwargs["requires_position_embeddings"] = True
        kwargs["requires_attention_mask"] = True

        super().__init__(name, config, submodules, **kwargs)

        # Reference to model's rotary_emb component (will be set later)
        self._rotary_emb = None

    def set_rotary_emb(self, rotary_emb):
        """Set reference to the model's rotary embedding component.

        Args:
            rotary_emb: The model's rotary_emb component (from model.model.rotary_emb)
        """
        self._rotary_emb = rotary_emb

    def get_random_inputs(
        self,
        batch_size: int = 2,
        seq_len: int = 8,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> Dict[str, Any]:
        """Generate random inputs for Gemma-3 attention testing.

        Gemma-3's position_embeddings are generated by calling rotary_emb(seq_len, device)
        which returns a tuple of (cos, sin) tensors with shape [seq_len, head_dim].

        Args:
            batch_size: Batch size for generated inputs
            seq_len: Sequence length for generated inputs
            device: Device to place tensors on
            dtype: Dtype for generated tensors

        Returns:
            Dictionary with keys: hidden_states, position_embeddings, attention_mask
        """
        if device is None:
            device = torch.device("cpu")
        if dtype is None:
            dtype = torch.float32

        # Generate base hidden_states
        d_model = self.config.d_model if self.config and hasattr(self.config, "d_model") else 1152
        inputs: Dict[str, Any] = {
            "hidden_states": torch.randn(batch_size, seq_len, d_model, device=device, dtype=dtype)
        }

        # Generate Gemma-3-specific position_embeddings
        # Gemma-3 attention expects position_embeddings as a tuple of (cos, sin) with shape [1, seq_len, head_dim]
        # Note: batch dimension is ALWAYS 1, regardless of actual batch_size
        num_heads = (
            self.config.num_attention_heads
            if self.config and hasattr(self.config, "num_attention_heads")
            else 4
        )
        head_dim = self.config.head_dim if self.config and hasattr(self.config, "head_dim") else 256

        # Create dummy Q/K tensor and position_ids with batch=1
        dummy_qk = torch.randn(1, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        position_ids = torch.arange(seq_len, device=device).unsqueeze(0)  # [1, seq_len]

        # Generate position_embeddings using rotary_emb
        if self._rotary_emb is not None:
            # Use the model's actual rotary_emb component
            try:
                position_embeddings = self._rotary_emb(dummy_qk, position_ids)
                inputs["position_embeddings"] = position_embeddings
            except Exception as e:
                # Fallback: generate dummy position_embeddings with correct shape
                # Gemma-3 expects tuple of 2 tensors, each [1, seq, head_dim]
                cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                inputs["position_embeddings"] = (cos, sin)
        else:
            # Fallback if rotary_emb not available: generate dummy position_embeddings
            # Gemma-3 expects tuple of 2 tensors, each [1, seq, head_dim]
            cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
            sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
            inputs["position_embeddings"] = (cos, sin)

        # Generate attention_mask
        # Gemma-3 attention expects attention_mask to be None when called directly
        inputs["attention_mask"] = None

        return inputs

    def forward(self, *args: Any, **kwargs: Any) -> Any:
        """Simplified forward pass - minimal wrapping around original component.

        This override strips back the complex processing in AttentionBridge._process_output()
        to do minimal wrapping: hook_in → delegate → hook_out.

        This ensures we match HuggingFace's exact output without any intermediate
        pattern extraction or re-application logic.

        Args:
            *args: Input arguments to pass to the original component
            **kwargs: Input keyword arguments to pass to the original component

        Returns:
            The output from the original component, with only input/output hooks applied
        """
        if self.original_component is None:
            raise RuntimeError(
                f"Original component not set for {self.name}. Call set_original_component() first."
            )

        # Apply input hook to hidden_states
        # Gemma3Attention expects hidden_states kwarg, but in compatibility mode
        # it may be passed as a positional argument
        if "hidden_states" in kwargs:
            kwargs["hidden_states"] = self.hook_in(kwargs["hidden_states"])
        elif len(args) > 0:
            # hidden_states passed as first positional argument (compatibility mode)
            args = (self.hook_in(args[0]),) + args[1:]
            # Move it to kwargs for HF compatibility
            kwargs["hidden_states"] = args[0]
            args = args[1:]
        else:
            raise ValueError("Gemma3Attention requires hidden_states argument")

        # Forward through original component
        output = self.original_component(*args, **kwargs)

        # Apply hook_out to the output
        # HF Gemma3Attention returns a tuple: (hidden_states, attention_weights)
        # We only hook the first element (hidden_states)
        output = (self.hook_out(output[0]),) + output[1:]

        return output
