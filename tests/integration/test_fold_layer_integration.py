#!/usr/bin/env python3
"""
Integration Test for _fold_layer Function with Real GPT-2 Model
==============================================================

This test verifies that the _fold_layer function works correctly with:
1. Real GPT-2 model loaded from HuggingFace
2. GPT-2 architecture adapter for parameter key translation
3. Actual model weights and configurations
4. Both TransformerLens format (no adapter) and HuggingFace format (with adapter) processing
"""

import pytest
import torch
from transformers import GPT2Config, GPT2LMHeadModel

from transformer_lens import HookedTransformer
from transformer_lens.config import TransformerBridgeConfig
from transformer_lens.model_bridge.supported_architectures.gpt2 import (
    GPT2ArchitectureAdapter,
)
from transformer_lens.weight_processing import ProcessWeights


class TestFoldLayerIntegration:
    """Integration tests for _fold_layer function with real models."""

    @pytest.fixture
    def gpt2_model_and_config(self):
        """Load a real GPT-2 model and configuration."""
        model_name = "gpt2"
        device = "cpu"
        
        # Load HuggingFace model
        hf_model = GPT2LMHeadModel.from_pretrained(model_name)
        hf_config = hf_model.config
        
        # Load HookedTransformer model
        tl_model = HookedTransformer.from_pretrained(model_name, device=device)
        
        # Create architecture adapter
        # Convert HookedTransformerConfig to TransformerBridgeConfig
        bridge_config = TransformerBridgeConfig.from_dict(tl_model.cfg.__dict__)
        bridge_config.architecture = "gpt2"
        adapter = GPT2ArchitectureAdapter(bridge_config)
        
        return {
            "hf_model": hf_model,
            "hf_config": hf_config,
            "tl_model": tl_model,
            "adapter": adapter,
            "device": device
        }

    def test_fold_layer_with_real_gpt2_transformer_lens_format(self, gpt2_model_and_config):
        """Test _fold_layer with real GPT-2 model in TransformerLens format (no adapter)."""
        tl_model = gpt2_model_and_config["tl_model"]
        cfg = tl_model.cfg
        
        # Get the state dict from HookedTransformer (TransformerLens format)
        state_dict = tl_model.state_dict()
        
        # Test with layer 0
        layer_idx = 0
        
        # Check if LayerNorm parameters exist (they shouldn't for processed models)
        ln1_b_key = f"blocks.{layer_idx}.ln1.b"
        ln1_w_key = f"blocks.{layer_idx}.ln1.w"
        
        if ln1_b_key not in state_dict or ln1_w_key not in state_dict:
            # This is expected for processed HookedTransformer models
            # The LayerNorm parameters have already been folded out
            print(f"LayerNorm parameters not found in state dict - model is already processed")
            print(f"Available keys: {[k for k in state_dict.keys() if f'blocks.{layer_idx}' in k]}")
            
            # Test that _fold_layer handles this gracefully (should only do centering if requested)
            original_state_dict = {k: v.clone() for k, v in state_dict.items()}
            
            # Test _fold_layer with no adapter (TransformerLens format)
            ProcessWeights._fold_layer(
                state_dict, cfg, layer_idx=layer_idx, fold_biases=True, center_weights=True, adapter=None, gqa=""
            )
            
            # For processed models, _fold_layer should only center weights if LayerNorm params don't exist
            # Verify that weights are centered
            w_q_key = f"blocks.{layer_idx}.attn.W_Q"
            w_q = state_dict[w_q_key]
            w_q_mean = torch.mean(w_q, dim=1, keepdim=True)
            assert torch.allclose(w_q_mean, torch.zeros_like(w_q_mean), atol=1e-6)
            
            # Verify original state dict is unchanged
            for k, v in original_state_dict.items():
                assert torch.equal(v, original_state_dict[k])
            
            return  # Skip the rest of the test since model is already processed
        
        # Verify LayerNorm weights are removed
        assert f"blocks.{layer_idx}.ln1.w" not in state_dict
        assert f"blocks.{layer_idx}.ln1.b" not in state_dict
        assert f"blocks.{layer_idx}.ln2.w" not in state_dict
        assert f"blocks.{layer_idx}.ln2.b" not in state_dict
        
        # Verify attention weights are modified
        w_q_key = f"blocks.{layer_idx}.attn.W_Q"
        w_k_key = f"blocks.{layer_idx}.attn.W_K"
        w_v_key = f"blocks.{layer_idx}.attn.W_V"
        
        assert w_q_key in state_dict
        assert w_k_key in state_dict
        assert w_v_key in state_dict
        
        # Check that weights are centered (mean should be zero across d_model dimension)
        w_q_mean = torch.mean(state_dict[w_q_key], dim=1, keepdim=True)  # [n_heads, 1, d_head]
        w_k_mean = torch.mean(state_dict[w_k_key], dim=1, keepdim=True)
        w_v_mean = torch.mean(state_dict[w_v_key], dim=1, keepdim=True)
        
        assert torch.allclose(w_q_mean, torch.zeros_like(w_q_mean), atol=1e-6)
        assert torch.allclose(w_k_mean, torch.zeros_like(w_k_mean), atol=1e-6)
        assert torch.allclose(w_v_mean, torch.zeros_like(w_v_mean), atol=1e-6)
        
        # Verify attention biases are modified
        b_q_key = f"blocks.{layer_idx}.attn.b_Q"
        b_k_key = f"blocks.{layer_idx}.attn.b_K"
        b_v_key = f"blocks.{layer_idx}.attn.b_V"
        
        assert b_q_key in state_dict
        assert b_k_key in state_dict
        assert b_v_key in state_dict
        
        # Verify MLP weights are modified
        mlp_w_in_key = f"blocks.{layer_idx}.mlp.W_in"
        mlp_b_in_key = f"blocks.{layer_idx}.mlp.b_in"
        
        assert mlp_w_in_key in state_dict
        assert mlp_b_in_key in state_dict
        
        # Check that MLP weights are centered
        mlp_w_mean = torch.mean(state_dict[mlp_w_in_key], dim=0, keepdim=True)  # [1, d_mlp]
        assert torch.allclose(mlp_w_mean, torch.zeros_like(mlp_w_mean), atol=1e-6)
        
        # Verify original state dict is unchanged
        for k, v in original_state_dict.items():
            assert torch.equal(v, original_state_dict[k])

    def test_fold_layer_with_real_gpt2_huggingface_format(self, gpt2_model_and_config):
        """Test _fold_layer with real GPT-2 model in HuggingFace format (with adapter)."""
        hf_model = gpt2_model_and_config["hf_model"]
        tl_model = gpt2_model_and_config["tl_model"]
        adapter = gpt2_model_and_config["adapter"]
        cfg = tl_model.cfg
        
        # Get the state dict from HuggingFace model (HuggingFace format)
        state_dict = hf_model.state_dict()
        
        # Test with layer 0
        layer_idx = 0
        
        # Make a copy for comparison
        original_state_dict = {k: v.clone() for k, v in state_dict.items()}
        
        # Test _fold_layer with adapter (HuggingFace format)
        ProcessWeights._fold_layer(
            state_dict, cfg, layer_idx=layer_idx, fold_biases=True, center_weights=True, adapter=adapter, gqa=""
        )
        
        # Verify LayerNorm weights are removed (using HuggingFace keys)
        assert f"transformer.h.{layer_idx}.ln_1.weight" not in state_dict
        assert f"transformer.h.{layer_idx}.ln_1.bias" not in state_dict
        assert f"transformer.h.{layer_idx}.ln_2.weight" not in state_dict
        assert f"transformer.h.{layer_idx}.ln_2.bias" not in state_dict
        
        # Verify combined QKV weight is modified
        qkv_weight_key = f"transformer.h.{layer_idx}.attn.c_attn.weight"
        qkv_bias_key = f"transformer.h.{layer_idx}.attn.c_attn.bias"
        
        assert qkv_weight_key in state_dict
        assert qkv_bias_key in state_dict
        
        # Split the processed QKV weight back into Q, K, V to verify centering
        qkv_weight = state_dict[qkv_weight_key]
        w_q, w_k, w_v = torch.tensor_split(qkv_weight, 3, dim=1)
        
        # Check that weights are centered (mean should be zero across d_model dimension)
        w_q_mean = torch.mean(w_q, dim=0, keepdim=True)  # [1, d_model]
        w_k_mean = torch.mean(w_k, dim=0, keepdim=True)
        w_v_mean = torch.mean(w_v, dim=0, keepdim=True)
        
        assert torch.allclose(w_q_mean, torch.zeros_like(w_q_mean), atol=1e-6)
        assert torch.allclose(w_k_mean, torch.zeros_like(w_k_mean), atol=1e-6)
        assert torch.allclose(w_v_mean, torch.zeros_like(w_v_mean), atol=1e-6)
        
        # Verify MLP weights are modified
        mlp_w_in_key = f"transformer.h.{layer_idx}.mlp.c_fc.weight"
        mlp_b_in_key = f"transformer.h.{layer_idx}.mlp.c_fc.bias"
        
        assert mlp_w_in_key in state_dict
        assert mlp_b_in_key in state_dict
        
        # Check that MLP weights are centered
        mlp_w_mean = torch.mean(state_dict[mlp_w_in_key], dim=0, keepdim=True)  # [1, d_mlp]
        assert torch.allclose(mlp_w_mean, torch.zeros_like(mlp_w_mean), atol=1e-6)
        
        # Verify original state dict is unchanged
        for k, v in original_state_dict.items():
            assert torch.equal(v, original_state_dict[k])

    def test_fold_layer_equivalence_between_formats(self, gpt2_model_and_config):
        """Test that _fold_layer produces equivalent results for both formats."""
        hf_model = gpt2_model_and_config["hf_model"]
        tl_model = gpt2_model_and_config["tl_model"]
        adapter = gpt2_model_and_config["adapter"]
        cfg = tl_model.cfg
        
        layer_idx = 0
        
        # Get state dicts from both models
        tl_state_dict = tl_model.state_dict()
        hf_state_dict = hf_model.state_dict()
        
        # Process TransformerLens format
        tl_processed = {k: v.clone() for k, v in tl_state_dict.items()}
        ProcessWeights._fold_layer(
            tl_processed, cfg, layer_idx=layer_idx, fold_biases=True, center_weights=True, adapter=None, gqa=""
        )
        
        # Process HuggingFace format
        hf_processed = {k: v.clone() for k, v in hf_state_dict.items()}
        ProcessWeights._fold_layer(
            hf_processed, cfg, layer_idx=layer_idx, fold_biases=True, center_weights=True, adapter=adapter, gqa=""
        )
        
        # Compare the results by checking that both formats produce the same mathematical transformations
        # We can't directly compare the tensors since they have different shapes and keys,
        # but we can verify that both produce centered weights
        
        # Check TransformerLens format centering
        tl_w_q = tl_processed[f"blocks.{layer_idx}.attn.W_Q"]
        tl_w_q_mean = torch.mean(tl_w_q, dim=1, keepdim=True)
        assert torch.allclose(tl_w_q_mean, torch.zeros_like(tl_w_q_mean), atol=1e-6)
        
        # Check HuggingFace format centering
        hf_qkv_weight = hf_processed[f"transformer.h.{layer_idx}.attn.c_attn.weight"]
        hf_w_q, hf_w_k, hf_w_v = torch.tensor_split(hf_qkv_weight, 3, dim=1)
        hf_w_q_mean = torch.mean(hf_w_q, dim=0, keepdim=True)
        assert torch.allclose(hf_w_q_mean, torch.zeros_like(hf_w_q_mean), atol=1e-6)
        
        # Both should have LayerNorm weights removed
        assert f"blocks.{layer_idx}.ln1.w" not in tl_processed
        assert f"transformer.h.{layer_idx}.ln_1.weight" not in hf_processed

    def test_fold_layer_with_different_layers(self, gpt2_model_and_config):
        """Test _fold_layer with different layers to ensure it works across all layers."""
        tl_model = gpt2_model_and_config["tl_model"]
        cfg = tl_model.cfg
        
        # Test with multiple layers
        test_layers = [0, 1, cfg.n_layers - 1]  # First, second, and last layer
        
        for layer_idx in test_layers:
            state_dict = tl_model.state_dict()
            original_state_dict = {k: v.clone() for k, v in state_dict.items()}
            
            # Test _fold_layer
            ProcessWeights._fold_layer(
                state_dict, cfg, layer_idx=layer_idx, fold_biases=True, center_weights=True, adapter=None, gqa=""
            )
            
            # Verify LayerNorm weights are removed
            assert f"blocks.{layer_idx}.ln1.w" not in state_dict
            assert f"blocks.{layer_idx}.ln1.b" not in state_dict
            assert f"blocks.{layer_idx}.ln2.w" not in state_dict
            assert f"blocks.{layer_idx}.ln2.b" not in state_dict
            
            # Verify weights are centered
            w_q = state_dict[f"blocks.{layer_idx}.attn.W_Q"]
            w_q_mean = torch.mean(w_q, dim=1, keepdim=True)
            assert torch.allclose(w_q_mean, torch.zeros_like(w_q_mean), atol=1e-6)
            
            # Verify original state dict is unchanged
            for k, v in original_state_dict.items():
                assert torch.equal(v, original_state_dict[k])

    def test_fold_layer_with_different_options(self, gpt2_model_and_config):
        """Test _fold_layer with different processing options."""
        tl_model = gpt2_model_and_config["tl_model"]
        cfg = tl_model.cfg
        layer_idx = 0
        
        # Check if LayerNorm parameters exist (they shouldn't for processed models)
        state_dict = tl_model.state_dict()
        ln1_b_key = f"blocks.{layer_idx}.ln1.b"
        ln1_w_key = f"blocks.{layer_idx}.ln1.w"
        ln2_b_key = f"blocks.{layer_idx}.ln2.b"
        ln2_w_key = f"blocks.{layer_idx}.ln2.w"
        
        if ln1_b_key not in state_dict or ln1_w_key not in state_dict:
            # This is expected for processed HookedTransformer models
            print(f"LayerNorm parameters not found - model is already processed")
            
            # Test 1: No bias folding, with centering (should only do centering)
            state_dict = tl_model.state_dict()
            original_state_dict = {k: v.clone() for k, v in state_dict.items()}
            
            ProcessWeights._fold_layer(
                state_dict, cfg, layer_idx=layer_idx, fold_biases=False, center_weights=True, adapter=None, gqa=""
            )
            
            # For processed models, LayerNorm parameters should still not be present
            assert ln1_b_key not in state_dict
            assert ln2_b_key not in state_dict
            
            # But weights should be centered
            w_q = state_dict[f"blocks.{layer_idx}.attn.W_Q"]
            w_q_mean = torch.mean(w_q, dim=1, keepdim=True)
            assert torch.allclose(w_q_mean, torch.zeros_like(w_q_mean), atol=1e-6)
            
            # Test 2: With bias folding, no centering (should do nothing for processed models)
            state_dict = tl_model.state_dict()
            original_state_dict = {k: v.clone() for k, v in state_dict.items()}
            
            ProcessWeights._fold_layer(
                state_dict, cfg, layer_idx=layer_idx, fold_biases=True, center_weights=False, adapter=None, gqa=""
            )
            
            # For processed models, LayerNorm parameters should still not be present
            assert ln1_b_key not in state_dict
            assert ln2_b_key not in state_dict
            
            # For processed models, weights are already centered from the original processing
            # So even with center_weights=False, they remain centered
            w_q = state_dict[f"blocks.{layer_idx}.attn.W_Q"]
            w_q_mean = torch.mean(w_q, dim=1, keepdim=True)
            # The weights should still be centered (they were already centered from original processing)
            assert torch.allclose(w_q_mean, torch.zeros_like(w_q_mean), atol=1e-6)
            
            return  # Skip the rest of the test since model is already processed
        
        # Test 1: No bias folding, with centering
        state_dict = tl_model.state_dict()
        original_state_dict = {k: v.clone() for k, v in state_dict.items()}
        
        ProcessWeights._fold_layer(
            state_dict, cfg, layer_idx=layer_idx, fold_biases=False, center_weights=True, adapter=None, gqa=""
        )
        
        # LayerNorm biases should still be present when fold_biases=False
        assert f"blocks.{layer_idx}.ln1.b" in state_dict
        assert f"blocks.{layer_idx}.ln2.b" in state_dict
        
        # But weights should be centered
        w_q = state_dict[f"blocks.{layer_idx}.attn.W_Q"]
        w_q_mean = torch.mean(w_q, dim=1, keepdim=True)
        assert torch.allclose(w_q_mean, torch.zeros_like(w_q_mean), atol=1e-6)
        
        # Test 2: With bias folding, no centering
        state_dict = tl_model.state_dict()
        original_state_dict = {k: v.clone() for k, v in state_dict.items()}
        
        ProcessWeights._fold_layer(
            state_dict, cfg, layer_idx=layer_idx, fold_biases=True, center_weights=False, adapter=None, gqa=""
        )
        
        # LayerNorm weights should be removed
        assert f"blocks.{layer_idx}.ln1.w" not in state_dict
        assert f"blocks.{layer_idx}.ln1.b" not in state_dict
        
        # But weights should NOT be centered (mean should not be zero)
        w_q = state_dict[f"blocks.{layer_idx}.attn.W_Q"]
        w_q_mean = torch.mean(w_q, dim=1, keepdim=True)
        # The mean should NOT be close to zero (since centering is disabled)
        assert not torch.allclose(w_q_mean, torch.zeros_like(w_q_mean), atol=1e-6)


if __name__ == "__main__":
    # Run the tests
    pytest.main([__file__, "-v"])
