#!/usr/bin/env python3
"""
Integration Compatibility Test for Weight Processing
====================================================

This test verifies that:
1. HookedTransformer with processing matches expected Main Demo values (3.999 â†’ 5.453)
2. HookedTransformer without processing matches expected unprocessed values (~3.999 â†’ ~4.117)
3. TransformerBridge with processing matches HookedTransformer with processing
4. TransformerBridge without processing matches HookedTransformer without processing
5. Processing maintains mathematical equivalence for baseline computation
6. Processing changes ablation results as expected (for better interpretability)
"""

import torch
from jaxtyping import Float

from transformer_lens import HookedTransformer, utils
from transformer_lens.model_bridge.sources.transformers import boot


def test_integration_compatibility():
    """Test integration compatibility between HookedTransformer and TransformerBridge."""
    model_name = "gpt2"
    device = "cpu"

    # Test text from Main Demo
    test_text = "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets."

    # Ablation parameters from Main Demo
    layer_to_ablate = 0
    head_index_to_ablate = 8

    # Expected values
    expected_hooked_processed_orig = 3.999
    expected_hooked_processed_ablated = 5.453
    expected_hooked_unprocessed_orig = 3.999
    expected_hooked_unprocessed_ablated = 4.117

    # Tolerance for comparisons
    tolerance = 0.01

    def create_ablation_hook():
        """Create the exact ablation hook from Main Demo."""

        def head_ablation_hook(
            value: Float[torch.Tensor, "batch pos head_index d_head"], hook
        ) -> Float[torch.Tensor, "batch pos head_index d_head"]:
            value[:, :, head_index_to_ablate, :] = 0.0
            return value

        return head_ablation_hook

    def test_model_ablation(model, model_name: str):
        """Test a model and return original and ablated losses."""
        tokens = model.to_tokens(test_text)

        # Original loss
        original_loss = model(tokens, return_type="loss").item()

        # Ablated loss
        ablated_loss = model.run_with_hooks(
            tokens,
            return_type="loss",
            fwd_hooks=[(utils.get_act_name("v", layer_to_ablate), create_ablation_hook())],
        ).item()

        print(f"{model_name}: Original={original_loss:.6f}, Ablated={ablated_loss:.6f}")
        return original_loss, ablated_loss

    print("Testing HookedTransformer with processing...")
    hooked_processed = HookedTransformer.from_pretrained(
        model_name,
        device=device,
        fold_ln=True,
        center_writing_weights=True,
        center_unembed=True,
        fold_value_biases=True,
    )
    hooked_proc_orig, hooked_proc_ablated = test_model_ablation(
        hooked_processed, "HookedTransformer (processed)"
    )

    print("Testing HookedTransformer without processing...")
    hooked_unprocessed = HookedTransformer.from_pretrained_no_processing(model_name, device=device)
    hooked_unproc_orig, hooked_unproc_ablated = test_model_ablation(
        hooked_unprocessed, "HookedTransformer (unprocessed)"
    )

    print("Testing TransformerBridge with processing...")
    bridge_processed = boot(model_name, device=device)
    # Apply processing to the bridge
    # TODO: Need to implement processing for TransformerBridge
    bridge_proc_orig, bridge_proc_ablated = test_model_ablation(
        bridge_processed, "TransformerBridge (processed)"
    )

    print("Testing TransformerBridge without processing...")
    bridge_unprocessed = boot(model_name, device=device)
    # No processing applied
    bridge_unproc_orig, bridge_unproc_ablated = test_model_ablation(
        bridge_unprocessed, "TransformerBridge (unprocessed)"
    )

    # Assertions
    print("\nRunning assertions...")

    # Test 1: HookedTransformer processed matches Main Demo
    assert (
        abs(hooked_proc_orig - expected_hooked_processed_orig) < tolerance
    ), f"HookedTransformer processed original loss {hooked_proc_orig:.6f} != expected {expected_hooked_processed_orig:.3f}"
    assert (
        abs(hooked_proc_ablated - expected_hooked_processed_ablated) < tolerance
    ), f"HookedTransformer processed ablated loss {hooked_proc_ablated:.6f} != expected {expected_hooked_processed_ablated:.3f}"
    print("âœ… HookedTransformer processed matches Main Demo")

    # Test 2: HookedTransformer unprocessed matches expected
    assert (
        abs(hooked_unproc_orig - expected_hooked_unprocessed_orig) < tolerance
    ), f"HookedTransformer unprocessed original loss {hooked_unproc_orig:.6f} != expected {expected_hooked_unprocessed_orig:.3f}"
    assert (
        abs(hooked_unproc_ablated - expected_hooked_unprocessed_ablated) < tolerance
    ), f"HookedTransformer unprocessed ablated loss {hooked_unproc_ablated:.6f} != expected {expected_hooked_unprocessed_ablated:.3f}"
    print("âœ… HookedTransformer unprocessed matches expected")

    # Test 3: Baseline mathematical equivalence
    orig_diff = abs(hooked_proc_orig - hooked_unproc_orig)
    assert (
        orig_diff < 0.001
    ), f"Baseline computation not mathematically equivalent: diff={orig_diff:.6f}"
    print("âœ… Baseline computation is mathematically equivalent")

    # Test 4: Ablation interpretability enhancement
    ablated_diff = abs(hooked_proc_ablated - hooked_unproc_ablated)
    assert (
        ablated_diff > 0.5
    ), f"Ablation results should be significantly different for interpretability: diff={ablated_diff:.6f}"
    print("âœ… Ablation results show interpretability enhancement")

    # Test 5: TransformerBridge processed matches HookedTransformer processed
    assert (
        abs(bridge_proc_orig - hooked_proc_orig) < tolerance
    ), f"TransformerBridge processed original {bridge_proc_orig:.6f} != HookedTransformer processed {hooked_proc_orig:.6f}"
    assert (
        abs(bridge_proc_ablated - hooked_proc_ablated) < tolerance
    ), f"TransformerBridge processed ablated {bridge_proc_ablated:.6f} != HookedTransformer processed {hooked_proc_ablated:.6f}"
    print("âœ… TransformerBridge processed matches HookedTransformer processed")

    # Test 6: TransformerBridge unprocessed matches HookedTransformer unprocessed
    assert (
        abs(bridge_unproc_orig - hooked_unproc_orig) < tolerance
    ), f"TransformerBridge unprocessed original {bridge_unproc_orig:.6f} != HookedTransformer unprocessed {hooked_unproc_orig:.6f}"
    assert (
        abs(bridge_unproc_ablated - hooked_unproc_ablated) < tolerance
    ), f"TransformerBridge unprocessed ablated {bridge_unproc_ablated:.6f} != HookedTransformer unprocessed {hooked_unproc_ablated:.6f}"
    print("âœ… TransformerBridge unprocessed matches HookedTransformer unprocessed")

    print("\nðŸŽ‰ ALL TESTS PASSED! Integration compatibility verified!")


if __name__ == "__main__":
    test_integration_compatibility()
